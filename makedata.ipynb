{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db13ca2-c8cb-4989-941f-fe1bfc876ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a0ae9a-23ba-4a9b-ab1b-1bf5f289afc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_label_data(folder_path, label, columns_to_remove=None):\n",
    "    sequences = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # CSV 파일을 로드\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # 필요 없는 컬럼 제거\n",
    "            if columns_to_remove:\n",
    "                df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "            # DataFrame과 레이블을 튜플로 저장\n",
    "            sequences.append((df, label))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e43dd6-70ab-4516-84d9-dfb681eb7c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_remove = [\"nose-conf\", \"left-eye-conf\", \"right-eye-conf\", \"left-ear-conf\", \"right-ear-conf\", \"left-shoulder-conf\", \"right-shoulder-conf\", \"left-elbow-conf\", \"right-elbow-conf\", \"left-hand-conf\", \"right-hand-conf\", \"left-hip-conf\", \"right-hip-conf\", \"left-knee-conf\", \"right-knee-conf\", \"left-foot-conf\", \"right-foot-conf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc71e26a-43fc-4a64-964e-fe60deba3d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parkinson_sequences = load_and_label_data(\"C:/jihoon/walking_dataset_withcsv/parkinson\", 0, columns_to_remove)\n",
    "limping_sequences = load_and_label_data(\"C:/jihoon/walking_dataset_withcsv/limping\", 1, columns_to_remove)\n",
    "normal_sequences = load_and_label_data(\"C:/jihoon/walking_dataset_withcsv/normal\", 2, columns_to_remove)\n",
    "\n",
    "all_sequences = parkinson_sequences + limping_sequences + normal_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30dd92a-c0c3-4ccb-a575-70c7b4ddd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_sequences = load_and_label_data(\"/Users/jihoon/venvs/skeleton-rnn/walking_dataset_withcsv/parkinson\", 0, columns_to_remove)\n",
    "limping_sequences = load_and_label_data(\"/Users/jihoon/venvs/skeleton-rnn/walking_dataset_withcsv/limping\", 1, columns_to_remove)\n",
    "normal_sequences = load_and_label_data(\"/Users/jihoon/venvs/skeleton-rnn/walking_dataset_withcsv/normal\", 2, columns_to_remove)\n",
    "\n",
    "all_sequences = parkinson_sequences + limping_sequences + normal_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e49e7f6-572d-4aec-85a9-3fa88783b0fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1640.765427  1001.122040  1652.949249   991.098228  1646.052826   \n",
       "  1    1600.238403   993.187775  1611.536728   982.314766  1606.944733   \n",
       "  2    1649.612701  1012.943954  1658.838684   999.474632  1649.444275   \n",
       "  3    1640.742645  1013.150887  1641.238464   999.229019  1643.798187   \n",
       "  4    1630.034103  1016.890434  1632.069626  1004.006180  1636.213791   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  403   332.656816   680.685841   332.485747   671.064034   331.137372   \n",
       "  404   341.571198   692.332527   340.558084   681.191095   341.088711   \n",
       "  405   330.325912   703.322773   331.354206   693.411131   327.423069   \n",
       "  406   334.791238   796.464302   334.146452   791.504623   334.086088   \n",
       "  407   330.000000   797.139414   330.646460   789.812023   330.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     986.208366  1701.713287  1006.014130  1675.134537   984.905960  ...   \n",
       "  1     978.133137  1679.934448   999.684204  1684.505432   973.981873  ...   \n",
       "  2     996.182655  1708.033600   994.886528  1688.181702   983.231201  ...   \n",
       "  3     996.926193  1671.182953   997.882202  1717.616577   981.689346  ...   \n",
       "  4    1001.949760  1670.543854  1011.849281  1699.023499   999.997742  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  403   670.757277   324.048019   670.708427   331.512020   670.972047  ...   \n",
       "  404   680.823752   331.089705   682.367259   339.117662   673.728064  ...   \n",
       "  405   692.660742   325.358763   693.934574   323.000000   684.525146  ...   \n",
       "  406   790.669632   322.000000   791.214607   329.773623   791.150475  ...   \n",
       "  407   789.129477   330.000000   790.728405   331.262236   791.246952  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1600.249298  1227.315338  1548.562721  1205.163269   1530.754105   \n",
       "  1    1621.829636  1420.383575  1603.962616  1477.793457   1590.327621   \n",
       "  2    1627.294769  1415.286163  1617.287888  1516.937805   1597.456894   \n",
       "  3    1634.730225  1448.436584  1621.137863  1543.451233   1592.182785   \n",
       "  4    1638.539978  1303.562500  1589.562576  1389.509491   1567.683685   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  403   337.522869   875.280853   335.603317  1015.737915    342.339039   \n",
       "  404   335.413056   899.217224   336.778005  1030.279419    342.153507   \n",
       "  405   323.612683   897.490143   330.024117  1020.984161    330.238097   \n",
       "  406   324.133722   956.959686   326.485486  1047.571167    328.578692   \n",
       "  407   330.566660   943.912354   331.284104  1039.981720    330.213994   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1195.417938  1477.659950  1201.965118   1472.229534   1206.412201  \n",
       "  1     1461.664062  1596.817505  1471.626709   1566.537827   1463.077942  \n",
       "  2     1502.156372  1588.121979  1526.656189   1544.263992   1516.528320  \n",
       "  3     1536.220398  1598.893677  1557.779114   1548.038361   1555.320435  \n",
       "  4     1373.243866  1538.207123  1566.712463   1526.076263   1563.461060  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  403   1014.476196   369.347321  1135.781555    368.522987   1136.123962  \n",
       "  404   1029.091553   371.940060  1141.192352    373.170372   1140.857117  \n",
       "  405   1019.768768   365.285664  1130.387268    361.901646   1129.697113  \n",
       "  406   1042.606354   364.555550  1132.769318    363.940983   1131.235016  \n",
       "  407   1039.698914   361.179741  1126.653076    360.450493   1126.091827  \n",
       "  \n",
       "  [408 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1687.517029  1006.741959  1690.652008  988.753754  1686.454468   \n",
       "  1    1677.585632  1002.072586  1683.447388  984.151871  1675.721893   \n",
       "  2    1676.100281  1001.824615  1678.935974  982.448921  1677.909180   \n",
       "  3    1660.661957   997.364578  1667.897766  979.559998  1658.557556   \n",
       "  4    1654.280701   994.983627  1660.452118  976.804771  1657.284729   \n",
       "  ..           ...          ...          ...         ...          ...   \n",
       "  410   354.319023   681.080826   352.842267  674.836437   353.933567   \n",
       "  411   352.888800   689.134979   351.127589  679.267456   351.354969   \n",
       "  412   351.060281   689.862906   350.411822  681.116156   349.433562   \n",
       "  413   352.263745   689.890683   359.155310  688.000000   346.295684   \n",
       "  414   348.331200   699.297741   350.532509  699.000000   346.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     990.688934  1739.353241  977.548523  1744.891663   979.503578  ...   \n",
       "  1     984.345352  1731.358765  967.424812  1749.562531   980.852562  ...   \n",
       "  2     983.798409  1728.646088  975.478073  1729.699219   973.085640  ...   \n",
       "  3     980.809647  1712.734619  968.655350  1732.802765   977.741806  ...   \n",
       "  4     978.345512  1713.733856  976.331009  1722.798706   971.701454  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  410   675.490059   342.534055  678.011395   368.492247   675.022526  ...   \n",
       "  411   678.802782   345.294576  684.385088   367.908611   682.046144  ...   \n",
       "  412   680.291924   345.907444  686.831636   361.371305   685.981077  ...   \n",
       "  413   688.000000   371.465771  688.000000   342.068170   689.764584  ...   \n",
       "  414   699.000000   354.032637  699.000000   357.217430   699.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1717.430847  1277.013062  1569.918259  1358.483215   1579.741684   \n",
       "  1    1713.498718  1353.997131  1602.001221  1424.676147   1563.457123   \n",
       "  2    1686.487457  1310.458282  1656.798279  1454.786743   1578.478699   \n",
       "  3    1680.529114  1305.521271  1662.035614  1476.640869   1584.512878   \n",
       "  4    1730.614624  1326.740753  1618.664703  1372.246155   1608.462616   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  410   416.837341   897.627136   377.243156  1030.899017    419.399780   \n",
       "  411   406.509071   894.972549   383.249722  1018.474030    410.209702   \n",
       "  412   391.360531   897.688477   388.127628  1024.482574    397.101723   \n",
       "  413   355.553098   899.163223   394.320690  1013.593658    359.935286   \n",
       "  414   373.917023   898.995392   375.380600  1022.241669    379.841568   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1341.049591  1619.769531  1506.265930   1620.458344   1491.979919  \n",
       "  1     1388.122437  1586.118225  1551.727722   1580.359344   1531.612610  \n",
       "  2     1402.194885  1636.302307  1575.847107   1557.865982   1548.836243  \n",
       "  3     1433.640442  1614.931061  1597.664795   1548.265457   1580.891113  \n",
       "  4     1347.464325  1551.581314  1614.761292   1543.404785   1599.060486  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  410   1019.508942   421.950996  1141.685150    429.841118   1131.110321  \n",
       "  411   1012.655212   390.642319  1119.027313    411.561394   1108.872650  \n",
       "  412   1022.581268   399.833244  1132.848389    405.468628   1131.238678  \n",
       "  413   1013.527710   397.877850  1108.366669    368.404205   1109.597778  \n",
       "  414   1020.598877   379.761635  1140.450287    382.560520   1138.548767  \n",
       "  \n",
       "  [415 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1661.523987  1191.430923  1671.515503  1178.503670  1665.439148   \n",
       "  1    1699.838776  1183.386452  1705.711243  1171.147934  1704.070435   \n",
       "  2    1651.041718  1194.812836  1665.943390  1178.018318  1648.724060   \n",
       "  3    1621.379150  1183.077538  1631.458466  1171.117867  1626.281647   \n",
       "  4    1610.149841  1194.611496  1616.481567  1181.544617  1615.215088   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  383   350.807175   963.459082   347.148376   957.866503   351.379795   \n",
       "  384   363.257103  1004.460438   362.853237   995.899559   362.251812   \n",
       "  385   357.624104   998.888260   358.753441   990.764355   355.328827   \n",
       "  386   350.053806  1010.117111   354.708622  1001.107224   349.844774   \n",
       "  387   339.722620  1012.697964   338.012262  1002.901127   340.202719   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1175.100883  1711.979401  1184.496162  1694.607727  1172.774963  ...   \n",
       "  1    1168.791550  1712.787598  1176.836998  1694.320404  1173.767540  ...   \n",
       "  2    1175.574135  1717.868744  1173.429306  1652.215118  1172.450890  ...   \n",
       "  3    1166.988602  1678.696198  1176.310493  1701.416748  1162.031860  ...   \n",
       "  4    1179.394745  1660.962646  1183.234825  1689.946075  1172.216362  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  383   957.890203   323.418743   962.851154   354.408134   959.687162  ...   \n",
       "  384   995.247131   354.180981   995.845932   369.001362   997.807201  ...   \n",
       "  385   990.052750   353.228909   996.658997   350.026222   993.094357  ...   \n",
       "  386   999.693245   365.525696  1006.979576   357.129787  1007.198166  ...   \n",
       "  387  1002.332863   329.941725  1004.838989   338.951714   998.307224  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1645.441864  1604.227905  1638.398071  1656.717896   1580.561340   \n",
       "  1    1650.424103  1490.529144  1611.618469  1542.075592   1568.697662   \n",
       "  2    1653.443848  1482.809540  1618.878021  1546.272949   1568.659790   \n",
       "  3    1634.457001  1448.017181  1641.643280  1598.499939   1570.817596   \n",
       "  4    1649.496704  1472.185791  1610.897461  1589.562683   1574.758240   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  383   377.894783  1200.206879   351.721039  1348.602631    371.074982   \n",
       "  384   363.698952  1193.828293   352.436653  1308.180969    361.357452   \n",
       "  385   352.091030  1202.980911   344.249500  1332.370880    348.624781   \n",
       "  386   352.212414  1206.695312   339.252209  1346.265015    344.615021   \n",
       "  387   346.855799  1202.320236   328.602998  1344.114777    337.386250   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1617.185425  1618.305237  1690.915039   1559.955566   1678.191040  \n",
       "  1     1519.606873  1579.518066  1687.993164   1559.601257   1667.347351  \n",
       "  2     1532.600342  1592.217346  1701.376099   1558.756195   1684.802124  \n",
       "  3     1566.236511  1625.044037  1731.379639   1546.795364   1705.781860  \n",
       "  4     1571.642517  1569.288391  1742.086548   1547.014236   1727.682495  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  383   1347.855286   393.547119  1485.365723    394.607803   1482.383667  \n",
       "  384   1306.112823   369.115814  1414.163330    373.758717   1408.017639  \n",
       "  385   1331.270935   356.542027  1446.826263    357.549191   1444.596893  \n",
       "  386   1344.396851   364.354801  1462.581635    362.722271   1460.657959  \n",
       "  387   1341.997986   374.207928  1470.463501    373.400543   1467.187073  \n",
       "  \n",
       "  [388 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1692.491852  1050.536808  1698.435547  1048.488010  1693.715424   \n",
       "  1    1688.790863  1017.711308  1692.699921  1009.602836  1688.159729   \n",
       "  2    1687.452850   958.505833  1698.202759   946.605770  1695.078278   \n",
       "  3    1679.952423   964.017120  1688.307343   953.108246  1686.407990   \n",
       "  4    1667.271942   965.748169  1675.635040   954.318619  1673.274506   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  495   332.024290   740.025703   332.417131   731.107185   329.286635   \n",
       "  496   331.485907   738.142143   331.589752   729.514513   328.068146   \n",
       "  497   328.286877   738.476866   331.427551   731.405018   326.260486   \n",
       "  498   329.824327   734.748362   331.511366   726.996029   326.546242   \n",
       "  499   336.317469   740.028950   335.836618   731.791508   333.296846   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1048.270100  1707.499573  1046.532330  1709.621979  1043.460880  ...   \n",
       "  1    1011.838702  1702.289856  1012.870418  1709.707947  1018.472735  ...   \n",
       "  2     945.870228  1730.186218   967.459785  1723.627563   960.820576  ...   \n",
       "  3     952.081772  1718.453003   968.495590  1735.576660   960.664268  ...   \n",
       "  4     953.682449  1706.657562   966.701347  1730.634796   962.009674  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  495   731.171595   328.922302   741.201805   337.250700   738.865574  ...   \n",
       "  496   729.696898   326.442747   738.014620   333.547199   737.267305  ...   \n",
       "  497   731.225571   331.253366   742.239925   338.889956   741.624416  ...   \n",
       "  498   726.633089   330.786320   736.133183   334.000797   733.941170  ...   \n",
       "  499   731.601248   327.813457   741.560333   332.089510   740.650105  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1686.587982  1215.333557  1662.857132  1260.482635   1676.276672   \n",
       "  1    1698.163300  1169.816391  1673.895935  1313.041901   1662.854706   \n",
       "  2    1721.752930  1237.253418  1667.294434  1336.878265   1648.430084   \n",
       "  3    1741.840088  1239.303284  1655.948822  1337.407166   1652.750824   \n",
       "  4    1735.469330  1262.116638  1648.517487  1352.016113   1647.008392   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  495   351.613667   988.951996   352.489223  1121.794739    382.286659   \n",
       "  496   354.543568   985.941589   344.013432  1116.183502    379.830544   \n",
       "  497   355.912003   981.939545   347.495548  1110.922241    372.692669   \n",
       "  498   348.205063   984.385681   349.388046  1114.387970    352.357971   \n",
       "  499   353.986481   984.621735   333.101665  1115.899567    360.740231   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1294.242035  1561.192307  1476.995239   1595.391220   1490.027130  \n",
       "  1     1308.228271  1566.586502  1502.320953   1562.462967   1496.795685  \n",
       "  2     1329.515167  1571.315872  1518.173584   1571.622101   1519.276855  \n",
       "  3     1332.784821  1566.621445  1530.169373   1568.168030   1529.143799  \n",
       "  4     1341.638794  1580.938721  1536.915222   1586.058945   1530.790161  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  495   1120.801025   357.604218  1263.216370    391.607956   1263.092529  \n",
       "  496   1115.893250   351.202106  1253.586731    379.052216   1254.377014  \n",
       "  497   1112.160461   354.227936  1248.008911    369.430580   1248.473389  \n",
       "  498   1114.442871   350.889034  1252.177551    351.475796   1252.185730  \n",
       "  499   1116.846832   338.860159  1252.466187    358.707245   1253.145386  \n",
       "  \n",
       "  [500 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1672.815582  952.373550  1684.984619  941.038040  1673.228897   \n",
       "  1    1667.042007  962.500381  1677.103943  950.144928  1670.397339   \n",
       "  2    1661.399841  944.848816  1670.128723  933.270172  1667.022430   \n",
       "  3    1661.554428  955.524475  1666.216187  941.528595  1666.404083   \n",
       "  4    1658.340683  965.913971  1660.179962  949.563652  1660.430115   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  369   316.893569  745.854034   316.851212  737.892750   320.519680   \n",
       "  370   312.846748  754.154617   313.314705  743.173576   312.889301   \n",
       "  371   311.245973  751.327362   313.064747  740.833988   311.329444   \n",
       "  372   311.655091  733.822714   313.372003  725.834795   311.929261   \n",
       "  373   319.564758  752.061260   320.238075  742.065186   318.406892   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     938.371826  1728.416351  937.205658  1696.831879   926.848038  ...   \n",
       "  1     947.960403  1722.936768  946.571510  1703.893616   941.674576  ...   \n",
       "  2     931.779739  1706.302704  941.876587  1715.452301   936.170181  ...   \n",
       "  3     940.881424  1694.738220  945.153900  1716.164490   941.689819  ...   \n",
       "  4     951.117203  1690.475281  940.186485  1715.640442   944.557213  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  369   738.356098   317.407126  746.657093   348.597149   744.233585  ...   \n",
       "  370   744.215523   320.774502  739.572433   335.957090   737.423225  ...   \n",
       "  371   741.551235   318.143840  742.469364   327.080967   743.509220  ...   \n",
       "  372   726.139221   312.562379  735.248304   320.946944   736.812166  ...   \n",
       "  373   741.824049   312.000000  742.972979   328.954733   761.582973  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1694.142090  1134.168365  1641.399780  1305.417969   1643.496536   \n",
       "  1    1712.119781  1177.629181  1651.719376  1297.604492   1645.986298   \n",
       "  2    1727.185028  1257.797241  1630.553909  1331.167236   1622.802200   \n",
       "  3    1741.873077  1250.969696  1634.225067  1323.649445   1636.328766   \n",
       "  4    1724.392334  1275.448761  1639.645569  1352.225525   1622.263168   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  369   370.210537   983.869934   382.629646  1128.666229    386.984352   \n",
       "  370   359.042606   987.884979   372.154366  1135.873260    368.587185   \n",
       "  371   348.953064  1001.054626   355.181458  1152.923340    354.369968   \n",
       "  372   340.399794   999.672913   341.955881  1152.701447    340.027166   \n",
       "  373   329.051226   989.352112   329.830000  1144.435608    330.522493   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1295.443787  1516.962112  1493.321411   1527.039085   1482.767883  \n",
       "  1     1286.866241  1523.569794  1501.890259   1522.563820   1497.899231  \n",
       "  2     1322.760254  1528.914574  1517.191284   1520.995476   1519.403992  \n",
       "  3     1317.361145  1528.865829  1532.851746   1521.702957   1533.369324  \n",
       "  4     1337.872559  1540.089767  1546.038208   1527.949394   1548.851379  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  369   1123.955200   476.328354  1254.260315    459.432938   1244.457642  \n",
       "  370   1125.344604   462.731995  1234.225098    409.568085   1217.005066  \n",
       "  371   1143.247162   447.668198  1228.041748    438.348068   1223.399780  \n",
       "  372   1139.589386   440.348846  1230.529724    430.714462   1226.458008  \n",
       "  373   1133.950500   418.891083  1234.330200    412.907158   1231.559631  \n",
       "  \n",
       "  [374 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1603.134430  1051.093231  1606.538879  1037.696922  1603.139450   \n",
       "  1    1584.621902  1052.258224  1587.489868  1037.658905  1586.024612   \n",
       "  2    1581.291489  1041.243790  1584.605438  1028.236664  1586.030991   \n",
       "  3    1549.533127  1032.151436  1556.515396  1019.020508  1553.185486   \n",
       "  4    1556.509857  1040.210266  1558.030243  1027.697639  1559.124725   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  381   381.610243   736.791786   382.139055   726.173283   382.288932   \n",
       "  382   388.567092   795.034172   387.200845   785.477959   389.157570   \n",
       "  383   374.000000   787.111183   374.000000   775.685432   374.000000   \n",
       "  384   373.000000   794.057198   373.781773   784.140884   373.000000   \n",
       "  385   373.000000   795.767624   373.000000   784.105137   373.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1037.059433  1641.966446  1028.219978  1640.787079  1028.694748  ...   \n",
       "  1    1038.089676  1624.401215  1036.242065  1650.895660  1032.318741  ...   \n",
       "  2    1027.969894  1609.539261  1035.217026  1655.197693  1026.420769  ...   \n",
       "  3    1019.940475  1589.923157  1030.925819  1624.295349  1019.690437  ...   \n",
       "  4    1028.848312  1578.582764  1033.531441  1630.891144  1022.500404  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  381   725.250992   384.466108   732.508907   398.244833   730.693871  ...   \n",
       "  382   784.547432   384.471142   787.662514   398.177177   786.513130  ...   \n",
       "  383   775.520546   375.141797   773.270161   391.187122   774.789291  ...   \n",
       "  384   784.576145   376.773025   785.215130   392.864689   787.054199  ...   \n",
       "  385   784.505095   373.000000   783.309685   388.252187   784.283443  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1638.447617  1338.748871  1586.200806  1429.219086   1577.600037   \n",
       "  1    1658.783173  1336.324036  1556.712173  1385.161835   1555.123245   \n",
       "  2    1628.786118  1361.590790  1625.889618  1521.032104   1593.391617   \n",
       "  3    1617.189621  1357.356140  1654.117676  1526.207825   1571.267700   \n",
       "  4    1617.269012  1359.661346  1668.618286  1551.101135   1592.467560   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  381   411.824326   979.634460   408.069765  1117.243378    425.170681   \n",
       "  382   410.562084   982.064758   399.579552  1117.307037    420.706482   \n",
       "  383   393.254623   984.316132   397.612736  1112.087494    409.518322   \n",
       "  384   392.202494   993.126709   397.948935  1116.334473    404.477108   \n",
       "  385   386.713579   986.023285   391.173401  1110.802460    397.888811   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1431.810730  1497.738464  1530.785095   1482.401695   1533.663147  \n",
       "  1     1379.692322  1499.948090  1545.830078   1495.537231   1543.157776  \n",
       "  2     1516.409607  1520.369522  1573.373291   1494.285263   1562.526978  \n",
       "  3     1505.013000  1570.394806  1602.388550   1504.846039   1586.036438  \n",
       "  4     1539.497620  1588.447662  1613.125244   1517.630768   1599.819519  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  381   1112.788483   461.686432  1233.755005    460.883354   1229.614563  \n",
       "  382   1111.570099   454.604034  1225.678040    455.069107   1222.084595  \n",
       "  383   1105.648438   443.213287  1212.925781    443.037971   1210.561920  \n",
       "  384   1112.852509   438.190872  1215.176849    437.688049   1215.028717  \n",
       "  385   1105.320160   429.722321  1216.993713    428.985416   1212.674103  \n",
       "  \n",
       "  [386 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2251.707870  873.736526  2249.070061  855.135757  2250.989952   \n",
       "  1    2255.854652  897.891846  2252.619064  880.060318  2255.371910   \n",
       "  2    2255.965958  897.712692  2255.650665  880.173401  2258.382561   \n",
       "  3    2261.497261  908.792892  2260.679855  890.056992  2265.565895   \n",
       "  4    2260.853668  922.196487  2260.641228  902.848389  2265.337219   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  162  3483.994797  624.714668  3478.131500  617.143410  3488.393219   \n",
       "  163  3460.291550  625.652477  3457.669350  618.427395  3464.324730   \n",
       "  164  3456.694618  624.749111  3457.024513  618.275688  3459.966286   \n",
       "  165  3459.382217  621.192703  3461.684662  613.978664  3462.603638   \n",
       "  166  3476.540054  623.385696  3477.936905  614.149963  3478.756393   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     856.212601  2179.348766  835.444366  2205.936752   841.714142  ...   \n",
       "  1     877.626785  2209.224060  859.912018  2228.054359   850.192322  ...   \n",
       "  2     876.474823  2196.767475  857.467255  2234.881973   852.456726  ...   \n",
       "  3     886.679779  2202.539989  857.339172  2245.007668   849.053452  ...   \n",
       "  4     900.749542  2204.264145  863.180099  2246.822311   860.773811  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  162   615.407387  3431.049416  618.606178  3492.749680   619.674706  ...   \n",
       "  163   617.636143  3439.061493  619.981667  3492.394562   624.634247  ...   \n",
       "  164   617.363049  3453.269791  622.461040  3493.906082   622.811607  ...   \n",
       "  165   613.431200  3463.688362  620.052216  3495.610413   621.514282  ...   \n",
       "  166   613.075302  3478.317253  619.440350  3490.116348   619.588928  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2213.942146  1263.615540  2283.005051  1389.841919   2271.334503   \n",
       "  1    2205.087852  1222.490601  2300.123749  1224.586609   2278.186111   \n",
       "  2    2194.609680  1264.383667  2287.270050  1223.210815   2261.823280   \n",
       "  3    2192.254345  1262.158203  2306.949341  1240.138916   2288.339050   \n",
       "  4    2207.796692  1254.138336  2317.828186  1203.918365   2309.611908   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  162  3479.348404   863.791626  3406.217667  1000.604065   3473.027611   \n",
       "  163  3478.327606   859.651978  3403.013744   989.432373   3461.987312   \n",
       "  164  3487.824242   849.297180  3418.690422   979.085114   3462.131310   \n",
       "  165  3492.371658   861.621368  3426.551018   982.020599   3473.701950   \n",
       "  166  3478.927437   863.050171  3446.996696   984.009674   3452.909309   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1401.661987  2282.297638  1401.147278   2268.159454   1396.836426  \n",
       "  1     1241.261230  2337.834320  1357.360107   2320.398514   1370.166138  \n",
       "  2     1251.120544  2336.643753  1342.519897   2325.550385   1354.947876  \n",
       "  3     1290.108948  2374.884125  1406.676941   2364.950623   1419.928589  \n",
       "  4     1269.541748  2412.062958  1476.708923   2414.861908   1514.771240  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  162    998.092041  3399.132973  1130.288879   3450.288185   1127.151489  \n",
       "  163    987.365387  3396.340717  1119.720581   3442.310860   1117.234192  \n",
       "  164    978.104095  3394.098408  1112.868408   3422.705727   1113.440613  \n",
       "  165    983.616364  3395.539364  1110.600403   3434.764458   1113.292969  \n",
       "  166    987.142059  3404.393452  1096.162720   3406.794659   1099.113525  \n",
       "  \n",
       "  [167 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2275.570297  895.591278  2274.227203  878.621399  2276.921844   \n",
       "  1    2304.655258  911.162109  2305.345840  895.317947  2306.882675   \n",
       "  2    2331.748779  929.526978  2335.678436  912.991165  2331.247375   \n",
       "  3    2340.207504  931.854858  2345.134933  914.264786  2336.706436   \n",
       "  4    2365.591125  942.360626  2372.273499  925.104492  2361.815674   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  122  3417.268478  539.020477  3414.230911  529.420933  3418.788055   \n",
       "  123  3430.636200  549.411705  3427.428604  537.003384  3426.308304   \n",
       "  124  3434.684982  553.432510  3432.858459  543.696918  3435.211685   \n",
       "  125  3390.106590  535.043709  3398.319237  528.425789  3389.039711   \n",
       "  126  3443.000000  533.655685  3443.000000  526.062351  3443.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     874.978310  2203.183220  865.795937  2253.458488   860.851822  ...   \n",
       "  1     891.596527  2245.656815  874.024628  2277.569946   869.559181  ...   \n",
       "  2     908.836456  2312.094131  893.963791  2308.067978   876.779678  ...   \n",
       "  3     910.764664  2323.238281  899.498840  2300.168060   875.195572  ...   \n",
       "  4     921.878616  2332.645309  906.673355  2312.051361   886.411217  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  122   529.110798  3351.016739  524.189919  3407.089127   530.983593  ...   \n",
       "  123   537.711254  3373.053581  538.814896  3397.647301   536.558559  ...   \n",
       "  124   542.059753  3386.403488  535.932007  3418.943817   534.983002  ...   \n",
       "  125   526.604836  3415.561447  527.524048  3399.169373   528.047806  ...   \n",
       "  126   525.339417  3443.000000  523.710407  3419.849800   523.021984  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2217.013725  1249.584961  2317.049393  1363.728638   2303.480240   \n",
       "  1    2215.030045  1228.866608  2321.524307  1340.813660   2294.695007   \n",
       "  2    2240.618881  1258.248779  2325.168991  1379.683594   2295.967316   \n",
       "  3    2234.118904  1257.337921  2323.589600  1389.479858   2297.047058   \n",
       "  4    2209.215031  1251.283722  2314.761078  1414.996704   2286.530045   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  122  3391.653633   775.873260  3364.092453   902.137146   3378.502541   \n",
       "  123  3392.755508   779.560150  3372.542770   913.004150   3379.920471   \n",
       "  124  3407.180504   768.265533  3385.556526   908.297821   3395.612862   \n",
       "  125  3394.263214   764.834503  3392.720909   902.271759   3394.178642   \n",
       "  126  3421.979950   747.875275  3414.416237   891.769653   3416.863014   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1414.679260  2377.553650  1567.677307   2339.033447   1609.050171  \n",
       "  1     1399.255066  2388.431702  1502.135254   2366.403534   1608.605652  \n",
       "  2     1438.617920  2382.122940  1553.614807   2343.877014   1616.618591  \n",
       "  3     1450.398071  2379.290527  1565.014832   2353.961945   1618.800293  \n",
       "  4     1467.324341  2376.949829  1579.863220   2361.195038   1619.895874  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  122    908.667633  3361.976357  1028.255005   3333.460701   1035.838379  \n",
       "  123    928.665344  3332.680508  1039.777588   3319.686089   1041.207031  \n",
       "  124    910.154480  3328.766609  1040.585388   3323.156708   1042.675232  \n",
       "  125    903.412415  3334.322094  1031.639160   3332.060745   1033.757812  \n",
       "  126    895.129395  3337.388912  1030.437012   3337.937160   1034.395508  \n",
       "  \n",
       "  [127 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    2058.654572  1026.980576  2061.515060  1015.580383  2051.932076   \n",
       "  1    2025.034241  1145.148376  2028.897354  1128.720612  2019.554802   \n",
       "  2    2117.822632   995.980667  2124.471710   982.652725  2104.636040   \n",
       "  3    2169.884460   991.102173  2169.029617   976.757683  2157.453384   \n",
       "  4    2075.337601  1052.965515  2079.489479  1045.600998  2068.372353   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  120  3514.937286   691.552376  3508.830734   679.906448  3513.791290   \n",
       "  121  3532.871597   676.163448  3527.624390   665.848042  3528.377945   \n",
       "  122  3537.963867   673.686874  3537.843964   661.949577  3535.734024   \n",
       "  123  3543.824860   667.518559  3544.000000   656.892064  3539.197075   \n",
       "  124  3538.000000   728.148354  3538.000000   714.886597  3538.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1017.497223  2045.358093  1001.763885  2050.309959   999.757957  ...   \n",
       "  1    1131.189499  2017.909473  1125.763031  2035.003460  1086.720779  ...   \n",
       "  2     985.516899  2104.151894   969.572197  2067.881294   978.690399  ...   \n",
       "  3     981.170723  2121.547974   971.000420  2102.399055   985.922668  ...   \n",
       "  4    1048.898102  2051.217003  1035.193130  2063.260666  1036.627411  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  120   680.107967  3443.226952   685.821613  3503.931625   681.369209  ...   \n",
       "  121   665.811211  3469.066055   675.674053  3505.874664   675.202232  ...   \n",
       "  122   661.912846  3508.859436   662.541180  3515.728081   668.276821  ...   \n",
       "  123   657.538677  3538.944763   655.595950  3535.679428   671.072268  ...   \n",
       "  124   715.607132  3538.000000   708.913233  3538.000000   702.720766  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2020.791496  1260.918579  2105.793159  1254.789124   2091.007973   \n",
       "  1    2041.942787  1272.121063  2129.047195  1290.941467   2106.780670   \n",
       "  2    2040.914238  1272.109467  2157.235992  1351.704010   2147.253723   \n",
       "  3    2053.197113  1275.998505  2152.292801  1356.034027   2139.118073   \n",
       "  4    2032.497391  1255.222473  2144.664795  1301.234406   2128.391800   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  120  3485.042747   949.925049  3438.303062  1088.646057   3472.060249   \n",
       "  121  3482.624619   920.038208  3455.910217  1060.589966   3473.781998   \n",
       "  122  3497.331665   909.761261  3479.046989  1036.151703   3484.658760   \n",
       "  123  3508.174210   891.831024  3500.346718  1038.510315   3508.067940   \n",
       "  124  3519.469002   887.352097  3498.083115  1050.445251   3509.756950   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1269.275146  2229.983231  1514.352295   2231.893600   1520.588562  \n",
       "  1     1299.793579  2244.227783  1551.923889   2236.742218   1563.992371  \n",
       "  2     1374.182861  2246.868347  1572.791992   2212.339722   1599.842590  \n",
       "  3     1376.819977  2237.253708  1575.714600   2215.804352   1602.358032  \n",
       "  4     1321.116638  2225.826279  1577.086060   2206.663132   1602.148438  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  120   1090.612213  3414.482719  1224.873169   3450.267769   1224.825623  \n",
       "  121   1062.340210  3441.774635  1205.138611   3456.126892   1208.722046  \n",
       "  122   1039.734497  3470.765427  1176.610840   3467.414330   1181.461670  \n",
       "  123   1043.470551  3463.060551  1200.627625   3469.202923   1206.391113  \n",
       "  124   1055.041168  3458.773304  1203.748108   3483.980629   1210.539429  \n",
       "  \n",
       "  [125 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2156.773727  914.598770  2154.837830  901.474915  2154.634705   \n",
       "  1    2186.091362  906.538345  2181.860947  892.034424  2186.941689   \n",
       "  2    2197.393112  918.637131  2196.465515  904.643036  2201.329620   \n",
       "  3    2228.671204  916.980629  2227.512421  902.077629  2230.819550   \n",
       "  4    2246.998886  918.113678  2248.128281  902.396942  2247.514069   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  121  3363.766197  521.760021  3360.921883  513.340153  3370.862076   \n",
       "  122  3374.370026  514.999615  3374.345703  509.126736  3378.348068   \n",
       "  123  3363.969208  517.171322  3364.423279  509.453621  3369.548943   \n",
       "  124  3386.258499  523.406685  3386.690086  515.720230  3387.245598   \n",
       "  125  3397.847656  539.479847  3398.000710  531.079355  3398.567505   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     900.276031  2119.097820  885.965927  2132.123077   883.262650  ...   \n",
       "  1     891.767136  2104.972183  879.135719  2163.605576   888.986237  ...   \n",
       "  2     902.614830  2127.850845  882.365654  2181.759750   892.168098  ...   \n",
       "  3     903.188835  2149.061920  873.948326  2206.759979   895.504509  ...   \n",
       "  4     903.666847  2167.638763  873.662628  2217.840103   898.288986  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  121   514.071140  3339.552971  513.993061  3393.474411   523.260475  ...   \n",
       "  122   508.095463  3365.649918  512.956097  3410.429535   513.141502  ...   \n",
       "  123   509.123869  3365.447060  512.599474  3412.385925   512.409510  ...   \n",
       "  124   514.592014  3379.678047  520.023136  3416.352264   516.985258  ...   \n",
       "  125   530.076195  3389.159622  530.541050  3409.201073   531.816229  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2124.028748  1164.859985  2207.336945  1205.687439   2192.671097   \n",
       "  1    2109.497776  1198.047638  2213.461365  1231.112671   2207.969467   \n",
       "  2    2079.787796  1233.105225  2213.863098  1253.638519   2123.184826   \n",
       "  3    2087.598053  1212.482056  2221.284698  1260.659546   2118.591267   \n",
       "  4    2091.504105  1197.075134  2225.873764  1256.230164   2122.915691   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  121  3364.574333   740.997131  3301.350033   871.429840   3363.647324   \n",
       "  122  3402.815353   756.103821  3348.617683   890.850403   3386.998238   \n",
       "  123  3407.677544   753.182709  3351.272255   888.557556   3389.997078   \n",
       "  124  3411.289909   754.075073  3368.790756   881.028748   3395.959305   \n",
       "  125  3401.019730   753.165237  3372.103191   876.202301   3390.165985   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1227.346436  2280.096512  1385.506958   2277.998291   1393.605896  \n",
       "  1     1235.025177  2244.888351  1443.982117   2263.496170   1447.655579  \n",
       "  2     1368.809753  2230.814438  1472.402893   2130.882988   1518.853333  \n",
       "  3     1415.711487  2237.456360  1471.232361   2133.164757   1526.677002  \n",
       "  4     1404.689087  2239.015457  1466.066956   2128.627502   1539.633728  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  121    873.471436  3292.062252  1003.314392   3364.102150   1009.477844  \n",
       "  122    890.504944  3336.232071  1022.993286   3362.947365   1020.264404  \n",
       "  123    888.209808  3344.237972  1014.442871   3358.475754   1015.374390  \n",
       "  124    884.111725  3359.941540  1004.621887   3374.291832   1008.131561  \n",
       "  125    877.689270  3358.765327   995.130676   3367.587265    997.953400  \n",
       "  \n",
       "  [126 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    2229.720055   974.265518  2234.493088   962.856453  2235.988785   \n",
       "  1    2265.671127   995.194382  2273.111115   984.722900  2274.204376   \n",
       "  2    2323.562363  1018.125748  2330.508698  1006.754211  2331.774094   \n",
       "  3    2404.762360  1025.801239  2414.873871  1011.502777  2407.038788   \n",
       "  4    2421.833862  1026.659149  2433.523529  1012.080688  2423.986908   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  96   3430.183899   598.219772  3430.084763   587.545776  3436.049011   \n",
       "  97   3442.317719   582.804176  3442.085327   571.818344  3445.330429   \n",
       "  98   3429.751572   559.133701  3427.441116   551.481556  3433.712067   \n",
       "  99   3428.975662   583.089169  3429.388977   574.509296  3434.279739   \n",
       "  100  3448.446854   587.592258  3447.954514   578.073635  3451.478683   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     957.820114  2191.778641  923.316078  2218.348839   923.345963  ...   \n",
       "  1     978.981476  2228.715065  941.889763  2253.379425   945.327225  ...   \n",
       "  2    1001.221130  2275.684479  965.564407  2309.134766   963.161575  ...   \n",
       "  3    1007.818130  2360.846283  977.583908  2361.297516   973.168900  ...   \n",
       "  4    1011.058167  2380.317657  966.549149  2376.746277   974.655426  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  96    585.745506  3367.522614  564.173626  3424.613647   576.073692  ...   \n",
       "  97    570.896904  3386.342331  557.590626  3434.249512   564.569847  ...   \n",
       "  98    550.823227  3403.773857  548.043510  3452.180695   552.149700  ...   \n",
       "  99    571.794151  3424.444275  572.673309  3452.818253   578.055923  ...   \n",
       "  100   577.111799  3442.660210  576.651747  3453.312576   578.671040  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2152.463287  1250.547760  2219.883301  1323.704742   2161.366001   \n",
       "  1    2145.228807  1216.009216  2211.072556  1319.990021   2158.890877   \n",
       "  2    2141.299858  1175.643555  2232.044037  1323.988922   2208.674301   \n",
       "  3    2124.793850  1111.165466  2214.889572  1269.989258   2205.935669   \n",
       "  4    2126.682030  1119.263214  2218.917786  1286.558472   2223.100464   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  96   3370.333511   820.699036  3295.355659   944.399689   3360.519150   \n",
       "  97   3390.277435   808.746368  3343.032402   941.838715   3380.584442   \n",
       "  98   3443.439331   798.146820  3372.324036   936.087830   3439.072296   \n",
       "  99   3436.015366   791.308884  3401.919197   912.703918   3427.148987   \n",
       "  100  3442.057144   831.202972  3432.804359   960.380554   3443.705063   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1429.333313  2302.254272  1469.075989   2273.931961   1520.178040  \n",
       "  1     1442.216553  2296.691315  1502.757690   2262.354401   1556.377441  \n",
       "  2     1348.556274  2305.579834  1474.658508   2282.896820   1528.703979  \n",
       "  3     1274.349091  2276.597870  1491.403748   2275.781937   1512.917175  \n",
       "  4     1288.751465  2255.246384  1511.377747   2274.285385   1519.854431  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  96     963.885010  3253.563251  1058.309204   3331.174187   1103.207458  \n",
       "  97     949.354706  3317.276199  1073.397766   3333.272743   1089.469299  \n",
       "  98     934.361847  3355.142914  1074.887268   3413.055878   1072.068787  \n",
       "  99     912.279968  3387.390793  1049.342285   3394.129395   1045.862488  \n",
       "  100    961.889465  3421.570076  1064.949768   3427.581398   1065.938782  \n",
       "  \n",
       "  [101 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1648.956848  1115.821320  1645.449280  1099.341461  1647.315765   \n",
       "  1    1638.398438  1112.729149  1639.494537  1094.499237  1639.182587   \n",
       "  2    1646.829346  1115.051849  1644.393188  1094.179131  1649.290619   \n",
       "  3    1645.405975  1112.936356  1640.422058  1094.270515  1642.195374   \n",
       "  4    1629.132446  1108.642075  1632.549500  1088.630157  1628.461609   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  466   321.576375   700.829221   320.145843   697.000000   320.951482   \n",
       "  467   325.826870   702.890020   324.237310   701.000000   325.473566   \n",
       "  468   323.298618   707.000000   322.376784   707.000000   322.543011   \n",
       "  469   327.036260   714.000000   326.289026   714.000000   326.414968   \n",
       "  470   327.289377   714.834690   327.019462   709.692091   327.496284   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1101.141449  1692.211884  1074.787117  1707.909271  1074.772087  ...   \n",
       "  1    1097.377457  1683.188080  1074.839828  1719.959686  1079.584991  ...   \n",
       "  2    1097.544518  1679.591644  1069.433167  1711.220367  1077.233444  ...   \n",
       "  3    1097.314804  1687.362366  1068.184242  1688.792786  1071.506622  ...   \n",
       "  4    1089.358383  1674.909790  1066.206963  1694.291718  1074.576454  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  466   697.000000   312.000000   697.000000   325.116085   698.334406  ...   \n",
       "  467   701.000000   315.000000   701.000000   327.048100   701.000000  ...   \n",
       "  468   707.000000   313.000000   707.000000   325.252572   707.000000  ...   \n",
       "  469   714.000000   317.035524   714.000000   327.045507   714.000000  ...   \n",
       "  470   708.967802   320.000000   707.551382   322.509681   709.198204  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1685.485352  1354.444153  1580.727615  1285.705963   1583.112808   \n",
       "  1    1678.879944  1462.823914  1576.273285  1439.650238   1558.329727   \n",
       "  2    1729.936340  1417.560699  1596.533142  1329.244110   1598.028107   \n",
       "  3    1647.797577  1476.867584  1641.326721  1572.587891   1616.426239   \n",
       "  4    1690.091522  1403.284058  1573.688141  1354.874329   1570.682098   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  466   337.349489   924.389984   326.331505  1050.312988    335.035873   \n",
       "  467   329.605562   920.259064   325.275726  1050.093567    332.255032   \n",
       "  468   325.872648   920.450165   317.942883  1049.444427    324.528634   \n",
       "  469   322.308447   923.270065   317.000000  1055.159180    326.463335   \n",
       "  470   320.000000   907.523834   321.887796   971.600464    325.681360   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1270.774750  1495.049957  1340.142365   1504.387772   1317.496185  \n",
       "  1     1408.383087  1569.736679  1497.557770   1560.609314   1477.330933  \n",
       "  2     1293.751221  1519.322571  1524.893799   1530.259521   1508.857056  \n",
       "  3     1524.640137  1591.199829  1571.313110   1561.367340   1542.359985  \n",
       "  4     1306.014038  1506.849182  1571.558411   1514.457245   1547.843262  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  466   1046.957855   366.308228  1165.529175    367.588501   1163.193665  \n",
       "  467   1047.429077   359.870003  1161.207611    359.623119   1159.871643  \n",
       "  468   1046.822083   352.447197  1160.040497    351.386093   1158.374908  \n",
       "  469   1052.542328   335.752621  1169.193695    343.043552   1168.501007  \n",
       "  470    968.912231   342.602575   961.065704    341.656691    958.482300  \n",
       "  \n",
       "  [471 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1620.913635  1114.790222  1617.666840  1094.298782  1616.407440   \n",
       "  1    1626.203064  1121.042099  1619.754272  1100.358475  1621.484406   \n",
       "  2    1620.123810  1120.920532  1613.661896  1098.805069  1616.509338   \n",
       "  3    1613.830078  1120.535583  1607.524780  1097.083206  1611.672241   \n",
       "  4    1697.773895  1106.378082  1696.255310  1088.507355  1698.423492   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  475   295.969501   824.000000   292.483721   824.000000   295.827272   \n",
       "  476   295.276516   843.282257   292.758957   836.065221   295.195141   \n",
       "  477   288.237411   825.082567   286.481441   825.000000   287.431987   \n",
       "  478   288.029957   819.439823   285.282042   811.628891   288.053419   \n",
       "  479   290.465761   818.230112   288.710429   809.144930   290.180814   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1099.657715  1658.441406  1066.846725  1671.922791  1078.865341  ...   \n",
       "  1    1106.593369  1644.161682  1057.962318  1650.672180  1073.289352  ...   \n",
       "  2    1104.453537  1648.132080  1069.471085  1670.199982  1079.876434  ...   \n",
       "  3    1106.073608  1643.191345  1065.206184  1667.940735  1082.449631  ...   \n",
       "  4    1090.426956  1647.528625  1060.952118  1652.693604  1072.901291  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  475   824.000000   276.351466   824.000000   292.527838   824.000000  ...   \n",
       "  476   836.052845   283.195379   833.460713   290.922146   831.154233  ...   \n",
       "  477   825.000000   278.074788   825.000000   291.180595   825.000000  ...   \n",
       "  478   811.692160   276.363150   808.748312   287.544303   803.374699  ...   \n",
       "  479   809.211254   281.614709   806.756168   287.943823   802.674011  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1676.381348  1423.438690  1633.792480  1504.434082   1628.672058   \n",
       "  1    1669.010071  1441.050262  1637.493256  1496.631531   1628.826080   \n",
       "  2    1656.625824  1450.926239  1623.435974  1445.090851   1606.604706   \n",
       "  3    1712.985382  1440.848785  1640.818787  1476.286926   1635.074005   \n",
       "  4    1628.553833  1413.304077  1595.369995  1413.381927   1610.846344   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  475   311.261620  1042.081467   279.312230  1176.012573    300.662445   \n",
       "  476   305.394020  1043.639328   284.405661  1178.011383    296.947332   \n",
       "  477   307.275928  1042.868423   275.953449  1167.558105    283.877000   \n",
       "  478   300.185316  1036.207886   275.000000  1152.397858    285.586940   \n",
       "  479   298.930202  1040.983948   282.559896  1135.814148    294.252661   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1501.105347  1622.739471  1487.144775   1612.722412   1479.804382  \n",
       "  1     1496.699646  1619.787720  1475.349304   1610.693634   1475.973267  \n",
       "  2     1441.001831  1608.417267  1431.869781   1590.586487   1428.639587  \n",
       "  3     1474.661316  1618.373993  1460.893188   1611.544525   1465.208435  \n",
       "  4     1408.804382  1542.163925  1401.379456   1576.766220   1404.405701  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  475   1173.720459   305.417225  1302.465210    318.755379   1300.719299  \n",
       "  476   1176.124695   303.061775  1306.384125    308.739067   1304.877716  \n",
       "  477   1164.435333   284.020739  1284.826019    285.111935   1280.570160  \n",
       "  478   1152.842072   275.000000  1196.803589    286.830052   1198.000000  \n",
       "  479   1135.553436   290.327965  1141.000000    300.558083   1141.000000  \n",
       "  \n",
       "  [480 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1648.678558  1081.323288  1642.927032  1063.998505  1645.645386   \n",
       "  1    1642.567078  1089.780472  1636.314819  1069.772873  1641.673401   \n",
       "  2    1720.591217  1057.171677  1720.248688  1040.882042  1717.887848   \n",
       "  3    1637.890411  1085.560440  1633.437256  1067.154266  1637.648682   \n",
       "  4    1630.002655  1088.635590  1623.734650  1071.897049  1627.912964   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  320   378.123035   794.850944   376.520920   789.239743   377.452164   \n",
       "  321   392.711674   803.264194   391.276283   798.254701   392.515896   \n",
       "  322   379.419132   799.000000   375.835150   799.000000   379.208023   \n",
       "  323   373.767546   803.786751   372.482164   803.000000   373.247147   \n",
       "  324   371.514246   813.000000   370.946465   813.000000   370.999725   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1067.926346  1676.006927  1039.234413  1700.316498  1040.050095  ...   \n",
       "  1    1077.256592  1663.353943  1034.845566  1686.525421  1044.524475  ...   \n",
       "  2    1042.929108  1675.321899  1027.970978  1666.275269  1039.065666  ...   \n",
       "  3    1073.862167  1659.908600  1038.906776  1690.767303  1045.322128  ...   \n",
       "  4    1075.290604  1659.168671  1048.535553  1681.728424  1049.532776  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  320   789.150527   355.044638   785.000000   383.982014   785.000000  ...   \n",
       "  321   798.335658   364.836603   796.000000   390.276505   796.000000  ...   \n",
       "  322   799.000000   354.742711   799.000000   377.362467   799.000000  ...   \n",
       "  323   803.000000   355.006312   803.000000   372.589706   803.000000  ...   \n",
       "  324   813.000000   354.872018   813.000000   368.374300   813.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1687.452393  1353.599548  1581.810043  1337.070282   1553.486755   \n",
       "  1    1676.885223  1389.591614  1617.581299  1402.659607   1604.087280   \n",
       "  2    1661.005859  1382.152588  1646.983276  1422.790131   1637.072327   \n",
       "  3    1672.261932  1372.500793  1598.879761  1403.971375   1598.792664   \n",
       "  4    1645.716888  1360.839935  1564.526688  1332.065857   1519.533173   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  320   400.065056  1003.017288   364.279289  1144.926849    384.575268   \n",
       "  321   389.706219   995.047791   362.932920  1144.979736    376.536055   \n",
       "  322   390.640446   996.071701   348.077505  1136.239410    368.225401   \n",
       "  323   373.404968   996.650925   346.000000  1128.380646    353.078445   \n",
       "  324   362.437077  1006.910172   347.000000  1151.449097    351.461746   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1329.527466  1565.986481  1325.783417   1557.324402   1322.263367  \n",
       "  1     1401.623627  1595.420776  1386.260376   1582.585541   1380.583832  \n",
       "  2     1424.620972  1602.070923  1408.300354   1602.407928   1409.689026  \n",
       "  3     1396.276245  1584.536133  1390.836700   1576.614151   1384.389252  \n",
       "  4     1329.158356  1559.408646  1327.165161   1542.206924   1323.812134  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  320   1145.275085   383.378132  1289.826019    390.213589   1287.459137  \n",
       "  321   1145.860596   371.611961  1296.112366    370.372320   1294.893768  \n",
       "  322   1135.083923   355.724000  1265.916870    367.010368   1267.141510  \n",
       "  323   1126.890320   346.458248  1256.268158    356.782088   1254.748505  \n",
       "  324   1150.920197   347.000000  1268.982697    351.362579   1270.780243  \n",
       "  \n",
       "  [325 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1556.953156  1055.150230  1551.629089  1038.558342  1555.317902   \n",
       "  1    1544.611252  1053.478813  1540.405807  1037.343552  1541.098679   \n",
       "  2    1546.967499  1054.697281  1542.156738  1037.212402  1543.845718   \n",
       "  3    1541.981705  1050.889435  1537.209824  1033.475121  1538.320236   \n",
       "  4    1543.911911  1049.777573  1538.610016  1031.663818  1540.640076   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  500   332.296942   795.560720   334.403305   790.983655   330.463745   \n",
       "  501   335.545376   789.000000   335.214001   789.000000   333.953290   \n",
       "  502   337.653011   792.000000   337.545509   792.000000   336.726919   \n",
       "  503   322.756963   780.235050   324.593669   774.917299   320.489200   \n",
       "  504   329.517978   782.270332   329.913972   777.980755   328.069970   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1040.279099  1575.848785  1012.529900  1595.793823  1018.538284  ...   \n",
       "  1    1038.417648  1573.708649  1008.011917  1584.552979  1012.540054  ...   \n",
       "  2    1039.010574  1573.293610  1010.576492  1587.835388  1015.583015  ...   \n",
       "  3    1034.741684  1571.977875  1008.204216  1583.728790  1012.565926  ...   \n",
       "  4    1033.842033  1567.873718  1006.267151  1583.985748  1012.996689  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  500   790.386614   333.137329   785.959867   333.272575   787.746538  ...   \n",
       "  501   789.000000   325.857858   789.000000   333.907719   789.000000  ...   \n",
       "  502   792.000000   329.286528   792.000000   333.757905   792.000000  ...   \n",
       "  503   774.394810   324.516565   773.634157   324.986760   773.869028  ...   \n",
       "  504   777.932844   324.006086   777.310514   329.279702   778.492235  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1537.863602  1391.350250  1529.645065  1548.902405   1528.131149   \n",
       "  1    1520.869263  1350.582581  1525.905518  1536.304504   1498.555664   \n",
       "  2    1541.389954  1368.719116  1509.536575  1523.331482   1508.624405   \n",
       "  3    1537.243347  1376.451324  1525.977661  1538.176758   1521.694565   \n",
       "  4    1539.852997  1372.305634  1503.596710  1491.770142   1504.874420   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  500   336.250881   988.032089   347.211826  1119.356079    341.857431   \n",
       "  501   340.254189   988.800797   336.708250  1121.012726    340.173998   \n",
       "  502   336.503372   990.553085   338.360556  1124.013062    340.456226   \n",
       "  503   323.703125   989.488968   329.343845  1102.813782    328.216746   \n",
       "  504   324.998344   979.292465   325.490661  1061.739075    333.073932   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1532.733582  1492.502930  1556.821899   1484.805801   1546.806458  \n",
       "  1     1504.652344  1462.020523  1577.922791   1436.497253   1563.895081  \n",
       "  2     1496.746399  1461.924042  1575.798645   1449.752213   1564.203979  \n",
       "  3     1513.386475  1487.928711  1581.125183   1470.926636   1569.562866  \n",
       "  4     1465.299316  1473.666870  1581.371338   1460.413895   1571.533264  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  500   1117.690002   391.928314  1239.946228    386.181076   1237.712036  \n",
       "  501   1118.561462   377.663673  1242.832062    375.662994   1242.011536  \n",
       "  502   1121.798676   365.978191  1246.188629    362.696358   1244.518188  \n",
       "  503   1100.625458   337.942011  1126.590393    332.919937   1127.930847  \n",
       "  504   1061.998077   334.243086  1062.024170    337.152830   1066.107117  \n",
       "  \n",
       "  [505 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1436.049057  1024.354370  1432.477234  1001.792923  1431.581726   \n",
       "  1    1428.814972  1030.246613  1426.109604  1005.814949  1423.733780   \n",
       "  2    1421.606567  1033.014847  1417.366653  1010.222404  1417.008728   \n",
       "  3    1414.861084  1036.799103  1410.041168  1014.057114  1409.195618   \n",
       "  4    1409.989090  1041.567459  1404.860703  1018.917610  1403.195419   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  469   356.383845   838.921391   358.791493   833.846233   353.145453   \n",
       "  470   360.986455   841.277403   362.176558   836.978745   358.882763   \n",
       "  471   361.523297   844.915346   362.752873   840.346777   359.725934   \n",
       "  472   359.935031   847.206106   361.473379   842.626990   358.175057   \n",
       "  473   354.701690   847.465309   356.588202   842.837565   352.868891   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1007.671783  1477.382507  974.630898  1466.587723   988.031982  ...   \n",
       "  1    1012.972321  1468.116409  979.936005  1462.571411   995.311813  ...   \n",
       "  2    1016.681984  1463.526169  984.778664  1455.914825   999.737305  ...   \n",
       "  3    1021.604446  1454.124237  986.196419  1444.616745  1002.849586  ...   \n",
       "  4    1026.791870  1448.473587  989.640945  1438.237656  1006.496597  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  469   833.118746   358.977112  831.448448   357.199067   828.728876  ...   \n",
       "  470   836.412239   357.977229  835.113657   361.708063   832.684772  ...   \n",
       "  471   839.904701   358.238647  838.322149   362.475798   836.045563  ...   \n",
       "  472   842.050426   358.095337  839.965831   361.408798   838.111882  ...   \n",
       "  473   842.357378   355.648470  841.153687   357.702380   839.005245  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1385.651047  1206.840057  1273.575470  1208.598267   1261.265163   \n",
       "  1    1375.016754  1205.159363  1250.203579  1209.482361   1256.551113   \n",
       "  2    1387.540695  1211.550354  1297.683891  1203.315155   1290.793686   \n",
       "  3    1379.301697  1214.810120  1244.171791  1212.917450   1241.907345   \n",
       "  4    1379.790024  1213.474976  1292.754463  1206.740723   1280.746017   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  469   364.836893  1039.302368   360.475862  1137.184387    362.286333   \n",
       "  470   365.744566  1038.944412   368.585258  1119.757202    373.814737   \n",
       "  471   363.224503  1039.549835   365.531988  1098.782776    369.685863   \n",
       "  472   358.797691  1038.717346   372.070139  1091.225098    371.069876   \n",
       "  473   360.477839  1031.763000   378.640804  1086.718750    375.597265   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1199.316925  1332.932716  1205.671173   1324.810318   1208.874298  \n",
       "  1     1195.313812  1325.599625  1203.053101   1325.103920   1204.668243  \n",
       "  2     1196.973419  1325.827705  1201.279694   1322.787102   1205.365967  \n",
       "  3     1201.440033  1306.871132  1208.845886   1292.768784   1213.931702  \n",
       "  4     1200.646606  1323.711411  1199.439484   1312.380150   1204.589661  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  469   1136.635193   365.492699  1145.748444    364.304670   1147.524445  \n",
       "  470   1120.359894   381.501507  1113.054749    380.919922   1113.939606  \n",
       "  471   1100.295929   385.109680  1092.095215    385.870125   1092.869751  \n",
       "  472   1092.238800   389.025883  1082.897446    384.705906   1084.169678  \n",
       "  473   1085.877930   390.810669  1084.698380    387.336262   1082.892685  \n",
       "  \n",
       "  [474 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1441.039749  1053.455582  1438.145996  1031.931519  1437.579697   \n",
       "  1    1429.259933  1054.793640  1427.126175  1033.511124  1426.160873   \n",
       "  2    1418.249130  1054.252960  1415.974701  1034.371567  1414.359253   \n",
       "  3    1407.496536  1056.220825  1407.479462  1036.978607  1403.830658   \n",
       "  4    1394.721588  1059.090805  1394.792038  1040.148766  1391.825333   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  394   300.762506   768.000000   296.526276   768.000000   301.106598   \n",
       "  395   293.197246   773.000000   290.086733   773.000000   293.186591   \n",
       "  396   295.038400   778.275640   292.375129   778.000000   294.706265   \n",
       "  397   285.828835   784.214618   285.165785   780.000000   284.836493   \n",
       "  398   286.246086   793.513904   286.919333   786.000000   283.274997   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1036.435165  1481.328430  1013.868286  1477.846313  1019.654694  ...   \n",
       "  1    1037.349960  1470.698593  1017.236862  1474.630829  1020.204422  ...   \n",
       "  2    1037.081161  1459.832001  1018.311516  1460.728088  1021.676613  ...   \n",
       "  3    1038.951111  1450.943970  1024.044037  1448.778214  1024.906425  ...   \n",
       "  4    1041.928535  1442.108185  1029.243317  1440.540726  1029.636734  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  394   768.000000   280.000000   768.000000   294.577274   768.000000  ...   \n",
       "  395   773.000000   278.000000   773.000000   291.041196   773.000000  ...   \n",
       "  396   778.000000   280.000000   778.000000   290.878161   778.000000  ...   \n",
       "  397   780.000000   280.000000   780.000000   290.039194   780.000000  ...   \n",
       "  398   786.000000   282.291839   789.516145   287.995922   786.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1397.192139  1248.431000  1285.611233  1250.478973   1285.708611   \n",
       "  1    1413.936264  1247.395935  1229.238944  1253.236603   1250.696997   \n",
       "  2    1420.561417  1252.655334  1238.482227  1258.093018   1263.891987   \n",
       "  3    1409.860077  1257.283997  1229.353493  1259.024841   1248.386446   \n",
       "  4    1411.789444  1266.546021  1245.360254  1273.786469   1273.828674   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  394   313.004490   990.558472   288.318501  1134.314484    322.298946   \n",
       "  395   305.408064   988.067535   301.750397  1130.429565    318.801960   \n",
       "  396   299.688831   988.066376   303.558855  1127.212616    314.785789   \n",
       "  397   289.304212   995.079926   307.870922  1127.862152    311.442474   \n",
       "  398   280.000000  1012.367432   308.035244  1138.733368    298.908146   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1239.298187  1350.612457  1242.851959   1348.191322   1245.543243  \n",
       "  1     1229.691315  1303.060303  1260.437836   1298.584061   1261.690735  \n",
       "  2     1234.820221  1305.838196  1260.781342   1310.184097   1255.969635  \n",
       "  3     1237.164276  1296.820915  1263.316650   1301.113205   1262.939301  \n",
       "  4     1257.144318  1314.200729  1264.604675   1337.706245   1264.864441  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  394   1132.814026   313.988869  1273.955505    338.173500   1273.696625  \n",
       "  395   1128.550201   333.955830  1269.261414    338.680500   1268.322968  \n",
       "  396   1125.626434   335.582470  1267.554810    337.475315   1266.071899  \n",
       "  397   1126.513123   337.144878  1264.322113    337.013092   1263.577484  \n",
       "  398   1136.974335   335.059002  1262.245972    328.171402   1259.411835  \n",
       "  \n",
       "  [399 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2168.725327  892.223709  2169.172104  870.506699  2157.234207   \n",
       "  1    2076.578571  911.299026  2080.634541  887.339142  2069.807915   \n",
       "  2    2188.729156  935.441360  2191.526413  914.618622  2183.563828   \n",
       "  3    2223.815948  941.638916  2228.145660  922.804871  2225.695770   \n",
       "  4    2244.895889  939.318985  2253.731598  922.004257  2251.117294   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  275  3523.057198  518.577812  3531.447144  510.285881  3525.887970   \n",
       "  276  3532.458611  518.975941  3540.238495  510.257877  3533.422562   \n",
       "  277  3546.271362  514.256821  3557.814026  506.137806  3545.006180   \n",
       "  278  3557.054359  516.218140  3569.445518  505.270056  3552.883308   \n",
       "  279  3578.430824  508.593828  3585.482689  500.404755  3572.731483   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     874.648468  2128.159447  856.615120  2088.923271   864.122231  ...   \n",
       "  1     892.187195  2129.664375  867.256134  2132.913033   869.128616  ...   \n",
       "  2     914.497070  2170.841629  895.076935  2141.460838   870.013092  ...   \n",
       "  3     917.301224  2176.180908  893.533386  2179.035919   879.121216  ...   \n",
       "  4     913.011063  2189.488373  885.514069  2203.728638   873.077347  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  275   509.767857  3562.488838  513.900433  3560.291519   510.551228  ...   \n",
       "  276   510.330135  3570.402206  509.843613  3560.483337   508.030228  ...   \n",
       "  277   506.117054  3584.126755  513.828484  3550.366890   512.180588  ...   \n",
       "  278   506.531734  3595.000000  510.340755  3551.594681   512.304958  ...   \n",
       "  279   500.936949  3598.000000  504.012093  3562.754913   506.206316  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2080.027077  1264.501343  2265.986542  1346.629578   2144.488083   \n",
       "  1    2105.446625  1231.974640  2261.988754  1389.233032   2129.740326   \n",
       "  2    2102.877136  1287.488647  2270.128632  1418.242676   2142.175568   \n",
       "  3    2116.653229  1254.755188  2253.855118  1408.445374   2174.930046   \n",
       "  4    2143.738121  1272.153320  2227.375122  1443.537903   2213.985397   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  275  3571.665543   729.190521  3522.126701   849.001678   3565.355537   \n",
       "  276  3566.591446   731.219727  3538.149254   850.367462   3559.715324   \n",
       "  277  3558.184807   724.095184  3552.896439   831.608276   3543.468048   \n",
       "  278  3558.862991   735.838623  3571.638588   843.303345   3540.099102   \n",
       "  279  3568.548149   724.238800  3577.771393   830.440887   3555.777596   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1432.630554  2298.225037  1575.734741   2200.945450   1607.974182  \n",
       "  1     1426.085144  2287.014709  1603.270508   2170.074081   1634.748413  \n",
       "  2     1498.143982  2285.120132  1634.608337   2188.538910   1655.553345  \n",
       "  3     1440.258057  2279.607712  1650.923279   2203.117203   1678.273987  \n",
       "  4     1468.585571  2267.601959  1678.578369   2234.295441   1704.466797  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  275    845.467682  3528.196495   931.293518   3568.237251    929.429657  \n",
       "  276    845.757812  3535.544945   930.161865   3559.750626    929.444153  \n",
       "  277    828.304779  3549.188454   920.586060   3545.840813    919.393768  \n",
       "  278    839.274353  3566.812111   925.000000   3541.334030    924.034912  \n",
       "  279    828.411835  3564.823708   915.576111   3545.439884    912.559845  \n",
       "  \n",
       "  [280 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    2139.377357  1108.892223  2146.283310  1104.337975  2124.778481   \n",
       "  1    2150.814682  1082.218906  2156.958145  1076.501404  2136.714394   \n",
       "  2    2119.646515   924.106785  2126.546570   910.794331  2100.075111   \n",
       "  3    2124.200943   927.266861  2119.004044   903.278137  2118.421616   \n",
       "  4    2132.683197   930.481949  2135.774796   912.122437  2131.681671   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  293  3409.108826   591.163315  3403.319214   580.124512  3410.617096   \n",
       "  294  3410.767105   593.430199  3405.254242   582.109753  3411.904160   \n",
       "  295  3415.409729   599.267342  3412.538132   590.151783  3420.670807   \n",
       "  296  3422.195648   599.279945  3416.434631   588.895023  3425.504761   \n",
       "  297  3428.910934   601.923637  3423.581604   591.482208  3431.811493   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1104.555412  2142.098541  1098.767487  2093.029312  1096.649925  ...   \n",
       "  1    1077.255219  2149.399597  1069.411009  2097.890152  1065.435730  ...   \n",
       "  2     914.940649  2121.328033   906.877016  2050.615280   910.419632  ...   \n",
       "  3     905.719032  2072.072655   897.050919  2079.569283   880.410278  ...   \n",
       "  4     911.045456  2080.795540   881.000336  2074.215988   879.796265  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  293   580.180008  3346.298660   578.189354  3403.466064   581.854225  ...   \n",
       "  294   582.285179  3348.462616   580.217464  3404.204544   582.172241  ...   \n",
       "  295   588.684307  3361.579025   580.256729  3416.821808   586.090633  ...   \n",
       "  296   588.878693  3368.469681   584.139282  3426.419617   590.467674  ...   \n",
       "  297   591.509338  3375.695938   587.031445  3433.108902   593.478138  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2093.049637  1247.418411  2228.613022  1330.998016   2134.150406   \n",
       "  1    2078.253174  1251.741806  2217.851807  1349.688049   2095.416519   \n",
       "  2    2066.500214  1240.361206  2216.846268  1310.911377   2175.763443   \n",
       "  3    2082.320885  1398.132263  2173.622894  1515.898682   2153.284805   \n",
       "  4    2066.039597  1318.752625  2178.475418  1490.749939   2127.459808   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  293  3383.068527   817.779022  3308.854340   949.914276   3397.182907   \n",
       "  294  3384.411194   822.934448  3310.077087   955.363495   3396.657272   \n",
       "  295  3393.330780   825.657684  3315.465652   958.324158   3403.084824   \n",
       "  296  3401.021240   830.793152  3318.825981   960.523285   3404.880493   \n",
       "  297  3405.846085   830.754395  3325.203190   956.616547   3410.540756   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1386.309509  2261.549347  1523.561951   2191.524704   1558.491028  \n",
       "  1     1417.792877  2260.806671  1536.449860   2135.384315   1572.505554  \n",
       "  2     1357.151001  2257.853333  1522.806824   2234.055847   1535.302124  \n",
       "  3     1527.402954  2233.721680  1572.575073   2200.990219   1565.728516  \n",
       "  4     1507.538940  2243.060669  1594.984741   2168.766937   1595.114075  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  293    943.067139  3313.572456  1081.966003   3385.083206   1076.533447  \n",
       "  294    952.529083  3315.190628  1084.574768   3391.108917   1081.683716  \n",
       "  295    952.969269  3316.994308  1087.460999   3400.216324   1084.129578  \n",
       "  296    961.503418  3319.281242  1085.866333   3398.453232   1083.664001  \n",
       "  297    960.763489  3320.552109  1076.398193   3398.353455   1082.364990  \n",
       "  \n",
       "  [298 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2049.192009  889.114456  2055.829124  866.903419  2027.052605   \n",
       "  1    2079.591949  803.007446  2085.204269  788.909851  2062.792786   \n",
       "  2    2056.152725  912.758301  2061.735703  893.163239  2052.073784   \n",
       "  3    2114.512680  907.551224  2110.219986  885.064316  2109.319931   \n",
       "  4    2111.612885  939.300781  2114.202286  916.977921  2107.841034   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  255  3194.020721  584.735596  3187.558182  573.009838  3197.082108   \n",
       "  256  3199.743439  581.039177  3193.648193  569.236416  3202.597763   \n",
       "  257  3191.177765  581.586304  3185.062317  570.262363  3194.000290   \n",
       "  258  3193.189499  581.225555  3186.569305  569.294804  3195.401688   \n",
       "  259  3194.413422  581.070251  3187.598526  569.022655  3196.005234   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     874.410301  2055.280563  851.198441  1995.436314   863.563217  ...   \n",
       "  1     790.956261  2069.870697  785.746452  2030.513412   770.859653  ...   \n",
       "  2     890.236298  2037.907440  857.874359  2002.343037   843.590874  ...   \n",
       "  3     886.608093  2058.884041  874.929115  2035.003906   861.880020  ...   \n",
       "  4     913.250702  2089.673874  896.481049  2051.083473   872.502869  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  255   574.084221  3140.102020  575.491745  3199.346619   581.945282  ...   \n",
       "  256   570.475243  3142.230972  570.290932  3200.487534   575.557884  ...   \n",
       "  257   571.317940  3140.062027  573.546944  3197.920258   579.599411  ...   \n",
       "  258   570.595875  3137.608612  572.092606  3198.206528   576.472389  ...   \n",
       "  259   570.302933  3135.199005  571.975147  3190.895691   576.219147  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2031.965225  1178.982422  2154.528442  1252.382843   2135.260834   \n",
       "  1    1997.164921  1177.057312  2151.978348  1180.263763   2082.397308   \n",
       "  2    2002.207466  1228.048920  2156.706192  1286.163025   2017.441872   \n",
       "  3    2047.170746  1291.989075  2191.953323  1233.809875   2153.479553   \n",
       "  4    2040.923759  1309.147156  2166.385406  1284.554321   2108.972519   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  255  3189.265366   854.411438  3099.360519   986.216949   3189.095322   \n",
       "  256  3190.198044   846.338623  3101.621346   982.236755   3190.657730   \n",
       "  257  3186.689758   847.374054  3103.421127   987.401947   3189.249466   \n",
       "  258  3185.976044   845.685181  3102.942848   990.745239   3185.122086   \n",
       "  259  3181.402344   844.549316  3103.606606   986.930603   3183.028076   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1285.724609  2223.479645  1457.523376   2214.461975   1464.500854  \n",
       "  1     1244.689423  2215.066223  1469.705200   2188.605301   1489.386414  \n",
       "  2     1376.058716  2223.610321  1493.976624   2086.223724   1523.429749  \n",
       "  3     1271.754700  2213.604004  1500.583496   2192.820374   1502.953308  \n",
       "  4     1343.379272  2207.145554  1520.561096   2162.444687   1530.588318  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  255   1001.224518  3107.846527  1103.074890   3169.085434   1136.220032  \n",
       "  256    992.436218  3107.675529  1115.600281   3170.210541   1128.705322  \n",
       "  257    994.608765  3109.469475  1123.859375   3169.219986   1130.158569  \n",
       "  258    998.155823  3110.197998  1125.838074   3169.076675   1137.676208  \n",
       "  259    991.591797  3117.768661  1119.318848   3168.747131   1130.473450  \n",
       "  \n",
       "  [260 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2285.932129  906.945404  2283.824951  890.450531  2283.960587   \n",
       "  1    2289.122101  928.404312  2287.368416  911.238434  2288.293877   \n",
       "  2    2304.054482  941.517807  2302.731316  923.702103  2305.372963   \n",
       "  3    2427.426392  935.786285  2439.952637  924.601913  2434.104004   \n",
       "  4    2445.238159  904.902985  2457.570068  892.606628  2451.031036   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  244  3466.168304  551.364731  3467.110100  542.279671  3469.496552   \n",
       "  245  3461.938824  552.745926  3464.081512  544.156372  3465.145050   \n",
       "  246  3455.500671  554.730347  3460.212765  546.506151  3458.506737   \n",
       "  247  3457.946934  551.064346  3464.006081  542.026806  3459.319397   \n",
       "  248  3462.747986  550.944557  3472.602676  541.556099  3461.572308   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     890.706001  2237.117043  869.004761  2242.747139   862.431156  ...   \n",
       "  1     911.027161  2251.310844  886.714386  2257.218758   876.119980  ...   \n",
       "  2     921.597321  2262.583855  899.399536  2276.156723   885.986954  ...   \n",
       "  3     915.587341  2371.547287  887.852913  2394.802551   878.563004  ...   \n",
       "  4     884.511337  2377.904602  853.868591  2409.196747   850.937431  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  244   541.673176  3474.738937  545.847630  3505.000000   545.327850  ...   \n",
       "  245   543.252708  3476.316948  547.838593  3502.000000   550.231205  ...   \n",
       "  246   545.814917  3479.607056  552.877823  3497.566994   553.111061  ...   \n",
       "  247   541.620663  3484.969414  547.632462  3487.589607   546.247135  ...   \n",
       "  248   540.829918  3497.307640  546.492409  3475.297760   544.613495  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2251.701141  1212.832428  2323.791031  1350.347412   2297.344101   \n",
       "  1    2249.860466  1210.838318  2328.850220  1353.271057   2281.844330   \n",
       "  2    2261.193901  1230.405396  2326.778320  1352.510071   2303.306068   \n",
       "  3    2243.818573  1164.981995  2321.101883  1335.876648   2314.739197   \n",
       "  4    2213.699287  1150.010406  2315.347702  1332.539062   2257.366791   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  244  3505.000000   765.633759  3464.182274   890.361145   3499.228737   \n",
       "  245  3499.336906   766.829346  3461.796677   879.700195   3493.704857   \n",
       "  246  3494.320801   769.287338  3458.847988   882.783020   3489.836044   \n",
       "  247  3486.927460   772.878998  3462.664925   884.193146   3482.993576   \n",
       "  248  3485.136673   764.903320  3477.663147   869.265381   3474.337234   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1370.603455  2344.280914  1495.352539   2328.173218   1529.935730  \n",
       "  1     1373.807617  2345.103516  1504.006958   2306.950226   1537.641724  \n",
       "  2     1373.587524  2349.353729  1509.682373   2323.630905   1548.136353  \n",
       "  3     1353.950989  2342.772278  1507.689880   2325.814224   1561.652222  \n",
       "  4     1361.771179  2345.017807  1512.639404   2306.159317   1575.061340  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  244    886.885223  3467.040321   964.283203   3492.056213    961.006775  \n",
       "  245    877.459656  3463.052254   944.000000   3487.484558    942.966888  \n",
       "  246    879.240204  3458.836559   954.000000   3487.769836    954.000000  \n",
       "  247    882.425049  3459.764259   938.000000   3480.621933    938.000000  \n",
       "  248    869.080139  3469.148323   920.501801   3464.390591    919.103302  \n",
       "  \n",
       "  [249 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2216.433830  889.133316  2216.665726  873.123642  2218.030029   \n",
       "  1    2246.438347  893.670837  2244.096176  881.312988  2251.810120   \n",
       "  2    2287.123810  867.185966  2282.909042  855.616463  2287.458260   \n",
       "  3    2284.531372  909.741776  2284.789040  898.267776  2291.208458   \n",
       "  4    2398.568558  872.439789  2406.330902  861.233887  2406.930450   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  277  3431.681297  709.652370  3431.083046  701.563601  3439.360252   \n",
       "  278  3453.233147  702.946609  3456.678513  697.262402  3456.128784   \n",
       "  279  3463.692390  706.334152  3468.398254  700.539080  3464.529778   \n",
       "  280  3459.597458  713.516932  3464.051666  708.587073  3461.455399   \n",
       "  281  3456.841110  737.728577  3460.451126  733.990393  3457.505959   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     868.591606  2195.166782  859.996582  2183.321041   832.240730  ...   \n",
       "  1     876.975723  2205.852089  860.558990  2209.283096   837.239609  ...   \n",
       "  2     853.511299  2232.299065  848.585732  2242.181931   843.583084  ...   \n",
       "  3     892.275299  2250.298820  873.884911  2256.961906   855.697517  ...   \n",
       "  4     855.175629  2341.329422  830.019897  2364.350235   823.611649  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  277   700.097414  3433.833672  703.595453  3469.000000   706.409628  ...   \n",
       "  278   696.286186  3454.394508  697.998287  3469.000000   701.841131  ...   \n",
       "  279   699.628494  3466.243423  701.404755  3470.000000   703.326204  ...   \n",
       "  280   707.328169  3466.005302  707.960376  3470.000000   709.853611  ...   \n",
       "  281   732.714005  3461.899406  730.387363  3458.581635   728.407293  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2258.383804  1055.000000  2364.394196  1011.862457   2336.753952   \n",
       "  1    2253.990532  1045.230103  2354.430771  1007.348297   2353.503113   \n",
       "  2    2248.132912  1053.018433  2393.941925  1011.655243   2386.391052   \n",
       "  3    2253.436203  1066.342804  2376.880096  1018.205185   2380.564270   \n",
       "  4    2210.552849  1048.714233  2334.299942  1018.360138   2368.805252   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  277  3453.134293   905.142380  3382.835972  1018.202209   3452.222504   \n",
       "  278  3452.885796   908.918625  3405.478691  1014.829651   3451.858932   \n",
       "  279  3450.957367   908.708374  3416.360619  1007.622375   3451.723251   \n",
       "  280  3454.514671   901.730148  3422.378838  1007.265808   3452.654846   \n",
       "  281  3445.050568   901.950867  3435.895416  1015.105621   3446.955750   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1012.164917  2427.352020  1026.296326   2387.996368   1000.759003  \n",
       "  1     1013.564240  2443.945770  1028.669342   2410.816895   1020.531921  \n",
       "  2     1026.109161  2467.374115  1031.007233   2423.891785   1020.966492  \n",
       "  3     1031.140808  2441.192932  1029.296509   2453.163513   1025.861267  \n",
       "  4     1025.119446  2308.869202  1021.686371   2326.162399   1012.973083  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  277   1030.036957  3370.486904  1126.966034   3442.275398   1145.389648  \n",
       "  278   1039.759460  3382.515724  1098.279877   3432.759041   1154.792114  \n",
       "  279   1035.746307  3389.908234  1095.096130   3436.174393   1142.712677  \n",
       "  280   1012.763062  3410.662746  1098.915100   3429.231201   1115.051422  \n",
       "  281   1016.195099  3426.045776  1117.026245   3432.466072   1117.412933  \n",
       "  \n",
       "  [282 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    2184.553688  1001.701523  2186.377670   986.975601  2179.986320   \n",
       "  1    2197.453423  1004.147873  2198.311111   988.303116  2192.783012   \n",
       "  2    2209.478767  1008.753128  2208.738899   993.366684  2205.939590   \n",
       "  3    2227.334053  1015.213684  2230.648735  1000.914978  2229.304596   \n",
       "  4    2255.428207  1019.892441  2255.849380  1005.711319  2254.108414   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  141  3063.533005   652.606850  3054.295532   637.785706  3067.577744   \n",
       "  142  3077.018997   657.472893  3069.361847   641.285248  3078.988937   \n",
       "  143  3075.888916   658.688034  3067.921432   642.704010  3078.945953   \n",
       "  144  3069.523880   659.655701  3060.289932   643.827240  3071.522095   \n",
       "  145  3063.305862   656.421295  3053.993011   641.002792  3064.837051   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     983.489502  2163.831787  972.828400  2146.116379   953.587082  ...   \n",
       "  1     985.343216  2171.938034  970.347916  2149.909492   956.187614  ...   \n",
       "  2     993.173813  2187.161140  979.282890  2176.394180   955.507072  ...   \n",
       "  3     996.671219  2182.135956  972.859970  2191.282845   964.454445  ...   \n",
       "  4    1001.402695  2217.833931  985.990685  2215.044006   975.338768  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  141   638.042328  2999.384384  638.889244  3079.756912   646.115746  ...   \n",
       "  142   643.344681  2999.972549  641.337448  3075.654419   648.090309  ...   \n",
       "  143   644.622368  3001.108444  641.725555  3077.998688   646.964775  ...   \n",
       "  144   644.513298  2995.457657  642.065437  3074.475418   648.201721  ...   \n",
       "  145   641.682144  2992.092728  642.369461  3067.318970   647.794083  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2127.120335  1316.172363  2242.122604  1426.450745   2163.567612   \n",
       "  1    2134.719154  1326.423309  2242.162354  1433.770935   2154.634384   \n",
       "  2    2166.566853  1282.493805  2238.053040  1413.609680   2201.110558   \n",
       "  3    2159.616390  1277.290253  2241.145668  1416.507507   2189.557503   \n",
       "  4    2167.473610  1302.748291  2241.175652  1426.666748   2187.741249   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  141  3052.063629   988.479340  2934.999390  1167.401306   3048.054901   \n",
       "  142  3052.712631   985.908264  2936.330673  1165.948364   3047.354355   \n",
       "  143  3051.155945   986.610626  2941.217690  1171.998352   3046.097687   \n",
       "  144  3050.412781   985.592743  2947.191132  1175.123047   3046.161484   \n",
       "  145  3048.058563   982.141296  2952.850204  1172.427917   3049.382782   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1464.876526  2261.435211  1572.986450   2199.085815   1606.246582  \n",
       "  1     1476.897339  2259.653397  1583.235352   2192.257629   1616.744263  \n",
       "  2     1458.125366  2253.626785  1587.882812   2222.876358   1624.160339  \n",
       "  3     1453.218079  2255.617752  1580.007324   2221.329437   1619.788208  \n",
       "  4     1463.035156  2261.308624  1587.213135   2214.678452   1630.907898  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  141   1200.873718  2931.559128  1341.235962   3018.081192   1390.515991  \n",
       "  142   1190.774963  2933.285271  1344.681519   3021.004257   1385.570740  \n",
       "  143   1193.383972  2947.218185  1344.834717   3021.795349   1382.325195  \n",
       "  144   1190.349426  2957.894981  1333.412598   3023.158112   1380.795593  \n",
       "  145   1180.469238  2963.689919  1337.500732   3025.466690   1367.604553  \n",
       "  \n",
       "  [146 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1708.299072   996.268845  1716.895416  978.388123  1708.677246   \n",
       "  1    1705.649078   999.612747  1712.526611  983.829956  1707.294739   \n",
       "  2    1703.658813  1001.563858  1713.826904  986.110184  1707.066620   \n",
       "  3    1707.256226  1001.925659  1718.049622  985.780869  1708.969879   \n",
       "  4    1702.771393  1002.689926  1715.554962  983.808990  1701.679504   \n",
       "  ..           ...          ...          ...         ...          ...   \n",
       "  519   414.884087   615.858149   409.036255  606.700599   413.235340   \n",
       "  520   374.589884   618.860065   374.210920  609.218565   372.840385   \n",
       "  521   396.550423   622.926647   393.609013  612.140541   394.035263   \n",
       "  522   377.714060   626.656025   379.731146  616.325600   375.257484   \n",
       "  523   381.299904   629.419769   380.553303  616.211197   379.458085   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     977.645676  1763.213287  968.970291  1775.748962   970.408447  ...   \n",
       "  1     982.293564  1770.644226  987.309555  1767.499939   977.422424  ...   \n",
       "  2     984.053856  1770.908905  984.818024  1762.294617   980.595688  ...   \n",
       "  3     983.346939  1765.681427  983.127319  1778.069580   973.426476  ...   \n",
       "  4     983.351677  1767.776947  979.249001  1762.011169   973.669365  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  519   605.387401   357.928686  613.448847   392.858902   615.551786  ...   \n",
       "  520   608.588835   355.000000  615.266056   378.211349   614.513664  ...   \n",
       "  521   610.677849   359.019833  617.477016   390.651646   616.673548  ...   \n",
       "  522   614.698408   368.968222  619.042049   373.762279   619.385033  ...   \n",
       "  523   615.211458   375.075130  623.623644   374.578366   616.403488  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1747.177399  1357.349060  1693.993774  1409.675659   1675.960480   \n",
       "  1    1740.685669  1298.373810  1654.419281  1354.623047   1650.060638   \n",
       "  2    1762.276550  1275.990417  1681.655289  1279.580261   1652.348465   \n",
       "  3    1779.529846  1292.462646  1639.685654  1291.894440   1636.534698   \n",
       "  4    1764.079651  1295.948914  1634.972641  1290.203491   1632.762924   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  519   394.929417   940.462524   412.684319  1081.812866    422.115875   \n",
       "  520   386.430109   955.718719   419.315163  1088.989410    422.075249   \n",
       "  521   379.989799   938.712097   414.652191  1085.901611    420.167709   \n",
       "  522   367.748451   944.874512   412.319576  1092.340027    419.630634   \n",
       "  523   375.492105   966.315155   412.135647  1095.862213    417.615543   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1387.985718  1642.765259  1474.686829   1629.967972   1459.776184  \n",
       "  1     1330.176178  1614.377274  1511.601685   1612.813416   1493.838928  \n",
       "  2     1245.733032  1599.994263  1526.540344   1592.636139   1508.259277  \n",
       "  3     1256.913452  1598.667465  1550.328430   1592.858978   1526.143005  \n",
       "  4     1265.252075  1587.751984  1570.418396   1578.999069   1554.197937  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  519   1081.164612   469.529732  1238.087769    464.527466   1236.909973  \n",
       "  520   1088.339325   475.012337  1243.935608    465.666954   1242.804138  \n",
       "  521   1085.608612   471.448822  1245.295105    461.174339   1243.014404  \n",
       "  522   1092.753174   479.868111  1250.518921    476.022629   1250.188965  \n",
       "  523   1097.129059   478.104401  1253.145264    471.289803   1253.819458  \n",
       "  \n",
       "  [524 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1662.793518  973.585915  1682.685699  971.091404  1649.751450   \n",
       "  1    1713.770538  932.317432  1732.366959  926.430889  1714.494446   \n",
       "  2    1692.401291  921.842045  1710.473801  919.892288  1683.501419   \n",
       "  3    1791.694519  910.140818  1807.877472  903.593849  1787.278625   \n",
       "  4    1729.186279  908.987236  1737.302856  893.521721  1728.895142   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  638   506.552685  655.542450   510.789379  643.024712   506.783211   \n",
       "  639   508.029716  656.877525   510.674080  643.944366   507.219006   \n",
       "  640   502.675098  658.900269   503.838627  645.581604   503.460167   \n",
       "  641   494.874516  659.649521   499.050858  646.357773   493.667164   \n",
       "  642   488.568607  652.197258   493.190903  639.449371   489.712719   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     966.030998  1712.690842  962.058300  1667.635483   931.000000  ...   \n",
       "  1     921.112590  1775.132416  936.691175  1744.205780   925.413984  ...   \n",
       "  2     915.034473  1754.015472  917.050375  1704.751419   903.903065  ...   \n",
       "  3     898.516693  1834.039825  909.126001  1776.443573   894.852405  ...   \n",
       "  4     895.313309  1799.708862  875.899651  1781.919189   875.952301  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  638   643.335999   538.079803  649.426895   587.316856   647.190392  ...   \n",
       "  639   644.621033   531.003899  649.368401   579.110466   647.672287  ...   \n",
       "  640   647.006981   523.710762  651.688316   574.827927   648.173576  ...   \n",
       "  641   647.455437   523.328018  651.625954   568.062599   650.548416  ...   \n",
       "  642   640.925865   518.536018  649.829311   566.183899   648.646965  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1564.795860  1153.272446  1556.108295  1385.664612   1532.397541   \n",
       "  1    1735.514618  1154.104797  1679.345718  1185.719391   1654.405762   \n",
       "  2    1685.444763  1065.980530  1726.321381  1197.768524   1600.751404   \n",
       "  3    1724.069305  1141.986496  1649.176941  1213.564575   1659.685211   \n",
       "  4    1774.735229  1166.812347  1645.433685  1214.760437   1664.948868   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  638   643.934387   929.735931   533.255913  1085.801544    649.281097   \n",
       "  639   644.457870   930.591583   528.901207  1082.647430    648.494934   \n",
       "  640   642.043411   933.964783   523.079041  1086.055420    643.216919   \n",
       "  641   635.233215   931.406555   520.565346  1078.741302    639.982422   \n",
       "  642   632.474167   934.525238   525.894753  1090.919281    642.502640   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1365.516235  1552.057922  1417.624481   1535.208958   1399.612732  \n",
       "  1     1127.910126  1607.790070  1436.070312   1596.987343   1408.736755  \n",
       "  2     1126.143036  1609.410889  1448.440735   1495.454102   1406.127441  \n",
       "  3     1107.696609  1603.523026  1465.189819   1534.644020   1190.204193  \n",
       "  4     1144.087799  1607.367538  1488.312866   1582.178192   1409.774658  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  638   1086.794556   515.712929  1238.569031    651.018890   1241.775635  \n",
       "  639   1083.920074   513.889709  1238.791809    652.932648   1241.073853  \n",
       "  640   1086.031342   508.380386  1240.319580    650.589310   1238.286621  \n",
       "  641   1081.464813   503.296261  1238.427856    651.077072   1240.959778  \n",
       "  642   1089.026306   508.717159  1240.204102    653.677505   1240.323059  \n",
       "  \n",
       "  [643 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1719.936523  974.689407  1721.443542  958.396133  1717.865387   \n",
       "  1    1698.684479  959.985321  1701.021576  939.977524  1695.899323   \n",
       "  2    1707.662628  984.053452  1705.613708  963.146240  1707.797119   \n",
       "  3    1690.688049  981.355057  1693.916870  963.946518  1693.531586   \n",
       "  4    1688.243835  985.784760  1688.972443  966.211349  1687.147949   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  526   446.425869  661.624756   440.946552  648.552952   447.803165   \n",
       "  527   451.929410  653.142929   447.839023  640.553532   451.610362   \n",
       "  528   441.885057  676.540604   443.975306  666.538143   440.000000   \n",
       "  529   443.000000  651.761936   444.408901  641.190609   443.000000   \n",
       "  530   484.042139  738.215660   490.476612  732.996067   483.157194   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     960.376709  1760.535522  932.757301  1772.087463   937.601761  ...   \n",
       "  1     940.893585  1760.781616  928.002022  1760.029999   922.769432  ...   \n",
       "  2     965.265152  1746.357971  940.595779  1773.171814   945.200958  ...   \n",
       "  3     962.622116  1743.191772  941.071854  1776.272888   941.982178  ...   \n",
       "  4     967.834076  1729.792511  950.311325  1757.080139   948.967804  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  526   649.635109   436.000000  656.141144   466.027037   659.204979  ...   \n",
       "  527   641.319725   440.000000  652.456802   456.228342   660.409515  ...   \n",
       "  528   666.643623   441.978926  671.310539   449.300703   670.421013  ...   \n",
       "  529   640.674528   443.000000  647.541637   449.715355   649.948601  ...   \n",
       "  530   730.627880   490.802929  733.283268   458.000000   713.672073  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1751.326324  1215.493683  1664.510437  1199.365417   1649.333221   \n",
       "  1    1728.117401  1237.725861  1678.480225  1367.925171   1674.494293   \n",
       "  2    1749.149658  1271.851318  1652.575836  1380.606201   1661.352295   \n",
       "  3    1776.429016  1253.765625  1640.933960  1212.931702   1652.410431   \n",
       "  4    1793.171844  1288.297974  1624.593506  1237.584961   1624.830536   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  526   537.455841   938.193848   479.148773  1100.615448    562.567635   \n",
       "  527   524.412880   940.994171   474.043793  1100.778320    554.045288   \n",
       "  528   491.963943   938.524628   528.124855  1092.632507    536.269821   \n",
       "  529   482.551750   942.869354   521.064812  1099.459747    525.796974   \n",
       "  530   484.984823   952.363342   513.090584  1101.068970    513.658520   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1176.890259  1618.610535  1418.493286   1614.493759   1406.812134  \n",
       "  1     1329.546875  1618.498657  1451.844299   1604.241959   1434.497681  \n",
       "  2     1340.710205  1600.475510  1477.034973   1597.039444   1459.485718  \n",
       "  3     1185.394867  1585.584747  1488.011414   1581.240723   1457.233215  \n",
       "  4     1223.677002  1565.926407  1510.077759   1572.531815   1499.039368  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  526   1095.070801   543.768402  1251.397827    595.987656   1255.647400  \n",
       "  527   1091.488739   530.176064  1247.856812    593.694702   1245.710144  \n",
       "  528   1091.247620   595.476517  1248.052795    587.535934   1247.541687  \n",
       "  529   1099.039612   595.844757  1248.595825    586.910034   1248.355591  \n",
       "  530   1098.988586   591.450821  1247.032043    589.714462   1244.874939  \n",
       "  \n",
       "  [531 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1490.558060  934.663086  1498.115463  919.832596  1489.174026   \n",
       "  1    1475.669426  929.137833  1485.582733  914.231323  1475.131516   \n",
       "  2    1433.593857  887.854568  1444.937935  873.609619  1435.436783   \n",
       "  3    1417.163620  852.603859  1432.299484  839.142471  1418.208900   \n",
       "  4    1404.544388  834.308014  1425.593750  823.265427  1405.414078   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  130   300.186020  574.264893   300.001691  562.102264   300.330902   \n",
       "  131   287.307639  567.467201   286.993595  556.192253   288.700459   \n",
       "  132   293.455729  541.289585   289.411127  532.997017   294.646788   \n",
       "  133   307.195862  534.006735   303.814739  527.298790   304.753143   \n",
       "  134   288.895910  559.189884   293.472797  550.968073   285.470928   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     917.894623  1549.611359  915.012756  1552.713333   898.781982  ...   \n",
       "  1     913.107040  1540.365189  910.878700  1526.111557   899.762062  ...   \n",
       "  2     870.987495  1509.806107  873.162483  1485.012268   860.811951  ...   \n",
       "  3     834.000710  1497.679947  841.182671  1471.557434   811.591019  ...   \n",
       "  4     815.041977  1484.862198  827.033691  1442.206589   790.947807  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  130   564.006592   313.488539  560.942322   356.895149   554.471085  ...   \n",
       "  131   557.596458   298.518847  559.793785   342.823227   555.922974  ...   \n",
       "  132   533.321709   279.787486  539.385292   326.923817   537.145588  ...   \n",
       "  133   527.428661   282.646494  536.002371   305.221310   533.921188  ...   \n",
       "  134   550.799599   293.483852  548.558397   285.711925   549.492708  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1651.000000  1162.128815  1538.162643  1238.670135   1551.035400   \n",
       "  1    1644.000000  1152.233154  1536.345535  1236.508942   1559.389694   \n",
       "  2    1626.425964  1043.565948  1526.831573  1209.863678   1552.704681   \n",
       "  3    1620.543396  1077.930359  1540.381042  1234.790619   1531.588837   \n",
       "  4    1573.330658  1041.580780  1573.175598  1233.969299   1523.978027   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  130   440.195480   818.279785   364.024582   989.705688    438.175858   \n",
       "  131   421.990524   809.904724   349.004349   982.485107    394.970695   \n",
       "  132   401.540878   811.284515   343.301559   981.947357    366.694374   \n",
       "  133   367.268044   814.778137   339.290535   988.670166    350.315727   \n",
       "  134   332.259640   823.426483   321.756512   979.339752    330.811577   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1232.507202  1546.830780  1449.744019   1567.597351   1438.900818  \n",
       "  1     1231.752319  1551.539108  1452.506531   1602.689392   1438.834961  \n",
       "  2     1204.842163  1545.073715  1446.800537   1580.741745   1429.313843  \n",
       "  3     1223.453735  1568.070236  1450.088440   1558.483749   1440.369568  \n",
       "  4     1228.664001  1588.867706  1449.894409   1544.972504   1440.690125  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  130    980.189972   376.744461  1140.871399    505.947830   1116.115051  \n",
       "  131    964.478668   376.787163  1140.211121    463.074249   1090.180664  \n",
       "  132    961.164673   378.463890  1136.160767    413.202118   1084.897644  \n",
       "  133    983.529572   368.940445  1137.055298    366.328026   1132.361328  \n",
       "  134    978.678680   373.351097  1128.460938    366.608055   1129.731934  \n",
       "  \n",
       "  [135 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1477.270401  879.729668  1486.019012  875.962360  1470.102295   \n",
       "  1    1551.846985  791.425957  1557.861618  780.588875  1545.549698   \n",
       "  2    1580.387253  715.018078  1587.147430  711.607035  1574.154892   \n",
       "  3    1500.642822  777.667595  1510.704498  765.741013  1502.797043   \n",
       "  4    1569.250397  805.476974  1569.033142  792.348244  1569.400848   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  164   266.863661  560.130348   272.873257  543.126495   262.717594   \n",
       "  165   251.471984  561.993759   255.726105  545.252144   247.526030   \n",
       "  166   233.596986  556.303734   234.897854  542.544807   232.533252   \n",
       "  167   262.683792  515.263317   258.362690  507.828880   261.909523   \n",
       "  168   244.647514  512.145426   247.044701  506.590540   241.200697   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     875.513317  1506.363266  873.464043  1447.009399   867.287142  ...   \n",
       "  1     784.167519  1570.479828  778.419662  1549.711884   782.112167  ...   \n",
       "  2     710.969584  1594.260895  708.428358  1563.676758   699.905894  ...   \n",
       "  3     765.631401  1545.984970  778.851723  1563.317932   775.106544  ...   \n",
       "  4     793.729935  1526.979980  776.632301  1550.487900   786.936592  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  164   547.145424   313.017418  532.077530   331.852631   535.610947  ...   \n",
       "  165   548.945900   294.384010  536.334595   316.815277   539.933868  ...   \n",
       "  166   543.971405   251.009632  541.535423   283.743053   539.943535  ...   \n",
       "  167   508.316479   229.621903  519.810577   260.109413   522.996006  ...   \n",
       "  168   506.147398   243.463287  504.401997   227.455578   519.131218  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1467.070969   972.527985  1514.278549  1101.491730   1467.350388   \n",
       "  1    1561.608627  1020.267151  1522.625412  1097.343353   1525.332520   \n",
       "  2    1525.155930   926.760132  1536.869705  1090.216675   1478.491478   \n",
       "  3    1543.588089  1002.827240  1540.178909  1108.311157   1501.680817   \n",
       "  4    1557.260803  1026.309631  1570.395523  1147.076508   1510.440094   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  164   419.977112   810.757050   312.525749   980.587830    430.575577   \n",
       "  165   399.934753   815.458008   298.452362   981.668274    418.945099   \n",
       "  166   353.872910   819.669037   262.975723   990.996582    362.491943   \n",
       "  167   300.874588   811.941803   274.850430   969.907104    297.285049   \n",
       "  168   219.392984   580.169762   277.269844   635.803268    276.211327   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1089.966797  1466.408607  1245.908081   1462.260704   1235.843048  \n",
       "  1     1090.700684  1464.156128  1253.576416   1465.770004   1245.636841  \n",
       "  2     1077.191528  1490.021591  1249.874084   1471.721840   1243.058228  \n",
       "  3     1098.184753  1515.314621  1252.771912   1477.883186   1246.114014  \n",
       "  4     1104.645569  1526.973907  1277.270569   1479.475143   1257.277588  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  164    973.215881   285.272446  1128.387146    459.630890   1124.203125  \n",
       "  165    971.305817   270.532837  1126.086609    456.343307   1124.576355  \n",
       "  166    977.515594   259.136433  1134.507751    443.272598   1117.578552  \n",
       "  167    966.167664   401.985321  1088.507751    402.222000   1090.676575  \n",
       "  168    639.600525   285.416000   768.455627    283.966393    769.579834  \n",
       "  \n",
       "  [169 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2112.827896  823.272949  2123.163330  807.139526  2111.965012   \n",
       "  1    2102.458481  827.307541  2111.878342  809.053940  2101.931885   \n",
       "  2    2091.809479  829.360657  2101.322388  811.099411  2087.543594   \n",
       "  3    2082.675140  829.392334  2092.787888  810.985474  2078.083710   \n",
       "  4    2074.192886  832.237427  2081.502609  811.361290  2072.425110   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  512   969.224503  474.090630   973.522854  462.642014   971.254990   \n",
       "  513   963.722919  470.521080   969.028828  459.993675   966.516590   \n",
       "  514   960.747620  474.568871   964.415245  463.527565   961.476654   \n",
       "  515   959.639339  473.408508   964.477676  462.293537   962.874802   \n",
       "  516   958.988182  469.314445   963.672960  459.188671   962.058231   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     806.976730  2176.042023  798.314713  2137.591095   802.590240  ...   \n",
       "  1     810.257736  2162.789825  803.831787  2135.263031   808.418152  ...   \n",
       "  2     812.009644  2151.773895  801.293243  2120.435608   808.685287  ...   \n",
       "  3     812.016296  2143.632751  801.001648  2114.571793   808.395546  ...   \n",
       "  4     813.854927  2137.220398  800.898506  2113.858322   808.487297  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  512   463.549393   994.127266  475.274681  1039.897507   472.177486  ...   \n",
       "  513   460.942509   991.065468  475.533920  1037.027138   472.932922  ...   \n",
       "  514   464.126083   981.351311  473.539803  1032.795158   475.643211  ...   \n",
       "  515   462.772800   985.652363  475.880798  1032.894157   472.417393  ...   \n",
       "  516   459.321007   984.904251  474.025177  1032.029526   472.578648  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2145.005768  1088.208466  2090.512955  1153.437042   2082.812927   \n",
       "  1    2156.727722  1093.627930  2089.350815  1135.545013   2086.265869   \n",
       "  2    2151.918427  1114.663910  2084.256454  1141.958923   2084.328339   \n",
       "  3    2135.642487  1122.391907  2074.655869  1151.724609   2077.513412   \n",
       "  4    2163.856323  1135.092712  2083.415115  1159.220703   2070.695465   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  512  1093.073044   737.427063  1050.335808   908.825012   1054.527298   \n",
       "  513  1084.097778   737.409760  1047.128746   906.630402   1049.294380   \n",
       "  514  1082.357162   739.068359  1043.044617   907.148926   1046.525101   \n",
       "  515  1079.353226   737.489777  1036.293930   905.705200   1044.148903   \n",
       "  516  1078.869461   733.127533  1034.463318   901.781097   1052.181885   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1145.174896  2063.355148  1357.979065   2056.346970   1353.557251  \n",
       "  1     1128.801453  2054.263977  1358.824280   2052.690567   1355.080872  \n",
       "  2     1139.181824  2054.711395  1368.846069   2055.419907   1364.449829  \n",
       "  3     1144.971405  2057.785141  1369.901306   2060.775711   1367.361877  \n",
       "  4     1148.401398  2068.383438  1384.990234   2064.126785   1383.776489  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  512    896.955811  1108.915985  1064.283325   1028.347557   1045.246826  \n",
       "  513    895.691833  1104.344086  1058.474243   1039.310211   1044.363159  \n",
       "  514    896.887970  1106.316910  1057.249084   1035.613785   1040.994019  \n",
       "  515    896.668518  1102.743652  1055.355347   1032.319107   1047.964844  \n",
       "  516    894.134705  1100.693069  1053.101562   1038.544556   1047.462341  \n",
       "  \n",
       "  [517 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2120.907700  734.599037  2132.883667  728.371326  2103.514236   \n",
       "  1    2064.307037  723.239033  2070.484299  716.149139  2049.190842   \n",
       "  2    2072.028641  641.837721  2081.784531  632.978300  2053.387161   \n",
       "  3    1975.596733  666.647800  1977.368362  656.681545  1959.854897   \n",
       "  4    1987.373291  687.536087  1987.840912  665.971970  1976.161896   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  576  3349.458359  335.646492  3343.703720  323.528233  3344.389877   \n",
       "  577  3352.405457  337.393829  3346.392120  325.317284  3346.623138   \n",
       "  578  3356.576538  341.449745  3350.483368  328.639126  3354.187271   \n",
       "  579  3365.919937  337.815468  3360.296402  325.879696  3360.963516   \n",
       "  580  3371.251144  337.905174  3363.462982  324.846619  3368.129059   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     726.309673  2129.996307  722.275352  2042.167641   729.565445  ...   \n",
       "  1     717.072689  2064.428741  711.461346  2016.077606   704.197920  ...   \n",
       "  2     634.277346  2080.242111  627.756128  2001.827057   628.082838  ...   \n",
       "  3     657.980339  1962.192062  650.871311  1921.729553   640.128624  ...   \n",
       "  4     665.934830  1963.677216  651.014465  1943.781006   653.359711  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  576   323.050732  3273.130089  335.247784  3321.996307   334.791092  ...   \n",
       "  577   325.188637  3276.494560  336.677063  3325.900833   338.129211  ...   \n",
       "  578   328.233181  3276.735435  333.905327  3331.865799   338.996750  ...   \n",
       "  579   325.405014  3289.888130  337.432060  3340.701050   336.632034  ...   \n",
       "  580   325.623985  3293.428162  336.257103  3341.928528   336.329102  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2015.292320   914.772797  2091.400192  1145.506256   2068.690414   \n",
       "  1    2014.794876   917.328659  2089.308563  1087.808990   2053.717468   \n",
       "  2    2013.602066   917.745087  2126.352234  1056.501251   2062.674500   \n",
       "  3    2016.274307   932.407135  2128.536530   968.163879   2102.851059   \n",
       "  4    1997.089828   958.966522  2124.195648   980.409180   2067.320404   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  576  3275.341530   646.603577  3280.979057   811.722107   3254.468422   \n",
       "  577  3279.981209   647.329651  3292.226097   815.637634   3255.549553   \n",
       "  578  3281.927856   640.076447  3298.426880   806.580322   3262.812988   \n",
       "  579  3292.901245   637.346344  3301.847061   802.845886   3268.459602   \n",
       "  580  3298.541229   639.562653  3305.113983   802.350525   3266.640182   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1161.574799  2089.465897  1262.802795   2062.884537   1274.308289  \n",
       "  1     1106.009003  2136.810349  1224.480713   2121.076218   1234.096619  \n",
       "  2     1099.764435  2136.332520  1223.244934   2101.307022   1252.573975  \n",
       "  3     1018.312439  2144.376404  1220.521606   2130.588898   1239.350830  \n",
       "  4     1051.425537  2144.995300  1224.155884   2119.096924   1248.375793  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  576    826.210266  3296.759827   959.006958   3205.895672    989.319397  \n",
       "  577    830.036011  3305.336746   963.667114   3203.642559    991.098511  \n",
       "  578    818.899231  3308.317917   958.590881   3208.328060    992.371765  \n",
       "  579    809.280151  3316.941452   958.554993   3211.899357    977.142700  \n",
       "  580    808.342651  3318.186462   960.457764   3209.658485    973.583008  \n",
       "  \n",
       "  [581 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2084.457260  667.636169  2078.792984  648.952835  2074.955009   \n",
       "  1    2100.179764  657.605118  2092.140350  638.109665  2091.846817   \n",
       "  2    2095.999451  665.542755  2088.806427  645.741760  2087.021118   \n",
       "  3    2110.053650  650.467987  2098.978043  631.424606  2098.276443   \n",
       "  4    2120.952393  654.376114  2114.076965  637.231522  2111.674942   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  543  3359.579239  360.436405  3355.120483  350.472565  3361.742355   \n",
       "  544  3371.208801  362.498631  3363.392868  349.384533  3369.130539   \n",
       "  545  3376.099457  355.843182  3368.994171  345.623344  3377.103912   \n",
       "  546  3383.064468  364.796665  3375.885635  352.383835  3381.597321   \n",
       "  547  3391.682663  362.428574  3384.662674  349.980946  3389.848480   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     652.919769  2048.640381  646.563873  2023.032658   658.277496  ...   \n",
       "  1     641.005554  2049.103729  642.411026  2039.340576   656.175842  ...   \n",
       "  2     646.980225  2049.395439  641.450638  2053.430397   651.296875  ...   \n",
       "  3     634.956879  2047.880051  637.214401  2050.740631   646.887527  ...   \n",
       "  4     637.792740  2060.642044  643.024841  2068.372925   646.583282  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  543   348.814140  3287.898026  352.146606  3353.025558   355.756035  ...   \n",
       "  544   350.605900  3289.455429  355.885647  3355.861115   356.247902  ...   \n",
       "  545   345.922241  3301.161339  351.750332  3367.031906   356.172581  ...   \n",
       "  546   352.551144  3301.537621  356.746639  3368.886765   359.951603  ...   \n",
       "  547   350.271393  3310.525826  355.535236  3374.587997   358.392094  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2076.255241   962.533112  2220.981628  1031.292847   2145.924103   \n",
       "  1    2065.751106  1043.570435  2228.764557  1029.501038   2175.975586   \n",
       "  2    2016.280899  1036.815857  2223.687378  1015.585449   2199.951782   \n",
       "  3    1995.956543  1038.826416  2225.790955  1059.937134   2170.809143   \n",
       "  4    2051.140526   959.481415  2230.808105  1087.176270   2135.237961   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  543  3316.192230   659.773865  3286.671959   826.395569   3309.985672   \n",
       "  544  3322.964882   663.821106  3300.586098   826.164307   3304.104912   \n",
       "  545  3325.643478   659.481689  3318.543137   824.356506   3301.198906   \n",
       "  546  3329.145073   663.584534  3331.478310   835.559265   3304.889648   \n",
       "  547  3336.083969   658.428070  3349.577934   819.763611   3313.115326   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1089.880310  2230.796509  1253.060242   2189.156647   1267.117615  \n",
       "  1     1115.896667  2237.513916  1269.512634   2215.170166   1296.038330  \n",
       "  2     1034.151550  2244.055450  1298.964111   2233.794769   1313.129578  \n",
       "  3     1084.986206  2251.575470  1316.860779   2225.663757   1329.586487  \n",
       "  4     1107.877991  2259.276672  1342.599426   2182.391663   1362.363098  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  543    831.534607  3267.209991   987.327881   3262.968071    995.002808  \n",
       "  544    837.683044  3295.392540   971.169067   3264.435749    994.690796  \n",
       "  545    839.837463  3323.603607   974.216797   3252.403015    997.002136  \n",
       "  546    844.357117  3341.305496   981.783447   3260.981564    997.342407  \n",
       "  547    833.051453  3362.703049   970.834839   3264.956608    995.113586  \n",
       "  \n",
       "  [548 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2009.800392  717.358734  2021.837227  693.361359  1993.428001   \n",
       "  1    2067.134933  684.341385  2063.351074  661.484879  2051.051605   \n",
       "  2    2074.002045  655.945633  2075.292191  633.515106  2056.904465   \n",
       "  3    2089.930206  678.792679  2080.068817  658.107956  2074.392563   \n",
       "  4    2100.345154  683.261459  2093.546432  663.730148  2090.242004   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  627  3135.311966  271.265190  3125.990692  256.784729  3139.748901   \n",
       "  628  3124.260101  271.236496  3114.635864  257.831993  3128.615082   \n",
       "  629  3119.204422  265.825043  3109.717255  252.926010  3124.062683   \n",
       "  630  3115.410095  266.368675  3106.073792  253.538734  3120.938110   \n",
       "  631  3110.050613  265.046425  3101.048248  252.133629  3115.823044   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     696.120605  2043.830124  664.261398  1964.379971   662.758606  ...   \n",
       "  1     668.689331  2035.762718  647.252594  1999.452431   660.809860  ...   \n",
       "  2     635.904007  2041.773537  620.753738  1991.399033   630.338531  ...   \n",
       "  3     665.020462  2042.876678  658.580429  2026.757225   673.981598  ...   \n",
       "  4     668.477203  2019.808258  669.540298  2050.104950   677.329926  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  627   258.200790  3073.224518  260.807907  3152.359329   260.991386  ...   \n",
       "  628   257.926529  3072.758049  258.815613  3153.525589   259.161888  ...   \n",
       "  629   252.781197  3074.054352  256.352402  3157.473740   257.183685  ...   \n",
       "  630   252.966278  3076.174362  256.881111  3159.084961   256.465004  ...   \n",
       "  631   251.406494  3078.495422  254.613876  3161.648972   255.288666  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1984.913483  1004.361511  2184.723419  1111.113525   2049.814865   \n",
       "  1    2017.144432  1020.756775  2197.163513  1106.612183   2162.511749   \n",
       "  2    1947.759857  1034.952759  2168.968338  1063.038330   2057.466675   \n",
       "  3    1964.513424  1044.912415  2183.362793  1080.728455   2147.399948   \n",
       "  4    1987.015327  1041.573364  2157.579666  1099.076965   2169.955978   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  627  3128.395645   607.934448  3019.622292   783.207886   3112.059448   \n",
       "  628  3131.901413   605.186707  3025.787148   777.768066   3112.430008   \n",
       "  629  3135.330460   598.284393  3022.516113   768.580566   3114.999069   \n",
       "  630  3137.547043   600.723907  3026.319115   773.792908   3114.492691   \n",
       "  631  3138.789810   602.252777  3028.648666   773.939209   3113.326843   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1131.574219  2231.533936  1311.499634   2149.418808   1323.714661  \n",
       "  1     1128.653015  2237.220978  1325.423828   2228.161926   1329.914673  \n",
       "  2     1102.846985  2242.322083  1342.648621   2138.644501   1373.984619  \n",
       "  3     1101.934448  2242.452789  1355.677795   2217.436920   1370.663330  \n",
       "  4     1119.773743  2234.189301  1371.168945   2233.019165   1392.210815  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  627    795.342834  2986.519096   942.193176   3080.352936    965.270264  \n",
       "  628    792.900574  2987.862820   934.330627   3076.196220    964.076355  \n",
       "  629    785.037964  2989.820206   935.480713   3075.258705    961.944336  \n",
       "  630    793.158447  2989.394596   937.405701   3075.980423    966.115295  \n",
       "  631    794.398682  2996.175598   936.323486   3074.185448    965.201477  \n",
       "  \n",
       "  [632 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1968.011810  726.136261  1975.810028  707.669464  1954.873901   \n",
       "  1    1989.680099  722.889771  1986.678085  707.158829  1978.338043   \n",
       "  2    2005.577072  726.442612  2004.121979  711.708069  1996.688599   \n",
       "  3    2015.459755  733.927200  2015.154076  716.981216  2007.043060   \n",
       "  4    2028.407570  734.228241  2026.939919  718.411278  2018.082893   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  448  3278.000000  397.931648  3278.000000  387.581676  3273.407776   \n",
       "  449  3275.935120  391.871628  3275.915573  382.543968  3272.287720   \n",
       "  450  3222.753502  399.027641  3223.206749  389.534645  3226.275154   \n",
       "  451  3202.807556  408.287270  3201.332085  394.193455  3207.470337   \n",
       "  452  3218.141136  408.587524  3217.631035  395.448761  3221.446167   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     708.651230  1983.507492  701.132034  1923.245132   703.088318  ...   \n",
       "  1     708.583847  1958.833084  709.059509  1937.494537   711.069687  ...   \n",
       "  2     711.705841  1971.483688  711.511612  1954.681213   708.093613  ...   \n",
       "  3     717.680008  1984.626366  717.436234  1955.563179   715.740417  ...   \n",
       "  4     718.594498  2000.355934  724.566849  1971.059162   717.552368  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  448   387.028751  3244.709595  390.959721  3250.595108   390.932022  ...   \n",
       "  449   381.744579  3242.734634  390.825348  3256.529907   389.761280  ...   \n",
       "  450   389.345215  3213.962082  391.019085  3258.734299   391.463413  ...   \n",
       "  451   394.093990  3198.789810  397.080151  3252.745361   398.686005  ...   \n",
       "  452   395.291256  3212.049782  396.128807  3248.355560   399.764622  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1935.856125   956.342377  2068.949173  1109.957397   2054.628662   \n",
       "  1    1918.503349  1020.317566  2074.642303  1118.736755   1995.888916   \n",
       "  2    1925.478859  1015.338776  2074.408905  1122.842590   2069.477280   \n",
       "  3    1936.768379  1052.230286  2075.988251  1121.965027   2003.232613   \n",
       "  4    1937.641634  1086.287109  2079.002121  1122.994385   1994.179115   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  448  3206.036789   702.025696  3178.663040   832.823517   3221.091652   \n",
       "  449  3216.235229   704.188171  3189.199287   851.408722   3217.777397   \n",
       "  450  3225.186401   697.946960  3180.997124   840.310669   3249.156158   \n",
       "  451  3228.620262   699.027679  3182.019005   835.297729   3240.446564   \n",
       "  452  3223.626244   686.109680  3200.509766   836.561462   3200.407005   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1111.387817  2058.119659  1356.203186   2049.728622   1360.937805  \n",
       "  1     1175.719299  2063.610855  1345.587463   2009.658875   1378.862854  \n",
       "  2     1125.395142  2059.291275  1356.133667   2052.518600   1363.163269  \n",
       "  3     1184.823608  2061.264023  1349.907776   2010.532906   1379.784058  \n",
       "  4     1215.430969  2059.570099  1353.535706   2015.397217   1389.527832  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  448    834.469604  3154.136757  1004.607849   3186.543495   1008.963135  \n",
       "  449    854.521301  3159.850719  1011.870911   3183.156654   1017.085571  \n",
       "  450    845.410797  3160.146690  1012.069458   3232.607094   1014.883240  \n",
       "  451    850.252258  3157.949020  1002.846985   3218.813011   1010.194397  \n",
       "  452    836.422180  3168.472984  1005.451294   3156.819656   1005.251282  \n",
       "  \n",
       "  [453 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2069.427313  728.171844  2073.399224  721.828918  2067.113059   \n",
       "  1    2274.606674  712.772430  2272.898758  696.576660  2265.383896   \n",
       "  2    2069.164864  617.776192  2075.090681  614.674683  2066.788826   \n",
       "  3    2056.556946  634.243629  2063.278585  628.891212  2050.917779   \n",
       "  4    2052.903481  694.097343  2056.323854  687.454041  2049.851123   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  505  3232.942337  280.581779  3227.917374  268.274811  3230.368149   \n",
       "  506  3237.705994  287.485428  3233.058228  274.223701  3235.515427   \n",
       "  507  3246.702011  286.253723  3241.342590  273.036430  3244.432327   \n",
       "  508  3255.465393  286.726830  3249.830276  272.929764  3254.019501   \n",
       "  509  3269.684479  285.802391  3263.775009  272.077774  3267.433807   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     720.832199  2075.875283  725.469955  2068.401008   731.183022  ...   \n",
       "  1     698.327744  2215.059464  690.720764  2217.811142   713.898804  ...   \n",
       "  2     613.022675  2072.410538  614.082726  2078.173115   586.868404  ...   \n",
       "  3     626.382233  2062.020844  624.176949  2040.055704   612.387283  ...   \n",
       "  4     685.982491  2054.359939  687.603806  2044.000000   691.845047  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  505   267.651237  3158.002609  274.810287  3213.747253   277.845863  ...   \n",
       "  506   273.783844  3165.682770  278.708466  3215.828445   280.108971  ...   \n",
       "  507   272.977341  3173.337158  278.217239  3226.308578   280.792412  ...   \n",
       "  508   273.538048  3181.234161  277.407906  3235.611450   281.779999  ...   \n",
       "  509   272.855286  3193.850342  279.024582  3243.985626   280.270576  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2065.643898   950.187653  2151.664757  1024.152496   2128.608620   \n",
       "  1    2074.145018   928.683350  2153.070732  1037.572571   2140.017738   \n",
       "  2    2049.607425   892.337524  2145.068153  1062.906433   2108.170082   \n",
       "  3    2038.000000   913.197327  2150.022423  1071.232910   2096.631828   \n",
       "  4    2047.786399   951.436584  2151.165779  1076.631744   2103.686874   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  505  3180.002029   587.718414  3099.252411   737.792419   3221.933868   \n",
       "  506  3186.049927   590.183502  3104.964447   740.832703   3229.752228   \n",
       "  507  3192.365906   587.149261  3109.844948   737.188293   3234.863815   \n",
       "  508  3204.695572   589.170654  3117.540253   739.527588   3240.744476   \n",
       "  509  3211.876938   590.394989  3124.913124   736.683472   3242.778122   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1042.510986  2173.113159  1252.397217   2161.992561   1263.436279  \n",
       "  1     1042.525635  2168.469398  1266.203613   2159.361267   1273.944092  \n",
       "  2     1070.862915  2165.273010  1293.314880   2103.020538   1315.303223  \n",
       "  3     1082.546387  2157.161995  1306.729431   2087.491432   1325.879272  \n",
       "  4     1090.817291  2151.661766  1310.351501   2101.532787   1324.094604  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  505    745.091675  3076.078075   901.129761   3249.959076    905.302307  \n",
       "  506    750.535095  3074.156441   901.685608   3250.617416    910.520569  \n",
       "  507    744.636047  3078.702618   899.150085   3250.528244    905.282288  \n",
       "  508    748.855957  3079.931141   903.211365   3248.327972    911.938965  \n",
       "  509    751.551392  3078.607204   896.174988   3249.201355    910.904907  \n",
       "  \n",
       "  [510 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2080.177708  640.191887  2086.819963  635.863941  2075.758902   \n",
       "  1    2297.346130  690.415070  2297.755737  679.918877  2297.253845   \n",
       "  2    2059.524475  627.207413  2059.005486  609.764481  2049.277111   \n",
       "  3    2069.657768  632.146118  2069.471344  615.755768  2060.239683   \n",
       "  4    2079.517052  636.910522  2077.263512  616.872360  2068.120140   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  508  3223.176849  344.231346  3217.278610  333.917774  3222.177048   \n",
       "  509  3230.550079  345.040573  3225.118668  334.538155  3229.315170   \n",
       "  510  3240.254883  341.152527  3234.275284  328.873474  3237.312820   \n",
       "  511  3242.409134  338.646118  3236.051041  326.509232  3238.522736   \n",
       "  512  3242.133301  348.050781  3237.114151  334.716064  3240.471222   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     633.248199  2090.139872  635.564018  2065.181509   631.588058  ...   \n",
       "  1     678.958984  2247.974380  670.283920  2270.608597   691.898529  ...   \n",
       "  2     611.020432  2046.777321  611.596886  2029.764225   612.309410  ...   \n",
       "  3     616.172714  2055.892130  614.989334  2039.544353   616.238983  ...   \n",
       "  4     615.758347  2052.623749  614.948654  2028.613693   612.502869  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  508   333.151897  3150.925865  339.726830  3208.905304   341.628334  ...   \n",
       "  509   333.598854  3158.825844  339.627899  3215.405685   341.198723  ...   \n",
       "  510   328.463150  3167.988304  338.522713  3222.672089   339.324287  ...   \n",
       "  511   326.117271  3169.678200  338.514801  3222.783585   339.649490  ...   \n",
       "  512   334.781677  3172.268333  339.826202  3227.442032   339.373344  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2062.487637   861.687805  2158.551155  1000.665833   2094.654152   \n",
       "  1    2080.679621   839.286346  2159.221397  1019.756622   2145.432747   \n",
       "  2    2044.422338   930.468170  2161.124283  1031.932648   2158.503799   \n",
       "  3    2035.000000   980.385193  2164.881149  1040.028015   2088.256584   \n",
       "  4    2017.681115   967.760803  2161.918091  1040.149780   2108.058723   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  508  3169.450188   630.791962  3196.987595   786.458618   3145.960045   \n",
       "  509  3176.387711   633.957825  3203.199814   788.557922   3152.447418   \n",
       "  510  3187.017914   633.329895  3209.821899   794.274719   3162.818741   \n",
       "  511  3191.423752   631.576721  3209.076675   794.087402   3174.259674   \n",
       "  512  3193.275848   642.247925  3205.748215   799.028564   3180.425217   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1026.980103  2173.803955  1243.758545   2083.518799   1281.463440  \n",
       "  1     1020.870209  2166.561546  1261.481873   2136.822067   1271.301636  \n",
       "  2     1041.582764  2164.712372  1263.305786   2155.482483   1279.846924  \n",
       "  3     1097.771484  2163.357529  1268.594849   2073.537895   1294.963806  \n",
       "  4     1077.239136  2160.620590  1272.119385   2085.708717   1301.223389  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  508    792.123413  3200.478073   956.776550   3087.443233    959.817749  \n",
       "  509    791.780762  3200.771606   957.595337   3092.321640    959.297913  \n",
       "  510    795.094849  3200.754089   961.553101   3095.649681    967.367920  \n",
       "  511    793.913757  3198.214203   958.169189   3097.649025    962.011719  \n",
       "  512    802.789185  3204.653870   957.128784   3097.247116    956.902466  \n",
       "  \n",
       "  [513 rows x 34 columns],\n",
       "  0),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1560.266998  1079.263077  1564.444946  1065.599541  1565.654297   \n",
       "  1    1579.433548  1102.907440  1587.307648  1087.771477  1579.751602   \n",
       "  2    1542.960098  1105.937698  1542.760117  1094.098679  1542.932297   \n",
       "  3    1618.691589  1104.555298  1615.348816  1087.838287  1619.770447   \n",
       "  4    1533.006897  1098.330513  1542.778458  1085.177269  1534.910614   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  406   314.987659   883.016876   318.134783   873.466892   315.418663   \n",
       "  407   306.982081   911.973404   307.702013   902.666183   306.917512   \n",
       "  408   312.986412   905.761475   316.220913   895.437336   311.104720   \n",
       "  409   312.921524   895.609079   313.041843   887.308136   311.764294   \n",
       "  410   306.827793   933.631893   309.618753   921.521706   304.983774   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1064.903511  1598.587433  1071.107018  1643.204071  1059.751534  ...   \n",
       "  1    1087.632645  1623.703583  1085.105568  1608.559204  1083.696762  ...   \n",
       "  2    1090.810364  1590.441071  1088.517410  1618.920258  1068.318344  ...   \n",
       "  3    1090.845383  1563.944427  1080.321266  1592.107666  1082.539047  ...   \n",
       "  4    1081.694450  1601.753967  1079.985611  1616.372894  1066.664505  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  406   871.426510   325.018505   872.387081   320.183903   871.267609  ...   \n",
       "  407   901.377869   308.087876   905.219765   314.942393   905.039787  ...   \n",
       "  408   894.999416   316.370481   900.116505   313.185026   885.189487  ...   \n",
       "  409   886.597807   306.930217   894.595964   302.738270   890.268068  ...   \n",
       "  410   920.824108   312.490547   923.132599   301.000000   916.556812  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1607.440582  1389.734741  1542.724228  1511.509766   1558.775253   \n",
       "  1    1560.777161  1400.541321  1558.714417  1509.216797   1545.649841   \n",
       "  2    1562.854080  1330.150146  1459.574539  1322.615204   1442.439255   \n",
       "  3    1571.344254  1368.151398  1543.184006  1454.404846   1535.912216   \n",
       "  4    1609.290192  1400.490051  1554.073441  1504.821838   1550.570526   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  406   326.259838  1085.853973   347.089836  1217.488617    351.301979   \n",
       "  407   326.264204  1086.102341   345.044308  1214.702972    347.458836   \n",
       "  408   315.276400  1067.702255   340.471432  1205.463104    342.144642   \n",
       "  409   306.051596  1078.793884   334.715302  1206.735413    337.097435   \n",
       "  410   306.281796  1087.231049   335.228943  1212.518646    337.973682   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1507.240601  1533.779617  1495.865051   1544.475922   1498.782166  \n",
       "  1     1503.802490  1535.505615  1519.561584   1516.243561   1515.956238  \n",
       "  2     1301.806183  1466.860657  1315.822449   1450.233833   1298.725494  \n",
       "  3     1451.341064  1519.034042  1431.189453   1523.906082   1434.372711  \n",
       "  4     1494.077332  1530.768539  1490.173859   1524.330902   1487.177643  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  406   1215.884277   372.890808  1349.222351    370.118889   1346.524414  \n",
       "  407   1213.348419   369.646461  1345.604309    368.589607   1343.333649  \n",
       "  408   1205.181580   368.736801  1341.503021    368.822266   1339.132111  \n",
       "  409   1207.450195   366.440208  1335.319275    367.246162   1333.851715  \n",
       "  410   1210.666016   363.114887  1340.426453    364.609612   1338.092438  \n",
       "  \n",
       "  [411 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1551.497543  1002.257965  1559.959915   993.982620  1557.385620   \n",
       "  1    1566.793991  1043.230766  1574.667404  1034.611626  1573.299881   \n",
       "  2    1610.011200  1041.737381  1607.297852  1030.713234  1616.839294   \n",
       "  3    1590.723602  1068.459564  1589.328995  1056.012726  1591.796127   \n",
       "  4    1594.188324  1060.633942  1601.626678  1046.315460  1588.617065   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  411   323.095418   925.298145   321.256030   916.198053   322.680900   \n",
       "  412   334.609365   930.516529   332.506199   921.561939   332.803381   \n",
       "  413   332.228863   928.282232   332.705181   919.526051   329.853914   \n",
       "  414   320.891237   941.027775   320.351397   930.994047   320.194496   \n",
       "  415   340.662333  1026.039856   339.814335  1014.993347   338.074913   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     990.772186  1601.981171  1010.214920  1626.732819   999.675392  ...   \n",
       "  1    1029.070206  1624.947845  1049.610161  1630.786591  1020.815605  ...   \n",
       "  2    1030.772209  1613.781036  1039.822235  1639.071564  1038.231178  ...   \n",
       "  3    1055.066620  1633.358826  1044.852394  1640.156311  1031.834923  ...   \n",
       "  4    1048.961227  1633.868683  1036.486717  1613.477692  1035.462048  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  411   915.549835   318.750595   919.237415   347.922421   919.640213  ...   \n",
       "  412   921.048441   322.184624   926.248665   339.764238   923.590775  ...   \n",
       "  413   918.936893   328.299082   924.530396   335.983700   923.881136  ...   \n",
       "  414   930.846361   316.916394   936.481007   329.933087   936.440365  ...   \n",
       "  415  1013.923119   331.219707  1017.285934   333.509450  1001.786949  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1538.994415  1212.491211  1497.125397  1212.946075   1463.604088   \n",
       "  1    1555.790665  1240.688141  1489.509048  1236.940399   1464.613838   \n",
       "  2    1545.488007  1245.833771  1505.970947  1223.037323   1479.285141   \n",
       "  3    1580.186066  1237.876984  1497.451843  1230.239746   1478.935852   \n",
       "  4    1618.539398  1337.251160  1528.357559  1389.325928   1546.844589   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  411   362.616310  1164.470093   352.295883  1311.043274    373.695309   \n",
       "  412   346.399708  1168.293701   363.311123  1314.848206    367.174397   \n",
       "  413   339.101370  1168.631104   364.242336  1315.209839    366.641190   \n",
       "  414   334.346165  1167.542755   357.850067  1312.957184    362.022259   \n",
       "  415   338.084470  1195.475189   360.547676  1327.796539    360.718445   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1201.649078  1437.207703  1225.993622   1428.212849   1224.012390  \n",
       "  1     1222.367767  1484.404716  1227.124451   1449.955269   1221.682281  \n",
       "  2     1218.834717  1500.338943  1236.564178   1458.770828   1220.884338  \n",
       "  3     1212.560364  1445.837509  1238.885284   1436.034592   1233.688385  \n",
       "  4     1382.781616  1517.822281  1383.899567   1515.376236   1370.458435  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  411   1308.003387   394.486855  1436.921997    397.452919   1433.067749  \n",
       "  412   1313.198547   401.523605  1439.685974    398.169373   1435.226624  \n",
       "  413   1313.369995   400.983017  1447.834229    398.522087   1443.318542  \n",
       "  414   1311.118500   401.987556  1441.101807    397.368568   1438.369995  \n",
       "  415   1328.945526   397.063705  1453.622925    394.333282   1453.745911  \n",
       "  \n",
       "  [416 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1658.573486  1014.815582  1660.749603   998.843964  1661.739014   \n",
       "  1    1663.268158  1032.406723  1662.709961  1016.395370  1664.006073   \n",
       "  2    1656.727661  1017.639114  1657.521210  1004.483521  1658.700562   \n",
       "  3    1647.507141  1018.785995  1648.701782  1005.181671  1649.518341   \n",
       "  4    1624.775421   996.457199  1629.284851   985.368629  1632.411407   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  415   306.847483   664.485783   306.872592   657.008780   305.153645   \n",
       "  416   304.938703   669.705873   306.875106   660.355964   302.183558   \n",
       "  417   337.929531   755.969299   336.569166   747.294060   336.306881   \n",
       "  418   326.645908   734.051598   329.573458   725.257465   321.568851   \n",
       "  419   324.672475   749.074978   325.780159   738.090225   320.520892   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1000.810883  1704.322632  989.371307  1725.615021   982.910637  ...   \n",
       "  1    1017.108124  1702.810425  999.640945  1729.577423   988.965141  ...   \n",
       "  2    1002.997543  1697.603363  997.736084  1734.371887   979.926193  ...   \n",
       "  3    1003.205276  1700.422211  996.208572  1727.804840   979.604416  ...   \n",
       "  4     985.310776  1670.296875  999.627213  1725.923279   977.910141  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  415   657.410303   301.816625  658.943016   312.871244   656.971973  ...   \n",
       "  416   659.884156   309.590963  665.749716   310.882009   664.437605  ...   \n",
       "  417   746.717964   323.700085  745.577843   324.076008   746.315178  ...   \n",
       "  418   725.078648   330.705357  729.049026   313.318744   732.339603  ...   \n",
       "  419   738.591404   321.524432  736.815247   316.421499   744.920128  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1683.771576  1369.098114  1569.295105  1440.102295   1576.647461   \n",
       "  1    1733.468719  1347.942566  1621.949326  1401.963379   1638.059937   \n",
       "  2    1725.894989  1327.193451  1643.018219  1435.533752   1671.798706   \n",
       "  3    1715.168823  1365.915710  1585.509567  1429.394226   1606.294586   \n",
       "  4    1722.248810  1389.848083  1573.619568  1425.246399   1595.892624   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  415   347.503616   899.404633   358.084202  1039.297333    369.817505   \n",
       "  416   349.140175   930.063934   367.916115  1069.489990    369.576096   \n",
       "  417   343.412167   921.152573   368.367046  1064.208679    368.553520   \n",
       "  418   332.224583   926.338333   369.367855  1067.656830    361.296722   \n",
       "  419   336.721821   929.850204   366.433857  1061.510712    369.917046   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1434.831909  1562.722427  1438.800171   1558.379700   1436.899841  \n",
       "  1     1383.877014  1601.859680  1421.750366   1615.165604   1414.701172  \n",
       "  2     1421.715454  1614.397781  1435.926941   1636.209473   1426.190186  \n",
       "  3     1408.131165  1577.864319  1433.846191   1584.867767   1427.082214  \n",
       "  4     1406.165649  1557.699539  1426.455017   1579.931503   1421.601868  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  415   1036.694397   378.650276  1184.925049    383.520950   1181.055542  \n",
       "  416   1070.048950   390.960411  1202.810425    386.135788   1201.226135  \n",
       "  417   1064.816498   393.098595  1199.239868    386.571030   1198.329102  \n",
       "  418   1069.666931   395.590218  1198.458557    392.104874   1195.171906  \n",
       "  419   1061.218903   390.799713  1192.056091    390.546082   1191.280640  \n",
       "  \n",
       "  [420 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1697.539154  1099.766035  1704.279022  1096.383875  1694.346542   \n",
       "  1    1694.968414  1100.479076  1701.838043  1096.859110  1693.245483   \n",
       "  2    1708.156403   976.427624  1709.662445   959.987877  1710.743317   \n",
       "  3    1714.677765   986.059601  1709.827148   966.307884  1715.208313   \n",
       "  4    1701.735077   979.681870  1697.882782   961.319565  1703.110870   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  364   391.139652   811.420040   387.739624   801.580083   388.482037   \n",
       "  365   364.465076   830.171642   360.945650   818.969337   363.269178   \n",
       "  366   360.396349   827.523598   357.850855   816.759289   358.663488   \n",
       "  367   379.599260   852.190017   377.533913   843.899533   377.611792   \n",
       "  368   380.531300   867.000000   378.196123   867.000000   380.349472   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1096.692642  1720.613525  1092.076220  1704.529083  1100.691256  ...   \n",
       "  1    1096.579906  1717.336884  1093.825749  1702.398773  1094.666367  ...   \n",
       "  2     963.453918  1736.050537   953.816093  1752.297699   957.058033  ...   \n",
       "  3     970.502907  1727.602844   953.034401  1780.209747   953.339233  ...   \n",
       "  4     964.106682  1718.063202   953.082306  1777.742920   950.040215  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  364   801.525056   360.860816   807.558309   379.235889   816.248689  ...   \n",
       "  365   819.242311   351.582309   821.699545   368.195080   817.498257  ...   \n",
       "  366   817.185312   353.698243   820.419914   363.196633   816.220757  ...   \n",
       "  367   844.641247   362.554383   848.130428   366.450438   843.903947  ...   \n",
       "  368   867.000000   357.603657   867.000000   367.948666   867.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1722.078125  1170.961411  1697.603165  1273.329773   1696.121521   \n",
       "  1    1716.948975  1197.047562  1703.305634  1290.707504   1697.041351   \n",
       "  2    1754.695190  1229.157623  1700.956146  1295.481598   1696.139557   \n",
       "  3    1789.102051  1295.878693  1706.241058  1325.825256   1694.901306   \n",
       "  4    1765.241119  1298.590759  1734.642181  1364.200104   1689.529175   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  364   368.485662  1179.166809   361.654274  1290.333557    372.014250   \n",
       "  365   373.999338  1111.082520   381.039829  1249.049438    378.934675   \n",
       "  366   372.469742  1116.348175   376.638203  1243.252655    383.138763   \n",
       "  367   369.035628  1072.255188   371.050913  1209.239197    383.605499   \n",
       "  368   367.465937  1077.857208   364.915033  1213.259521    383.602306   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1277.219742  1594.628952  1449.628571   1603.161118   1447.325592  \n",
       "  1     1294.124619  1605.004486  1457.471375   1607.860077   1458.316895  \n",
       "  2     1292.534668  1625.471710  1474.091187   1610.813843   1472.506897  \n",
       "  3     1307.252228  1643.294495  1487.191406   1616.338516   1479.383606  \n",
       "  4     1315.491394  1694.501953  1508.060120   1618.709061   1492.867065  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  364   1293.038910   381.449253  1325.641113    382.896275   1326.329285  \n",
       "  365   1245.864929   397.234016  1334.306519    390.122009   1328.432434  \n",
       "  366   1237.018250   399.378056  1338.841736    399.925808   1333.066956  \n",
       "  367   1206.095001   398.617661  1331.949921    403.046299   1329.797180  \n",
       "  368   1210.646088   387.627285  1335.940277    398.672680   1333.673370  \n",
       "  \n",
       "  [369 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1687.317596  876.125309  1694.119629  867.765972  1686.783722   \n",
       "  1    1697.522339  935.751701  1708.617340  922.571869  1693.879395   \n",
       "  2    1666.647461  941.893433  1670.251282  923.018623  1670.925873   \n",
       "  3    1664.280121  948.007904  1665.066010  931.286690  1665.022949   \n",
       "  4    1654.840942  953.078781  1656.671967  936.246307  1653.524963   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  334   319.116770  832.000000   318.274891  832.000000   318.587421   \n",
       "  335   319.064899  835.000000   319.742899  835.000000   316.848046   \n",
       "  336   313.134212  836.000000   314.357563  836.000000   311.283223   \n",
       "  337   319.914557  854.000000   321.737617  854.000000   318.766310   \n",
       "  338   317.071049  864.000000   319.478857  864.000000   315.731136   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     869.049755  1709.473419  867.582600  1696.696320   860.283234  ...   \n",
       "  1     920.200588  1725.972870  921.867580  1690.369690   916.336945  ...   \n",
       "  2     924.702919  1703.279236  920.950081  1716.682373   921.427223  ...   \n",
       "  3     933.346306  1692.100464  919.459442  1709.406586   923.192490  ...   \n",
       "  4     939.460098  1686.342529  919.673676  1698.075745   927.934685  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  334   832.000000   298.000000  832.000000   310.361906   832.000000  ...   \n",
       "  335   835.000000   311.572385  835.000000   305.862186   835.000000  ...   \n",
       "  336   836.000000   307.369289  836.000000   302.279163   836.000000  ...   \n",
       "  337   854.000000   312.932751  854.000000   306.793295   854.000000  ...   \n",
       "  338   864.000000   315.801988  864.000000   304.205411   864.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1685.905609  1116.166382  1665.637878  1268.360443   1646.733978   \n",
       "  1    1680.451782  1119.245102  1647.525116  1252.225098   1654.289124   \n",
       "  2    1717.142487  1223.036560  1690.446289  1297.440735   1658.068817   \n",
       "  3    1708.809692  1229.489563  1723.573547  1364.003296   1644.053436   \n",
       "  4    1705.564087  1212.700989  1719.088684  1394.731445   1640.004547   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  334   320.257866  1056.377121   314.435083  1191.625488    319.296116   \n",
       "  335   303.004514  1049.461563   314.136354  1189.642944    303.146705   \n",
       "  336   296.000000  1055.382294   308.870263  1192.902954    301.864064   \n",
       "  337   302.000000  1066.762314   309.541153  1201.138947    302.000000   \n",
       "  338   302.000000  1055.521179   309.845677  1195.139160    302.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1248.120697  1559.048386  1440.461060   1546.947174   1432.619202  \n",
       "  1     1254.668091  1547.729675  1446.431641   1550.965057   1448.627930  \n",
       "  2     1256.320496  1574.303574  1467.998230   1552.899811   1468.130737  \n",
       "  3     1269.953094  1679.568726  1492.237000   1561.666763   1476.019226  \n",
       "  4     1273.709137  1683.588135  1502.544434   1573.319122   1485.768311  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  334   1188.575439   344.195518  1314.944489    346.078621   1313.307404  \n",
       "  335   1188.069366   342.108059  1313.294159    336.116550   1312.233704  \n",
       "  336   1190.922028   337.922291  1308.903046    335.086796   1308.078369  \n",
       "  337   1200.404755   338.028770  1315.782471    332.142393   1316.460846  \n",
       "  338   1194.437195   344.245232  1313.171265    342.309940   1312.169708  \n",
       "  \n",
       "  [339 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1688.112900  1081.542969  1691.545502  1068.411377  1691.536407   \n",
       "  1    1682.150177  1094.097672  1686.095032  1077.885620  1680.645752   \n",
       "  2    1671.819611  1081.349670  1674.410065  1066.492371  1672.470764   \n",
       "  3    1670.954620  1090.827133  1672.418365  1074.892685  1669.590103   \n",
       "  4    1623.370270  1083.271774  1629.898560  1068.034195  1621.879898   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  355   319.259233   872.587753   314.487906   860.648380   320.273432   \n",
       "  356   320.785433   865.018707   317.387575   854.313238   320.778320   \n",
       "  357   314.097138   868.141373   312.310094   857.421949   314.198107   \n",
       "  358   309.029521   874.918583   308.337192   864.018347   308.147214   \n",
       "  359   314.001413   869.628265   312.489167   859.403004   314.094658   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1068.625397  1726.003021  1059.873703  1727.934326  1065.315094  ...   \n",
       "  1    1079.892395  1717.648132  1062.452568  1724.965240  1068.223663  ...   \n",
       "  2    1068.192841  1699.907349  1057.305687  1720.971069  1063.460945  ...   \n",
       "  3    1077.459274  1692.195557  1059.515465  1710.585846  1066.088394  ...   \n",
       "  4    1070.496246  1670.208618  1062.386086  1693.485352  1061.410866  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  355   860.791798   304.000000   865.177437   324.046671   864.367474  ...   \n",
       "  356   854.339144   305.000000   861.916101   320.968804   862.689320  ...   \n",
       "  357   857.149549   306.000000   866.309101   319.774630   865.280891  ...   \n",
       "  358   863.641455   307.000000   872.583511   318.641549   872.907459  ...   \n",
       "  359   858.691481   305.052155   868.216412   319.151743   871.564846  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1727.598846  1312.851654  1668.831924  1411.947357   1664.682785   \n",
       "  1    1741.193115  1343.952728  1739.189972  1475.630188   1675.466156   \n",
       "  2    1716.932007  1350.836334  1711.327972  1487.913269   1651.579300   \n",
       "  3    1748.004425  1315.400452  1690.023651  1404.815979   1678.163177   \n",
       "  4    1749.718628  1341.893616  1691.104126  1441.860260   1670.391327   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  355   351.709709  1118.472992   311.311197  1249.496643    368.045341   \n",
       "  356   340.294708  1113.865021   312.640252  1245.491272    367.903404   \n",
       "  357   335.364237  1132.946350   330.072199  1246.185577    366.116814   \n",
       "  358   336.840561  1131.712738   358.779007  1247.144440    363.092087   \n",
       "  359   325.673386  1163.529877   327.649326  1284.672882    341.783909   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1403.060547  1593.062469  1565.527954   1602.296616   1558.255554  \n",
       "  1     1393.246338  1629.506851  1591.459961   1597.214539   1566.228882  \n",
       "  2     1448.429535  1626.240189  1606.805786   1601.114182   1590.448792  \n",
       "  3     1399.471954  1613.134277  1604.609009   1608.803192   1597.316467  \n",
       "  4     1410.717407  1621.251541  1622.069641   1610.981537   1609.311829  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  355   1248.191803   325.678268  1375.365845    377.660873   1378.513550  \n",
       "  356   1245.193207   324.387484  1378.577942    376.889099   1382.558899  \n",
       "  357   1245.864197   342.085861  1375.184998    372.892136   1377.696655  \n",
       "  358   1246.809937   372.347168  1372.761292    371.851746   1373.614380  \n",
       "  359   1284.629639   347.515865  1380.509705    354.913136   1381.904358  \n",
       "  \n",
       "  [360 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2254.115280   946.560623  2260.246307  938.043045  2240.564575   \n",
       "  1    2280.779373   991.436096  2273.503784  970.577545  2275.169800   \n",
       "  2    2297.058792   990.939758  2294.710464  973.622162  2293.146698   \n",
       "  3    2289.430740  1008.659241  2286.264847  990.284515  2291.483841   \n",
       "  4    2296.026703   996.168228  2294.017883  978.543488  2294.783646   \n",
       "  ..           ...          ...          ...         ...          ...   \n",
       "  176  3545.403259   630.868282  3546.095215  623.366352  3545.298355   \n",
       "  177  3549.348129   626.070803  3549.533112  617.186886  3549.245010   \n",
       "  178  3557.246803   627.542881  3557.916229  619.622812  3555.295067   \n",
       "  179  3556.449493   628.002937  3559.000000  621.369232  3554.950241   \n",
       "  180  3544.349594   663.167709  3547.000000  659.410210  3541.561768   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     938.488121  2242.650970  932.430771  2218.733437   946.074280  ...   \n",
       "  1     975.213287  2206.076729  963.345078  2210.505287   970.327545  ...   \n",
       "  2     973.363113  2226.660149  964.970818  2252.098053   970.923904  ...   \n",
       "  3     988.574829  2216.008492  969.799133  2261.342453   973.656685  ...   \n",
       "  4     977.482658  2228.931023  963.375435  2253.990845   965.777847  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  176   622.556200  3524.052063  624.125042  3551.427338   626.243198  ...   \n",
       "  177   616.589653  3535.015549  617.471512  3556.775719   619.314867  ...   \n",
       "  178   619.009542  3546.749367  621.853697  3552.442955   622.230383  ...   \n",
       "  179   620.739189  3559.000000  623.144768  3549.957359   627.104294  ...   \n",
       "  180   659.248123  3547.000000  659.167885  3538.632469   657.358376  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2189.653740  1207.564667  2283.979645  1343.947998   2259.213272   \n",
       "  1    2189.432663  1360.618713  2302.607208  1359.019653   2265.221680   \n",
       "  2    2178.152390  1313.669128  2301.263733  1397.255554   2256.885788   \n",
       "  3    2202.749786  1349.320007  2317.842484  1395.474854   2285.250626   \n",
       "  4    2201.489944  1340.950378  2313.591553  1399.641785   2274.703766   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  176  3543.090111   869.177917  3483.699364  1004.332733   3537.052849   \n",
       "  177  3541.100533   876.904633  3528.925438  1008.450775   3537.346573   \n",
       "  178  3539.839569   862.656982  3543.679909   992.279388   3548.830261   \n",
       "  179  3543.498032   863.510529  3557.488319   962.949371   3554.185051   \n",
       "  180  3547.000000   881.952209  3547.000000   911.097137   3547.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1364.117920  2384.275757  1585.013428   2344.441223   1605.756714  \n",
       "  1     1396.397278  2344.818161  1581.504028   2305.494537   1599.736023  \n",
       "  2     1422.082886  2359.679108  1599.255798   2292.710785   1621.751099  \n",
       "  3     1414.875244  2370.712921  1606.649841   2310.113266   1618.783508  \n",
       "  4     1422.255249  2371.713440  1613.673462   2323.562790   1628.285828  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  176   1005.854065  3461.288841  1124.415161   3524.598854   1124.275757  \n",
       "  177   1010.419250  3494.437618  1111.395630   3489.862362   1109.943481  \n",
       "  178    996.273132  3515.236279  1105.809631   3526.287682   1113.207336  \n",
       "  179    967.431427  3548.500870   983.641785   3545.097809    993.811035  \n",
       "  180    919.740784  3521.615326   908.314911   3519.300888    914.563812  \n",
       "  \n",
       "  [181 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    2114.709976   928.736679  2121.640121   917.713562  2097.248169   \n",
       "  1    2135.289200   997.915466  2138.893845   978.478226  2118.153702   \n",
       "  2    2140.152573  1004.560257  2146.554443   984.969688  2122.489059   \n",
       "  3    2122.776764  1024.327118  2120.308395  1003.091141  2116.286179   \n",
       "  4    2140.689316  1018.923576  2138.437012  1001.523720  2132.393295   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  158  3451.346161   583.974289  3452.227760   577.362816  3454.959511   \n",
       "  159  3456.726799   582.197193  3457.064209   574.501907  3458.707970   \n",
       "  160  3488.960388   584.589592  3488.492043   576.089287  3488.025078   \n",
       "  161  3488.711517   595.535328  3492.000000   588.098614  3486.789276   \n",
       "  162  3486.003296   600.455743  3488.756714   594.000000  3483.156952   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     920.704231  2113.162460  907.640450  2067.215424   871.793348  ...   \n",
       "  1     982.071007  2116.717499  966.287834  2056.953659   966.414261  ...   \n",
       "  2     987.269043  2135.164658  979.006516  2066.148132   978.486610  ...   \n",
       "  3    1006.977432  2080.724045  985.966637  2073.202866   978.641518  ...   \n",
       "  4    1005.748749  2097.101959  984.069702  2082.806175   981.824112  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  158   576.386349  3448.979805  579.995453  3488.410439   580.544418  ...   \n",
       "  159   572.604670  3455.663429  581.815773  3491.981827   579.689579  ...   \n",
       "  160   575.003397  3468.811646  578.847387  3487.573807   578.434568  ...   \n",
       "  161   585.834709  3488.705215  588.058001  3482.105347   581.085749  ...   \n",
       "  162   594.000000  3489.054626  594.000000  3478.135239   594.986922  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2051.730545  1174.361206  2164.273666  1315.136719   2169.972946   \n",
       "  1    2070.385956  1249.056763  2200.668488  1346.662567   2194.111938   \n",
       "  2    2047.906105  1264.676819  2191.786804  1349.211884   2183.273041   \n",
       "  3    2034.028221  1341.229645  2182.991226  1333.168640   2186.560532   \n",
       "  4    2068.523140  1281.788177  2199.304092  1376.358551   2196.575058   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  158  3485.977791   818.589508  3426.280933   955.010071   3480.953247   \n",
       "  159  3487.966072   798.573303  3432.860386   938.805847   3479.546326   \n",
       "  160  3471.739937   811.679016  3445.125023   945.376770   3457.117523   \n",
       "  161  3464.882019   796.324509  3459.203011   924.816467   3455.120979   \n",
       "  162  3473.156700   816.351593  3483.342018   938.364044   3458.724640   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1330.945770  2320.954681  1536.220581   2322.210052   1539.970276  \n",
       "  1     1366.635986  2311.932159  1549.987549   2314.494080   1557.953491  \n",
       "  2     1355.406006  2314.826630  1565.103210   2311.847260   1572.384827  \n",
       "  3     1358.601929  2307.813416  1573.155029   2299.540619   1587.050964  \n",
       "  4     1381.326752  2293.843018  1587.460388   2298.036743   1591.916443  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  158    953.729828  3426.902256  1086.956421   3471.567337   1087.328857  \n",
       "  159    937.684113  3425.734356  1070.688110   3460.322800   1070.871033  \n",
       "  160    947.058960  3428.408745  1071.464783   3430.740372   1075.207275  \n",
       "  161    924.941711  3438.358196  1047.935883   3435.219704   1049.194489  \n",
       "  162    937.134003  3462.428493  1050.053009   3439.058575   1048.281219  \n",
       "  \n",
       "  [163 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2229.795975  977.099197  2232.699173  963.708710  2220.831909   \n",
       "  1    2242.176987  992.111923  2248.503235  974.962563  2231.077637   \n",
       "  2    2230.856949  956.561127  2235.616745  940.724098  2217.525314   \n",
       "  3    2241.391083  967.718536  2238.117065  951.867790  2236.586685   \n",
       "  4    2255.102478  935.064751  2254.658478  920.012051  2252.611450   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  156  3552.523590  568.991814  3556.207657  562.966206  3554.824570   \n",
       "  157  3535.833488  578.578907  3534.963493  569.514782  3542.016037   \n",
       "  158  3549.609238  575.301323  3550.923477  567.888906  3553.846359   \n",
       "  159  3557.169411  572.670860  3560.359589  566.757664  3558.934059   \n",
       "  160  3561.438721  572.699150  3565.250298  566.983429  3563.224586   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     964.516083  2195.272064  949.534500  2160.147308   953.852730  ...   \n",
       "  1     980.068115  2209.705948  938.520454  2150.333344   948.162239  ...   \n",
       "  2     945.453728  2202.664459  922.863823  2158.370834   944.117546  ...   \n",
       "  3     955.478653  2171.181953  937.693588  2179.327171   950.839470  ...   \n",
       "  4     921.159805  2189.412376  906.862358  2190.724762   947.400002  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  156   562.254793  3551.209625  559.213827  3548.789368   563.249527  ...   \n",
       "  157   567.382681  3533.840591  566.251062  3569.000000   571.625839  ...   \n",
       "  158   564.991920  3547.982536  564.828161  3566.259811   565.457779  ...   \n",
       "  159   565.811108  3559.324379  565.804058  3567.101669   567.119366  ...   \n",
       "  160   566.213993  3564.214470  567.325306  3565.588455   567.846951  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2113.646622  1202.424103  2172.814796  1227.071075   2160.916122   \n",
       "  1    2115.274826  1226.476898  2197.939041  1308.721191   2186.650497   \n",
       "  2    2103.821949  1202.917847  2187.478653  1254.687836   2144.080910   \n",
       "  3    2149.448761  1218.964966  2232.252945  1329.704010   2210.745148   \n",
       "  4    2141.445450  1203.584991  2233.299759  1323.897278   2213.520798   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  156  3515.367126   786.136459  3498.699287   917.252655   3504.866142   \n",
       "  157  3562.072128   780.836487  3492.911667   910.128265   3543.363159   \n",
       "  158  3541.814041   782.278046  3514.700073   907.978729   3536.937691   \n",
       "  159  3547.784180   781.501282  3532.702728   903.450867   3548.412010   \n",
       "  160  3544.564728   789.968506  3530.513794   897.275085   3536.343323   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1222.862885  2301.704529  1452.356934   2305.682404   1450.555359  \n",
       "  1     1307.334473  2312.268677  1503.237244   2316.696960   1498.497314  \n",
       "  2     1263.453461  2312.781830  1511.143066   2306.790436   1517.395447  \n",
       "  3     1350.804047  2284.863007  1522.359985   2258.337540   1546.282166  \n",
       "  4     1347.480865  2290.290497  1519.244141   2281.564697   1534.042053  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  156    916.753876  3469.323284  1040.241608   3467.897427   1039.278320  \n",
       "  157    912.874359  3481.696815  1039.055176   3526.157051   1041.305939  \n",
       "  158    908.802795  3511.342834  1039.643372   3521.553627   1040.467712  \n",
       "  159    905.543671  3527.609230  1020.481232   3537.384117   1022.576721  \n",
       "  160    897.802246  3517.742432   971.588043   3524.215729    973.024933  \n",
       "  \n",
       "  [161 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2196.818344  936.406326  2193.488586  922.069481  2188.848137   \n",
       "  1    2217.618927  940.526535  2213.519913  926.431076  2210.981865   \n",
       "  2    2244.901718  924.198380  2237.857498  908.764336  2230.370544   \n",
       "  3    2222.621956  958.744049  2220.747223  944.419235  2221.102715   \n",
       "  4    2238.678528  970.776627  2240.839340  954.999374  2236.762253   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  144  3524.328979  630.088531  3521.645294  619.016403  3529.980972   \n",
       "  145  3543.101318  624.046638  3541.086929  611.783409  3547.518646   \n",
       "  146  3542.945541  614.384613  3539.465912  606.749931  3547.050812   \n",
       "  147  3544.504059  606.479313  3541.717216  599.094109  3548.550186   \n",
       "  148  3530.657326  611.825317  3531.915970  602.458593  3530.209984   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     922.636398  2149.758392  918.385727  2138.149006   919.261635  ...   \n",
       "  1     927.325531  2159.241638  921.430954  2155.834091   926.725540  ...   \n",
       "  2     912.370201  2186.908997  912.814064  2172.025635   926.211052  ...   \n",
       "  3     944.782715  2163.513985  928.365532  2180.851471   935.093903  ...   \n",
       "  4     954.668823  2187.137512  935.037659  2194.927536   938.659630  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  144   619.607960  3463.075676  600.614540  3519.528671   607.196129  ...   \n",
       "  145   611.651573  3476.493546  594.098618  3536.503586   604.648930  ...   \n",
       "  146   605.745964  3499.985497  597.542751  3548.255615   600.263329  ...   \n",
       "  147   598.662376  3511.634949  595.091901  3556.552521   599.795485  ...   \n",
       "  148   602.962133  3522.064087  598.143359  3556.379105   604.949244  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2117.112583  1233.878174  2229.328110  1327.217712   2124.132603   \n",
       "  1    2109.054995  1257.855072  2215.411011  1293.926788   2123.912354   \n",
       "  2    2112.705282  1266.476868  2220.800308  1306.162659   2122.765259   \n",
       "  3    2128.202187  1267.554626  2223.638329  1334.321899   2131.773365   \n",
       "  4    2117.845366  1270.997559  2200.794693  1291.456055   2158.625584   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  144  3483.847626   820.668396  3440.867508   935.244141   3471.610420   \n",
       "  145  3492.334198   830.210846  3449.095764   932.632690   3487.295647   \n",
       "  146  3516.528717   808.048828  3452.785801   933.706299   3501.395065   \n",
       "  147  3530.496048   835.080734  3461.560230   944.392914   3512.557602   \n",
       "  148  3542.779175   845.782928  3480.088551   936.757996   3508.224686   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1447.282227  2287.397476  1488.482971   2220.864929   1500.856262  \n",
       "  1     1477.800110  2278.214035  1480.463867   2161.939568   1523.932556  \n",
       "  2     1479.001709  2267.405075  1486.778198   2139.844158   1544.564148  \n",
       "  3     1453.570007  2261.563644  1500.056152   2151.031845   1544.406799  \n",
       "  4     1365.056213  2245.228180  1493.746643   2192.090218   1518.739014  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  144    942.041779  3437.136581  1061.338806   3448.422188   1077.009155  \n",
       "  145    940.111694  3452.809044  1051.262543   3463.575027   1058.486420  \n",
       "  146    932.695465  3451.588688  1067.920959   3500.352051   1069.403168  \n",
       "  147    954.257446  3456.090630  1073.809784   3482.246582   1073.179047  \n",
       "  148    945.458069  3457.773487  1060.827332   3465.519775   1068.757507  \n",
       "  \n",
       "  [149 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2158.330139  804.971802  2153.277901  788.715866  2155.717178   \n",
       "  1    2185.514023  800.425201  2180.316673  785.160172  2180.130180   \n",
       "  2    2204.841415  798.294937  2196.152542  784.512054  2199.196396   \n",
       "  3    2232.375015  830.958847  2231.664398  816.352432  2230.134354   \n",
       "  4    2252.960297  841.756882  2254.071609  826.933350  2251.473160   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  123  3449.367889  512.504555  3447.298782  501.896214  3451.990936   \n",
       "  124  3439.873138  506.966209  3438.912933  498.723433  3444.704437   \n",
       "  125  3445.603783  503.452467  3445.305992  496.740538  3448.101837   \n",
       "  126  3442.890404  520.054169  3442.722610  513.718636  3444.427086   \n",
       "  127  3456.760590  514.391031  3459.000000  508.455716  3455.583908   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     789.159943  2117.572468  775.614120  2132.654274   773.949646  ...   \n",
       "  1     786.334930  2140.722481  775.734329  2142.445732   776.305908  ...   \n",
       "  2     784.679230  2146.293922  783.544724  2164.976822   786.529358  ...   \n",
       "  3     815.123383  2177.546364  804.250992  2192.861763   807.875076  ...   \n",
       "  4     824.905014  2198.726158  812.443878  2211.958908   817.171082  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  123   500.768890  3388.859169  492.034950  3441.635223   494.848206  ...   \n",
       "  124   498.065710  3396.497955  488.989962  3442.289581   497.630556  ...   \n",
       "  125   495.223066  3419.214661  492.245991  3453.418137   494.055803  ...   \n",
       "  126   512.438717  3418.886513  507.418837  3448.163162   506.576565  ...   \n",
       "  127   506.805704  3445.582428  504.935891  3450.071487   502.482850  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2114.960697  1083.259918  2208.421310  1131.550110   2201.877335   \n",
       "  1    2117.381500  1121.604614  2215.442108  1137.747437   2190.031288   \n",
       "  2    2115.460011  1121.659149  2223.048157  1159.341034   2132.407093   \n",
       "  3    2120.381695  1141.763489  2221.283424  1184.570190   2142.268391   \n",
       "  4    2120.654613  1145.106323  2216.171600  1175.575378   2138.795345   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  123  3406.229797   741.675262  3365.850044   862.747772   3402.324776   \n",
       "  124  3409.001602   727.295502  3363.638508   856.163849   3411.167725   \n",
       "  125  3435.390305   738.550842  3376.995007   864.279327   3427.888321   \n",
       "  126  3432.895691   733.049683  3383.858963   871.844177   3413.131744   \n",
       "  127  3429.935997   732.364166  3395.434925   870.568237   3412.518608   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1143.837250  2284.421158  1334.314636   2281.303314   1338.834229  \n",
       "  1     1172.646423  2281.731949  1339.778870   2272.587875   1353.941040  \n",
       "  2     1250.172668  2270.585114  1355.713135   2144.530983   1398.931519  \n",
       "  3     1345.345947  2258.259659  1380.838806   2178.838203   1432.169617  \n",
       "  4     1335.744629  2260.382706  1380.134766   2161.811779   1443.812744  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  123    865.675446  3374.728569   980.962769   3390.739090    990.467529  \n",
       "  124    854.872894  3382.014709   984.084106   3413.868736    985.083374  \n",
       "  125    863.625397  3385.210732   995.813110   3424.615295    996.709900  \n",
       "  126    870.794708  3384.235130  1004.327148   3403.881989   1004.933411  \n",
       "  127    872.771912  3386.977463  1003.029663   3395.193924   1006.543610  \n",
       "  \n",
       "  [128 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2184.504478  898.984177  2184.343719  883.329170  2173.967751   \n",
       "  1    2197.268631  899.795830  2193.891716  887.724937  2192.489426   \n",
       "  2    2195.275627  913.150955  2194.195129  900.089149  2192.823517   \n",
       "  3    2193.552681  923.554062  2193.342598  910.330215  2193.653702   \n",
       "  4    2213.237366  923.007568  2212.669556  909.702835  2213.045891   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  104  3507.366287  579.446609  3503.924911  568.559155  3509.162643   \n",
       "  105  3517.004349  573.003880  3514.071365  563.830956  3519.450439   \n",
       "  106  3505.166412  580.031616  3501.411469  571.048656  3507.785629   \n",
       "  107  3516.561630  578.379719  3514.199142  569.785336  3516.450958   \n",
       "  108  3502.338448  583.580769  3500.474419  575.225510  3505.077103   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     883.080048  2163.223846  880.684525  2126.413479   881.961601  ...   \n",
       "  1     886.705467  2147.451355  881.455429  2141.748474   881.058441  ...   \n",
       "  2     899.194695  2152.205856  884.449539  2151.662006   886.871658  ...   \n",
       "  3     908.785202  2152.818016  890.982079  2165.308403   891.255737  ...   \n",
       "  4     909.901459  2159.154575  886.714996  2177.469955   893.589302  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  104   568.127117  3442.504738  560.310841  3493.752975   567.004795  ...   \n",
       "  105   562.305767  3462.923294  558.514595  3512.861023   562.271404  ...   \n",
       "  106   570.468517  3461.136574  565.231422  3509.108749   567.559128  ...   \n",
       "  107   568.728836  3479.430038  567.892330  3511.332657   569.251568  ...   \n",
       "  108   573.746098  3490.110825  576.479986  3521.626038   578.827190  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2121.143627  1125.812927  2176.316277  1204.344086   2151.703598   \n",
       "  1    2120.876699  1130.956818  2176.533012  1206.270325   2170.274750   \n",
       "  2    2127.272190  1163.084961  2190.816444  1235.565674   2186.066254   \n",
       "  3    2136.137772  1199.214722  2189.089668  1299.080994   2177.472702   \n",
       "  4    2128.275229  1155.348175  2189.927422  1251.968475   2186.796265   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  104  3457.263672   821.537079  3374.442429   967.570953   3452.616608   \n",
       "  105  3482.623535   810.978516  3403.815254   951.183502   3468.566711   \n",
       "  106  3501.707886   798.496124  3424.633125   940.814606   3494.400146   \n",
       "  107  3502.617905   805.637726  3435.068726   948.042175   3484.127853   \n",
       "  108  3506.775848   787.270416  3463.773026   915.994049   3496.778893   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1226.535645  2278.443192  1367.035156   2269.682175   1376.791504  \n",
       "  1     1224.506775  2271.655212  1383.993652   2279.724991   1390.230774  \n",
       "  2     1251.441925  2273.438797  1403.963318   2277.657166   1407.094055  \n",
       "  3     1307.468201  2260.539276  1428.310181   2267.970184   1421.934875  \n",
       "  4     1262.561523  2241.721603  1428.538391   2259.014359   1432.804138  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  104    965.605621  3370.720669  1126.869446   3426.947304   1122.745300  \n",
       "  105    951.423553  3380.920856  1098.985352   3446.899445   1095.344116  \n",
       "  106    943.264160  3393.894081  1096.288818   3475.582520   1104.943604  \n",
       "  107    948.155853  3395.285675  1097.469788   3437.968620   1099.629395  \n",
       "  108    920.696045  3446.310745  1044.938812   3472.068474   1052.162354  \n",
       "  \n",
       "  [109 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1764.653076  1207.932251  1757.441406  1188.063202  1761.461365   \n",
       "  1    1766.679993  1207.573883  1757.283936  1186.207367  1762.779663   \n",
       "  2    1764.474884  1208.420380  1757.548462  1187.528198  1760.846741   \n",
       "  3    1762.685974  1202.952835  1754.576538  1183.228271  1758.171295   \n",
       "  4    1757.340027  1200.182922  1752.149597  1180.546570  1753.559021   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  349   332.456459   869.998804   328.058603   861.111158   332.841297   \n",
       "  350   331.115217   864.796090   327.968236   863.000000   331.641253   \n",
       "  351   334.769775   872.371732   332.145555   872.000000   334.889860   \n",
       "  352   330.515997   880.318977   328.272256   879.000000   330.931656   \n",
       "  353   326.978602   885.000000   326.149519   885.000000   327.196901   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1194.169067  1806.934998  1156.519020  1818.268188  1164.930077  ...   \n",
       "  1    1192.911407  1803.936798  1152.209938  1813.780853  1152.152992  ...   \n",
       "  2    1195.249817  1800.124725  1151.500931  1804.434021  1162.277481  ...   \n",
       "  3    1188.761505  1800.445312  1150.692245  1805.450012  1154.658493  ...   \n",
       "  4    1184.538132  1803.457184  1150.995758  1822.317627  1147.312683  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  349   860.581628   314.058293   859.627289   331.684641   864.665379  ...   \n",
       "  350   863.000000   313.242948   863.000000   329.642504   863.000000  ...   \n",
       "  351   872.000000   313.598792   872.000000   327.408512   872.000000  ...   \n",
       "  352   879.000000   313.148537   879.000000   326.003728   879.000000  ...   \n",
       "  353   885.000000   315.390906   885.000000   325.163182   885.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1785.920410  1402.125732  1674.164276  1398.717377   1683.046173   \n",
       "  1    1803.357635  1496.031403  1756.534912  1635.613708   1752.020905   \n",
       "  2    1782.672699  1433.499481  1683.831238  1379.021271   1687.096466   \n",
       "  3    1789.524872  1450.568542  1686.266815  1407.136993   1678.138184   \n",
       "  4    1833.680206  1479.902924  1687.464264  1380.183594   1702.909424   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  349   349.325829  1100.931793   327.751625  1251.541901    361.832211   \n",
       "  350   343.185373  1101.879059   335.600994  1251.129028    356.162807   \n",
       "  351   333.671228  1102.393829   336.104231  1245.091827    347.811443   \n",
       "  352   327.418867  1101.513229   334.188004  1244.266388    342.113771   \n",
       "  353   321.195032  1103.354874   329.446239  1244.023224    333.102163   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1375.060028  1553.455322  1474.978912   1596.777740   1444.100464  \n",
       "  1     1618.413452  1728.605896  1607.005249   1713.203766   1590.229004  \n",
       "  2     1359.669739  1552.199585  1450.785156   1579.655212   1432.833466  \n",
       "  3     1377.596405  1590.728638  1470.631622   1636.441483   1447.591644  \n",
       "  4     1365.047089  1585.750031  1479.400085   1682.050629   1484.287262  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  349   1250.135651   373.066246  1403.720520    385.644035   1401.813904  \n",
       "  350   1249.019836   376.587334  1402.326050    382.493256   1400.413391  \n",
       "  351   1243.779053   378.467766  1396.664185    379.764252   1395.174194  \n",
       "  352   1243.226379   376.698277  1392.088806    377.294601   1391.126709  \n",
       "  353   1242.615936   371.349133  1383.958954    369.322464   1383.274567  \n",
       "  \n",
       "  [354 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1696.384003  1073.207825  1687.213593  1051.297089  1689.960114   \n",
       "  1    1681.833069  1071.204422  1672.867035  1052.825897  1676.259521   \n",
       "  2    1678.342407  1080.038940  1668.327332  1057.375229  1674.228149   \n",
       "  3    1672.949493  1083.748840  1663.431824  1060.989105  1668.142639   \n",
       "  4    1669.165802  1079.141388  1657.961639  1057.455002  1663.821777   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  369   344.918451   867.429779   341.806466   858.566753   342.644055   \n",
       "  370   341.039547   864.424055   338.878044   856.788897   338.928279   \n",
       "  371   340.881071   877.486305   339.048791   868.441542   338.904634   \n",
       "  372   354.337233   919.260279   351.328784   911.466299   353.839170   \n",
       "  373   355.847090   928.000000   353.660667   928.000000   355.720985   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1056.336960  1713.244385  1020.574081  1750.318451  1021.178200  ...   \n",
       "  1    1054.929993  1703.604431  1032.379318  1747.416260  1014.388641  ...   \n",
       "  2    1062.803543  1692.110413  1030.042679  1754.528717  1022.184906  ...   \n",
       "  3    1066.684387  1689.165314  1030.351135  1721.454346  1026.504486  ...   \n",
       "  4    1063.212036  1680.554016  1026.097656  1754.769928  1008.844803  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  369   858.463017   328.000000   866.068079   348.173517   865.480715  ...   \n",
       "  370   855.987120   328.000000   864.006245   340.868175   862.370872  ...   \n",
       "  371   868.259906   334.856828   871.948751   340.063821   867.059665  ...   \n",
       "  372   911.513902   339.895471   911.349325   352.989845   909.576337  ...   \n",
       "  373   928.000000   341.279347   928.000000   357.657845   928.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1708.095154  1443.021118  1673.728607  1522.223267   1656.908936   \n",
       "  1    1717.363983  1462.874573  1642.612564  1490.884094   1611.010666   \n",
       "  2    1726.117004  1494.249695  1724.323334  1585.809204   1714.838867   \n",
       "  3    1722.912231  1487.568542  1748.205353  1622.334778   1713.585968   \n",
       "  4    1772.140381  1460.069031  1724.901184  1551.067749   1738.465790   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  369   370.309982  1150.707672   334.309175  1276.350494    363.976223   \n",
       "  370   367.444103  1149.670319   342.666149  1273.824280    363.147335   \n",
       "  371   362.668606  1146.218079   332.010738  1281.916687    357.463543   \n",
       "  372   358.814611  1139.921280   346.290207  1277.407806    354.894045   \n",
       "  373   358.274382  1143.216995   342.432863  1278.842133    357.343094   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1514.890686  1669.272003  1513.190735   1659.192963   1496.039612  \n",
       "  1     1484.814758  1641.858261  1474.828552   1655.641754   1466.142151  \n",
       "  2     1582.953491  1713.834930  1565.683899   1697.560486   1565.132019  \n",
       "  3     1614.333557  1729.698059  1602.780640   1696.540649   1603.286377  \n",
       "  4     1549.084167  1714.628265  1530.681885   1714.636505   1530.862183  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  369   1277.135681   348.192486  1387.323425    367.745827   1387.887085  \n",
       "  370   1274.794983   358.292128  1382.396240    366.445839   1384.326599  \n",
       "  371   1281.041138   338.182146  1417.332825    360.358063   1417.077087  \n",
       "  372   1275.509613   361.315620  1418.133362    365.267412   1417.119873  \n",
       "  373   1276.812927   355.274506  1412.268127    367.332703   1411.466675  \n",
       "  \n",
       "  [374 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1708.863312  1081.247665  1705.574738  1062.487915  1707.308960   \n",
       "  1    1770.185364  1093.882584  1760.770966  1072.178680  1770.639008   \n",
       "  2    1690.416809  1091.283096  1684.872498  1073.841125  1687.836121   \n",
       "  3    1689.373596  1107.758484  1681.793152  1087.594681  1684.780121   \n",
       "  4    1682.910919  1112.491547  1676.046265  1091.500000  1681.653198   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  377   381.119972   804.000000   380.552654   804.000000   379.754877   \n",
       "  378   379.829636   827.636071   380.322544   819.740901   378.307827   \n",
       "  379   380.904882   822.855752   381.704731   818.000000   379.825493   \n",
       "  380   378.342112   826.948364   378.022961   820.499940   378.313984   \n",
       "  381   377.644661   824.088688   377.995896   824.000000   376.954737   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1066.048889  1741.770996  1043.988708  1762.258179  1046.038254  ...   \n",
       "  1    1081.588470  1723.552856  1035.987320  1735.420593  1052.951721  ...   \n",
       "  2    1076.466888  1714.632111  1054.487320  1758.621277  1038.289658  ...   \n",
       "  3    1092.262650  1716.070404  1060.302139  1749.379120  1052.286438  ...   \n",
       "  4    1098.702713  1701.945129  1060.310867  1741.786407  1059.735641  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  377   804.000000   369.830610   804.000000   376.402521   804.000000  ...   \n",
       "  378   819.090820   372.398604   818.909634   375.707764   815.749773  ...   \n",
       "  379   818.000000   375.251324   818.000000   375.826421   818.000000  ...   \n",
       "  380   820.000000   360.229465   820.000000   376.289867   820.000000  ...   \n",
       "  381   824.000000   368.920594   824.000000   375.028628   824.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1759.699524  1424.053101  1753.047119  1522.955444   1727.308472   \n",
       "  1    1758.377106  1456.396790  1750.316467  1513.419739   1736.680267   \n",
       "  2    1729.931580  1465.707092  1735.066528  1507.422424   1692.436096   \n",
       "  3    1767.793457  1429.386169  1732.767670  1464.257812   1703.522339   \n",
       "  4    1756.078400  1457.637207  1717.619812  1477.916992   1688.586426   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  377   376.495958  1041.100098   357.834651  1192.349823    365.035153   \n",
       "  378   372.078202  1040.894226   359.782651  1192.945740    362.898620   \n",
       "  379   368.107895  1040.460571   360.497683  1191.331848    362.264243   \n",
       "  380   366.771938  1032.133057   360.000000  1156.479797    360.000000   \n",
       "  381   363.000000  1040.347488   363.000000  1198.982849    363.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1515.645386  1734.114807  1501.115356   1707.714905   1503.470520  \n",
       "  1     1515.417786  1706.880646  1478.516235   1696.348572   1483.518860  \n",
       "  2     1501.081360  1713.897705  1478.814087   1684.182892   1478.111450  \n",
       "  3     1455.865784  1694.968109  1439.508179   1683.617920   1438.794434  \n",
       "  4     1471.029907  1677.875000  1457.175964   1663.094330   1456.906921  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  377   1189.615570   379.390848  1316.972717    384.017597   1315.887390  \n",
       "  378   1190.218018   375.178215  1315.174164    378.940886   1313.492157  \n",
       "  379   1189.879791   367.839731  1302.812836    369.740437   1303.145050  \n",
       "  380   1153.714111   374.355535  1185.513855    375.663250   1180.762543  \n",
       "  381   1195.704010   372.720991  1300.992920    369.032880   1298.592163  \n",
       "  \n",
       "  [382 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1443.585510  856.682495  1441.109009  838.928635  1441.125488   \n",
       "  1    1436.381897  864.746552  1432.294739  846.528198  1433.806213   \n",
       "  2    1425.202484  871.128815  1420.548172  850.680588  1426.267853   \n",
       "  3    1405.404770  871.543106  1401.666122  853.370743  1403.246902   \n",
       "  4    1387.880814  878.327469  1383.996277  858.738892  1384.226822   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  371   237.730904  618.889069   236.709019  610.644546   234.477089   \n",
       "  372   232.609302  613.000000   232.200261  613.000000   230.200452   \n",
       "  373   241.010092  620.000000   240.264380  620.000000   240.030636   \n",
       "  374   243.926467  629.745898   242.537978  628.000000   243.786257   \n",
       "  375   238.988852  637.457230   240.092578  632.000000   237.220270   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     842.210144  1483.753906  817.072876  1475.404449   821.091133  ...   \n",
       "  1     850.931686  1476.761108  821.539787  1469.352081   827.228958  ...   \n",
       "  2     857.526489  1442.522766  821.972664  1448.945404   828.437447  ...   \n",
       "  3     857.116165  1444.010406  832.688087  1444.700562   834.973984  ...   \n",
       "  4     863.330872  1421.668640  835.057114  1426.515808   839.367088  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  371   611.355518   232.173719  612.431141   236.652494   610.685552  ...   \n",
       "  372   613.000000   228.043888  613.000000   231.833064   613.000000  ...   \n",
       "  373   620.000000   229.805359  620.000000   237.219034   620.000000  ...   \n",
       "  374   628.000000   231.000000  628.000000   242.132257   628.000000  ...   \n",
       "  375   632.000000   233.000000  632.000000   241.080215   632.000000  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1383.352859  1069.712585  1228.071404  1085.308380   1227.992855   \n",
       "  1    1377.263062  1066.882477  1225.315304  1073.208099   1229.566067   \n",
       "  2    1376.809433  1072.150177  1219.088867  1068.882111   1260.239830   \n",
       "  3    1381.492310  1074.229675  1293.174011  1077.506836   1285.454437   \n",
       "  4    1380.617538  1079.923004  1294.280350  1071.809601   1286.664322   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  371   255.995792   834.131287   258.493931   972.007202    273.697742   \n",
       "  372   242.655512   834.217255   264.605385   971.172119    263.758751   \n",
       "  373   242.335501   834.238495   260.661076   973.924072    265.590351   \n",
       "  374   238.190209   850.513382   259.165100   981.377197    265.469742   \n",
       "  375   235.519516   860.567459   255.668848   986.823425    254.625168   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1065.712067  1293.858902  1064.172211   1283.388527   1070.592285  \n",
       "  1     1052.786224  1285.051849  1059.026855   1255.232391   1066.638031  \n",
       "  2     1051.215576  1282.358704  1062.384094   1240.782242   1068.306458  \n",
       "  3     1065.360626  1287.736198  1064.476776   1248.155937   1071.304626  \n",
       "  4     1062.972290  1307.079987  1064.523956   1276.392014   1070.593750  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  371    970.124023   281.321793  1111.039398    286.417892   1108.714508  \n",
       "  372    969.854431   282.164616  1110.685333    280.370110   1108.269073  \n",
       "  373    971.811798   281.726608  1109.367889    281.406986   1108.001648  \n",
       "  374    978.425110   283.890202  1108.909119    283.246674   1108.009064  \n",
       "  375    981.723694   279.551266  1101.685944    277.215347   1097.141357  \n",
       "  \n",
       "  [376 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1488.501160  870.476456  1486.949890  855.578568  1490.781097   \n",
       "  1    1550.504639  856.168259  1544.838867  841.447220  1552.390167   \n",
       "  2    1466.567322  862.320465  1467.086975  847.689178  1470.048828   \n",
       "  3    1474.243896  871.664474  1475.887787  855.789505  1474.856140   \n",
       "  4    1478.537781  879.241516  1476.331390  862.836685  1478.478668   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  354   125.629360  634.712202   125.488251  628.736885   124.327868   \n",
       "  355   131.843731  630.000000   129.421558  630.000000   131.774603   \n",
       "  356   133.615267  649.938594   134.108194  645.193540   133.190237   \n",
       "  357   135.768362  659.706282   136.415377  654.589833   135.322094   \n",
       "  358   137.742153  654.000000   136.300732  654.000000   137.771385   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     857.742386  1519.078888  847.951355  1530.458832   843.374451  ...   \n",
       "  1     844.712677  1525.645477  839.837128  1528.044312   842.150986  ...   \n",
       "  2     849.249252  1508.756958  847.587479  1517.880707   837.839767  ...   \n",
       "  3     858.200485  1516.495270  846.694595  1509.915131   844.343094  ...   \n",
       "  4     866.863434  1509.283875  846.212311  1507.818726   849.426865  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  354   628.478377   109.998461  626.360825   123.010777   627.761995  ...   \n",
       "  355   630.000000   107.662392  630.000000   125.312881   630.000000  ...   \n",
       "  356   644.749292   121.172614  641.288226   126.153864   643.115188  ...   \n",
       "  357   653.728620   122.010117  649.177244   128.639135   652.421064  ...   \n",
       "  358   654.000000   114.257188  654.000000   129.859907   654.742708  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1443.347107  1063.084442  1382.175873  1029.421082   1378.542389   \n",
       "  1    1440.140228  1063.705475  1403.442520  1015.507385   1392.519608   \n",
       "  2    1432.825089  1060.764923  1349.539963  1042.080170   1347.493423   \n",
       "  3    1420.779922  1070.500000  1302.622169  1079.400482   1305.622070   \n",
       "  4    1434.952576  1062.930573  1285.314247  1095.428955   1362.762909   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  354   149.871105   859.844269   154.933163  1006.643616    176.083717   \n",
       "  355   141.369972   857.771957   145.672462   998.509949    171.117897   \n",
       "  356   133.285841   858.103989   154.723415   992.406891    156.357262   \n",
       "  357   128.481401   852.523987   149.115189   990.006531    149.861938   \n",
       "  358   127.695139   850.658112   138.136827   987.618927    145.690449   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1029.932861  1337.655273  1045.862762   1328.093811   1051.360840  \n",
       "  1     1018.335663  1311.389465  1050.537079   1305.974380   1054.757751  \n",
       "  2     1041.035278  1328.007980  1043.264374   1314.310265   1053.522034  \n",
       "  3     1067.434906  1309.420067  1061.593323   1289.454559   1066.803009  \n",
       "  4     1042.647583  1273.039604  1075.662903   1288.415611   1067.974030  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  354    997.505280   225.349457  1117.373962    233.243179   1106.565826  \n",
       "  355    992.978729   211.398315  1114.127319    220.717392   1107.885223  \n",
       "  356    991.850189   210.568161  1104.293427    208.211617   1104.471130  \n",
       "  357    990.648376   195.071259  1112.702911    187.832985   1111.956390  \n",
       "  358    985.683167   178.570213  1110.264862    178.393227   1109.933807  \n",
       "  \n",
       "  [359 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1493.435364  941.747787  1490.965271  925.046356  1494.628479   \n",
       "  1    1488.654572  946.403824  1484.320068  929.558075  1488.509521   \n",
       "  2    1472.981094  942.827362  1470.662201  927.039894  1473.565796   \n",
       "  3    1467.901199  951.355057  1463.027115  934.906815  1467.804474   \n",
       "  4    1442.798431  940.044067  1442.029572  926.502174  1444.824844   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  325   366.063143  738.727055   366.519849  733.792773   364.607598   \n",
       "  326   363.452998  750.020229   366.430911  745.102491   361.165952   \n",
       "  327   366.408415  758.047130   369.579590  753.230739   364.000000   \n",
       "  328   364.931087  769.225435   367.348623  764.737994   362.624082   \n",
       "  329   361.910657  781.124433   364.283279  777.257092   359.786045   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     929.173889  1526.493073  908.471123  1522.677582   910.438972  ...   \n",
       "  1     934.068573  1514.865387  908.967499  1525.247559   911.269142  ...   \n",
       "  2     929.671036  1505.295349  912.892647  1525.923309   908.451851  ...   \n",
       "  3     938.454865  1497.339752  915.105812  1515.517242   912.636162  ...   \n",
       "  4     927.103111  1485.819427  917.811089  1509.217133   908.861961  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  325   733.594202   360.060659  733.537726   365.432142   732.026853  ...   \n",
       "  326   744.439583   369.328758  744.072153   363.522963   741.397442  ...   \n",
       "  327   752.418802   372.178342  752.055081   364.000000   748.745361  ...   \n",
       "  328   764.280316   368.654884  763.227004   361.081105   759.795341  ...   \n",
       "  329   776.893559   365.188734  775.222538   359.364352   771.390579  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1418.197678  1124.372772  1378.478760  1113.891449   1357.897217   \n",
       "  1    1424.777054  1124.776245  1345.458290  1120.438293   1335.796089   \n",
       "  2    1432.315231  1122.911194  1324.103058  1122.158020   1320.807549   \n",
       "  3    1409.951874  1121.202576  1281.919609  1126.320374   1267.085094   \n",
       "  4    1407.696106  1126.804108  1257.475170  1133.308105   1247.793617   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  325   363.816099   955.472855   358.867147  1063.440552    363.475995   \n",
       "  326   361.000000   958.993271   361.000000  1026.499054    361.000000   \n",
       "  327   364.000000   957.049072   372.737397   980.682404    365.874352   \n",
       "  328   363.847260   942.846252   376.695356   954.050461    370.574022   \n",
       "  329   358.769850   942.296921   380.841692   936.689911    373.281090   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1110.650238  1397.978043  1122.121552   1366.199371   1122.595673  \n",
       "  1     1114.587036  1370.310852  1121.484406   1358.639999   1124.538330  \n",
       "  2     1115.411774  1364.419617  1120.139832   1347.543304   1125.314941  \n",
       "  3     1118.481750  1345.260986  1122.907867   1319.354675   1130.229095  \n",
       "  4     1124.906830  1327.651604  1126.922455   1309.558601   1133.654480  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  325   1060.951996   380.088020  1076.406525    375.578194   1079.342163  \n",
       "  326   1024.503632   379.077539  1016.861542    380.067741   1007.720947  \n",
       "  327    977.118179   391.066835   984.858551    391.583303    975.747910  \n",
       "  328    955.158264   391.030640   964.137146    390.779125    958.378860  \n",
       "  329    940.618515   386.140987   962.056137    387.420006    961.127960  \n",
       "  \n",
       "  [330 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2120.116837  985.386765  2115.195900  965.136612  2114.183533   \n",
       "  1    2135.885452  988.827454  2132.340149  969.288483  2132.858429   \n",
       "  2    2165.441177  980.788010  2161.093170  960.819397  2162.319000   \n",
       "  3    2167.917770  998.126801  2172.585434  977.885696  2168.323257   \n",
       "  4    2183.393616  992.340042  2189.113480  972.846329  2181.269836   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  251  3408.690689  548.108459  3405.793510  541.604805  3413.410675   \n",
       "  252  3422.434006  552.047401  3418.469788  545.606411  3426.099319   \n",
       "  253  3403.234818  553.459312  3400.719429  546.986973  3407.791550   \n",
       "  254  3406.012299  553.180393  3400.782318  544.518806  3408.459496   \n",
       "  255  3418.394737  548.932198  3412.698822  540.100750  3420.425377   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     964.499771  2088.589005  956.442719  2077.585304   921.509644  ...   \n",
       "  1     966.252884  2103.983589  955.158493  2087.387512   920.247955  ...   \n",
       "  2     958.603180  2123.331947  946.296753  2093.398170   921.445290  ...   \n",
       "  3     971.746567  2150.578873  950.371307  2117.637947   924.630852  ...   \n",
       "  4     965.253281  2172.349503  956.186462  2136.524368   923.605370  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  251   540.711395  3377.753227  544.275860  3427.973541   544.298401  ...   \n",
       "  252   544.606976  3380.281227  548.059299  3429.056320   548.147911  ...   \n",
       "  253   546.294880  3378.153465  551.036591  3428.558472   550.977287  ...   \n",
       "  254   545.321922  3380.020691  547.970615  3429.619415   549.236340  ...   \n",
       "  255   541.151745  3385.794716  543.664345  3433.876083   545.671646  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2031.357544  1368.576660  2135.911224  1335.243713   2098.855659   \n",
       "  1    2041.975487  1325.519043  2140.662292  1385.823853   2098.117912   \n",
       "  2    2039.554920  1329.302002  2143.060181  1374.003601   2089.491646   \n",
       "  3    2060.334656  1276.175354  2129.382919  1388.728516   2113.526794   \n",
       "  4    2038.034397  1283.724121  2160.763046  1403.055420   2105.076935   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  251  3422.322342   755.364594  3362.352356   865.796844   3408.228241   \n",
       "  252  3423.501587   758.507477  3365.388901   866.542633   3407.732422   \n",
       "  253  3426.843750   760.156097  3371.237312   865.472229   3413.222633   \n",
       "  254  3430.065811   760.928864  3374.036545   865.709290   3424.356888   \n",
       "  255  3432.936447   759.328766  3375.026749   864.207092   3440.909241   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1377.122192  2168.084229  1578.526428   2155.829391   1600.357544  \n",
       "  1     1431.495483  2180.073761  1597.156250   2145.780930   1621.033997  \n",
       "  2     1420.908264  2179.197433  1599.863281   2142.748062   1625.847412  \n",
       "  3     1414.090576  2207.466980  1617.274414   2175.375656   1633.597412  \n",
       "  4     1426.514893  2210.554764  1637.470032   2167.295212   1656.135254  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  251    867.508820  3368.064781   977.401642   3383.068535    978.174622  \n",
       "  252    870.902283  3372.344315   966.762268   3383.295204    978.941223  \n",
       "  253    868.608917  3375.799843   967.359100   3387.988152    972.146454  \n",
       "  254    867.781860  3381.925720   960.591431   3397.743439    950.423889  \n",
       "  255    863.834412  3383.637070   957.571411   3410.829361    942.802765  \n",
       "  \n",
       "  [256 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2068.885727  857.506805  2065.920547  833.931519  2052.466774   \n",
       "  1    2079.280090  891.625351  2075.425781  868.968460  2072.243423   \n",
       "  2    2108.247314  900.029922  2110.603943  879.267563  2104.329620   \n",
       "  3    2119.206192  905.118759  2120.748856  884.099609  2114.000000   \n",
       "  4    2140.397446  910.366013  2142.961838  890.712616  2136.690353   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  265  3508.575958  495.903353  3507.225952  488.123766  3512.168190   \n",
       "  266  3527.147881  496.443243  3529.555283  489.795238  3529.131920   \n",
       "  267  3533.857193  497.680275  3537.306259  490.711922  3534.633781   \n",
       "  268  3525.686913  528.495468  3524.687073  522.573647  3527.807510   \n",
       "  269  3523.774033  533.721685  3523.377968  527.729073  3525.363640   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     835.609879  2027.559914  829.311249  1990.365372   825.309601  ...   \n",
       "  1     871.612122  2040.554314  854.398865  2013.299942   834.784241  ...   \n",
       "  2     876.531570  2095.439026  862.031281  2054.097748   832.995285  ...   \n",
       "  3     881.502548  2099.995392  871.216141  2061.222847   838.468048  ...   \n",
       "  4     884.584518  2111.945724  873.719742  2081.658119   848.364891  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  265   487.703346  3509.841293  494.193476  3547.000000   496.320595  ...   \n",
       "  266   489.345406  3529.204628  490.935148  3548.231186   491.884138  ...   \n",
       "  267   490.141768  3534.323769  493.325850  3539.654892   493.996450  ...   \n",
       "  268   521.953596  3524.783485  522.642921  3537.771492   514.859293  ...   \n",
       "  269   527.178392  3524.099792  527.432236  3535.316727   521.909214  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2007.730835  1226.518311  2190.122437  1335.408691   2096.288528   \n",
       "  1    2015.668762  1266.024536  2189.264969  1276.597778   2130.541473   \n",
       "  2    2015.228558  1267.066467  2210.888672  1328.121338   2136.477493   \n",
       "  3    2012.318058  1268.563110  2211.255966  1343.673523   2126.347931   \n",
       "  4    2026.938873  1245.292664  2210.188095  1349.072998   2120.790878   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  265  3542.070740   705.947861  3494.617638   815.100677   3530.783730   \n",
       "  266  3540.348892   710.994675  3503.646015   815.536774   3533.293304   \n",
       "  267  3526.276001   712.909195  3507.821239   817.633026   3525.322021   \n",
       "  268  3534.586723   709.335678  3529.059860   820.089813   3533.803596   \n",
       "  269  3532.905319   700.316101  3530.537956   807.665161   3533.813812   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1401.376465  2240.879425  1562.287842   2124.957779   1593.704468  \n",
       "  1     1313.698181  2231.234802  1569.856445   2158.548630   1590.514282  \n",
       "  2     1371.609436  2241.021484  1608.667358   2185.376465   1618.272705  \n",
       "  3     1375.559082  2238.204590  1623.163086   2163.850174   1637.040588  \n",
       "  4     1385.563721  2223.101669  1623.688293   2137.516891   1652.731873  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  265    815.150909  3495.852676   904.257507   3521.264824    906.636932  \n",
       "  266    815.079041  3502.234009   905.961548   3522.657684    904.687347  \n",
       "  267    818.599823  3503.187012   905.463837   3517.458847    906.108276  \n",
       "  268    820.986389  3512.507862   879.843781   3516.890633    881.561096  \n",
       "  269    808.343323  3517.927315   863.077393   3521.521484    864.509460  \n",
       "  \n",
       "  [270 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2168.725327  892.223709  2169.172104  870.506699  2157.234207   \n",
       "  1    2076.578571  911.299026  2080.634541  887.339142  2069.807915   \n",
       "  2    2188.729156  935.441360  2191.526413  914.618622  2183.563828   \n",
       "  3    2223.815948  941.638916  2228.145660  922.804871  2225.695770   \n",
       "  4    2244.895889  939.318985  2253.731598  922.004257  2251.117294   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  275  3523.057198  518.577812  3531.447144  510.285881  3525.887970   \n",
       "  276  3532.458611  518.975941  3540.238495  510.257877  3533.422562   \n",
       "  277  3546.271362  514.256821  3557.814026  506.137806  3545.006180   \n",
       "  278  3557.054359  516.218140  3569.445518  505.270056  3552.883308   \n",
       "  279  3578.430824  508.593828  3585.482689  500.404755  3572.731483   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     874.648468  2128.159447  856.615120  2088.923271   864.122231  ...   \n",
       "  1     892.187195  2129.664375  867.256134  2132.913033   869.128616  ...   \n",
       "  2     914.497070  2170.841629  895.076935  2141.460838   870.013092  ...   \n",
       "  3     917.301224  2176.180908  893.533386  2179.035919   879.121216  ...   \n",
       "  4     913.011063  2189.488373  885.514069  2203.728638   873.077347  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  275   509.767857  3562.488838  513.900433  3560.291519   510.551228  ...   \n",
       "  276   510.330135  3570.402206  509.843613  3560.483337   508.030228  ...   \n",
       "  277   506.117054  3584.126755  513.828484  3550.366890   512.180588  ...   \n",
       "  278   506.531734  3595.000000  510.340755  3551.594681   512.304958  ...   \n",
       "  279   500.936949  3598.000000  504.012093  3562.754913   506.206316  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2080.027077  1264.501343  2265.986542  1346.629578   2144.488083   \n",
       "  1    2105.446625  1231.974640  2261.988754  1389.233032   2129.740326   \n",
       "  2    2102.877136  1287.488647  2270.128632  1418.242676   2142.175568   \n",
       "  3    2116.653229  1254.755188  2253.855118  1408.445374   2174.930046   \n",
       "  4    2143.738121  1272.153320  2227.375122  1443.537903   2213.985397   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  275  3571.665543   729.190521  3522.126701   849.001678   3565.355537   \n",
       "  276  3566.591446   731.219727  3538.149254   850.367462   3559.715324   \n",
       "  277  3558.184807   724.095184  3552.896439   831.608276   3543.468048   \n",
       "  278  3558.862991   735.838623  3571.638588   843.303345   3540.099102   \n",
       "  279  3568.548149   724.238800  3577.771393   830.440887   3555.777596   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1432.630554  2298.225037  1575.734741   2200.945450   1607.974182  \n",
       "  1     1426.085144  2287.014709  1603.270508   2170.074081   1634.748413  \n",
       "  2     1498.143982  2285.120132  1634.608337   2188.538910   1655.553345  \n",
       "  3     1440.258057  2279.607712  1650.923279   2203.117203   1678.273987  \n",
       "  4     1468.585571  2267.601959  1678.578369   2234.295441   1704.466797  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  275    845.467682  3528.196495   931.293518   3568.237251    929.429657  \n",
       "  276    845.757812  3535.544945   930.161865   3559.750626    929.444153  \n",
       "  277    828.304779  3549.188454   920.586060   3545.840813    919.393768  \n",
       "  278    839.274353  3566.812111   925.000000   3541.334030    924.034912  \n",
       "  279    828.411835  3564.823708   915.576111   3545.439884    912.559845  \n",
       "  \n",
       "  [280 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2261.888969  941.805557  2258.129715  926.021103  2267.433495   \n",
       "  1    2294.831581  948.157425  2294.013481  935.152954  2302.220009   \n",
       "  2    2317.488876  951.203156  2318.240128  936.481064  2326.496292   \n",
       "  3    2347.403198  938.070831  2346.494156  924.754959  2354.008682   \n",
       "  4    2370.382126  923.936218  2377.318802  910.289215  2377.618256   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  174  3492.730476  499.740814  3491.717331  491.639927  3498.488686   \n",
       "  175  3498.782852  498.717407  3499.812149  490.720827  3503.313995   \n",
       "  176  3525.829346  496.213860  3528.754868  488.322113  3528.080170   \n",
       "  177  3521.282593  517.866559  3524.822891  512.687316  3522.286583   \n",
       "  178  3520.468590  533.224526  3523.776337  528.225171  3521.265343   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     923.272369  2230.171406  899.076912  2243.203400   880.083961  ...   \n",
       "  1     928.613297  2251.906418  912.000938  2263.796112   889.257973  ...   \n",
       "  2     929.108353  2265.656624  910.246300  2287.881592   897.203552  ...   \n",
       "  3     918.400833  2309.902863  905.925362  2315.758530   887.298424  ...   \n",
       "  4     904.328453  2308.055145  880.166763  2332.021942   877.171623  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  174   489.857658  3478.499664  492.214985  3527.924164   493.902477  ...   \n",
       "  175   489.432775  3494.701080  492.029438  3535.751694   493.769764  ...   \n",
       "  176   487.641092  3525.291412  489.239695  3546.000000   492.759468  ...   \n",
       "  177   512.105436  3523.116524  510.127509  3530.121796   511.092634  ...   \n",
       "  178   527.975712  3524.851196  525.467142  3520.646767   523.826329  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2277.032158  1100.962250  2375.245483  1064.464661   2381.432968   \n",
       "  1    2272.920311  1099.577606  2373.210785  1058.023041   2383.175903   \n",
       "  2    2268.764717  1101.705627  2368.127472  1063.811279   2380.354309   \n",
       "  3    2289.980347  1107.931335  2401.542542  1074.192657   2437.910629   \n",
       "  4    2240.166954  1118.000000  2359.655594  1069.763214   2386.896805   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  174  3516.072739   752.136078  3430.358978   881.783966   3514.884338   \n",
       "  175  3525.856430   753.295929  3442.470001   875.409393   3517.980087   \n",
       "  176  3529.248352   751.255310  3469.438812   871.111816   3521.188133   \n",
       "  177  3520.538841   742.793503  3481.498177   847.821045   3516.570099   \n",
       "  178  3514.809059   733.744919  3499.956917   850.420410   3506.245934   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1063.687103  2441.119507  1069.944672   2427.103088   1050.125610  \n",
       "  1     1062.113403  2449.446838  1066.625916   2455.378174   1054.124878  \n",
       "  2     1075.152740  2405.512253  1070.957855   2421.641281   1065.585480  \n",
       "  3     1086.536194  2419.830887  1081.845490   2449.294861   1077.418823  \n",
       "  4     1083.179138  2337.527313  1080.310852   2357.302628   1079.731293  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  174    886.844879  3425.753311  1008.896851   3502.458649   1016.085266  \n",
       "  175    886.737762  3428.705120   996.483826   3503.047012   1018.426086  \n",
       "  176    886.380005  3446.433228   970.377502   3507.300758   1013.720581  \n",
       "  177    875.302063  3463.238876   942.915924   3501.249413   1002.610138  \n",
       "  178    852.730621  3485.361565   948.263580   3485.162521    951.066010  \n",
       "  \n",
       "  [179 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2263.344727  901.104141  2263.696190  886.486832  2268.605492   \n",
       "  1    2301.374176  895.207615  2300.955734  880.617134  2302.997635   \n",
       "  2    2328.686264  886.609810  2330.889313  870.653576  2330.482819   \n",
       "  3    2361.289093  865.954994  2364.966766  849.520439  2361.274567   \n",
       "  4    2376.511993  840.743889  2386.430511  823.909637  2378.314911   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  220  3531.000740  483.991405  3532.206345  477.311680  3535.077316   \n",
       "  221  3539.532784  489.359871  3541.046753  482.757568  3542.714691   \n",
       "  222  3554.274315  483.493267  3556.643982  476.696249  3556.367691   \n",
       "  223  3557.276527  482.516785  3560.688622  475.908802  3558.805603   \n",
       "  224  3565.625992  477.068283  3569.565094  469.358006  3566.692825   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     879.815567  2235.006142  867.757370  2232.216972   845.301582  ...   \n",
       "  1     873.637032  2241.400299  873.145679  2249.808426   845.982744  ...   \n",
       "  2     864.110378  2266.514923  855.117855  2282.392883   833.512737  ...   \n",
       "  3     843.752850  2291.212906  834.427101  2302.923279   817.953760  ...   \n",
       "  4     816.711823  2316.539520  797.898911  2326.734283   786.082821  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  220   476.819870  3532.303314  481.078407  3568.449509   483.076664  ...   \n",
       "  221   482.355942  3541.690468  485.247295  3571.000000   487.399796  ...   \n",
       "  222   476.360640  3554.414566  478.068724  3572.000000   480.078955  ...   \n",
       "  223   475.341991  3560.403275  477.146938  3571.669006   479.495947  ...   \n",
       "  224   469.142170  3570.000000  470.877243  3566.467461   472.459949  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2247.651169  1088.164307  2371.672226  1047.053162   2360.256012   \n",
       "  1    2176.627457  1238.508911  2269.781662  1346.989349   2223.853249   \n",
       "  2    2209.433113  1202.911255  2262.741699  1369.046631   2274.735062   \n",
       "  3    2224.958755  1156.932190  2269.752090  1337.549072   2290.157166   \n",
       "  4    2205.409927  1119.910126  2247.284332  1314.788696   2277.970688   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  220  3564.273361   698.685669  3494.730259   805.055450   3561.186386   \n",
       "  221  3566.536484   688.581314  3500.519268   786.394897   3559.839989   \n",
       "  222  3561.694069   693.698166  3511.841549   798.731384   3553.714676   \n",
       "  223  3558.751404   689.222214  3516.399727   795.957397   3552.827698   \n",
       "  224  3553.050072   696.657837  3549.336319   798.273865   3555.683540   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1044.622345  2407.107452  1048.530090   2384.476288   1023.806213  \n",
       "  1     1354.109070  2307.105499  1347.147400   2267.051010   1354.153198  \n",
       "  2     1380.650391  2258.981155  1441.179688   2273.677444   1458.000000  \n",
       "  3     1345.413208  2291.529861  1485.805481   2294.179764   1492.771729  \n",
       "  4     1326.921570  2285.566544  1492.085693   2291.558578   1507.072754  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  220    807.926300  3484.532001   905.690186   3550.491745    913.894409  \n",
       "  221    799.087158  3490.708626   879.720306   3543.289452    910.469971  \n",
       "  222    811.236725  3499.358120   878.552124   3535.143639    908.347717  \n",
       "  223    800.370697  3512.913330   875.196472   3537.875076    883.219238  \n",
       "  224    799.308136  3539.270187   825.000000   3542.698540    825.000000  \n",
       "  \n",
       "  [225 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1717.386444  1129.775173  1735.191101  1127.882832  1713.514435   \n",
       "  1    1724.662567  1115.833590  1740.873260  1113.834625  1722.467804   \n",
       "  2    1713.195160  1116.581186  1729.875244  1112.868605  1709.237274   \n",
       "  3    1763.808563   971.662930  1771.051392   956.197079  1767.571960   \n",
       "  4    1764.011597   992.513046  1764.957886   972.484001  1764.263702   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  343   422.540907   636.571930   429.284104   626.340191   427.315349   \n",
       "  344   418.506544   634.261673   424.466356   624.111652   422.426538   \n",
       "  345   418.229207   636.991829   422.426463   626.107155   421.442557   \n",
       "  346   421.734013   634.520950   423.031436   623.643936   426.170100   \n",
       "  347   429.953518   634.750755   427.722551   623.900856   434.123833   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1123.204334  1780.408722  1127.109283  1751.640167  1104.981028  ...   \n",
       "  1    1109.804165  1785.816010  1114.700876  1749.269867  1103.775257  ...   \n",
       "  2    1109.617149  1775.263794  1112.960279  1741.798889  1103.073481  ...   \n",
       "  3     957.934282  1813.637848   954.231544  1823.525787   951.070715  ...   \n",
       "  4     974.513145  1811.779083   957.633339  1824.004211   958.250778  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  343   627.016590   456.166080   638.391190   494.319405   635.907822  ...   \n",
       "  344   624.735687   448.872355   635.743034   487.167732   632.962189  ...   \n",
       "  345   626.880814   442.082104   634.950203   480.612305   633.952591  ...   \n",
       "  346   624.924850   430.826319   632.388008   487.603188   634.541924  ...   \n",
       "  347   624.660358   423.810840   630.367718   480.790749   629.188259  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1751.158997  1231.674011  1712.713287  1353.053192   1671.860687   \n",
       "  1    1787.638611  1249.161331  1654.788818  1279.776703   1667.269165   \n",
       "  2    1745.531158  1296.335693  1681.720673  1341.578613   1639.029053   \n",
       "  3    1837.155365  1323.761536  1645.097626  1322.432709   1660.517395   \n",
       "  4    1833.729889  1324.235168  1646.514099  1334.811279   1668.288635   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  343   536.157967   918.172882   531.883041  1084.185913    475.461266   \n",
       "  344   530.673431   913.430389   513.229874  1081.927673    478.042198   \n",
       "  345   516.560249   910.784790   503.383614  1079.102234    477.155930   \n",
       "  346   496.640335   912.943726   483.810669  1081.671326    491.381065   \n",
       "  347   496.216606   907.301331   470.639534  1078.941162    484.995064   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1312.900757  1620.741486  1489.313385   1622.272598   1481.489105  \n",
       "  1     1221.260162  1605.280640  1509.677216   1587.404800   1489.340302  \n",
       "  2     1308.316895  1594.241638  1537.203583   1592.180283   1526.337097  \n",
       "  3     1283.205719  1582.815842  1536.894531   1584.478333   1518.404297  \n",
       "  4     1293.275360  1569.604370  1566.561279   1573.260956   1539.092712  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  343   1078.311768   626.922256  1229.821289    494.372482   1234.861450  \n",
       "  344   1077.611816   621.213394  1229.265686    492.157494   1231.857422  \n",
       "  345   1075.165649   612.476837  1220.805176    502.153244   1223.246704  \n",
       "  346   1079.312683   601.426895  1212.747070    528.135780   1224.229797  \n",
       "  347   1077.049896   586.350693  1194.718567    505.797745   1228.300049  \n",
       "  \n",
       "  [348 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1809.306488  891.610321  1823.610535  876.700836  1816.403748   \n",
       "  1    1800.569122  893.708725  1815.573853  879.285187  1806.965027   \n",
       "  2    1796.779785  900.269104  1811.724548  883.912140  1805.088135   \n",
       "  3    1788.866882  903.731964  1804.929779  887.161911  1797.046722   \n",
       "  4    1779.402954  905.010178  1796.916016  885.261703  1787.470551   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  302   571.102879  736.979538   567.568554  727.248100   573.243340   \n",
       "  303   580.620327  726.504642   575.450657  714.793816   580.429913   \n",
       "  304   593.701370  726.943428   589.436642  713.603092   591.384987   \n",
       "  305   584.481686  733.246727   580.641621  719.238037   583.722298   \n",
       "  306   554.777679  717.137135   555.378637  703.828396   552.221331   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     871.954018  1874.608612  889.722977  1871.029572   873.426453  ...   \n",
       "  1     872.289566  1867.456665  896.261154  1877.911499   880.231102  ...   \n",
       "  2     880.326294  1869.834137  904.010056  1865.249451   884.394989  ...   \n",
       "  3     884.479378  1862.421204  908.531952  1861.857239   888.008766  ...   \n",
       "  4     883.308022  1857.783051  907.989532  1838.877716   892.860550  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  302   726.622917   542.924377  730.330521   582.541245   732.282906  ...   \n",
       "  303   714.252029   550.744847  721.209305   570.279306   718.953651  ...   \n",
       "  304   713.402641   548.426208  718.864117   572.763279   718.139294  ...   \n",
       "  305   718.436577   545.025802  720.987732   570.850260   719.923477  ...   \n",
       "  306   701.859304   546.529433  711.098850   551.885612   717.808300  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1875.038147  1236.048615  1738.109100  1153.472534   1736.454437   \n",
       "  1    1853.103149  1227.344666  1728.634705  1195.005402   1726.427521   \n",
       "  2    1882.357422  1249.569672  1720.062500  1210.007080   1723.415985   \n",
       "  3    1892.092957  1258.331116  1713.141724  1239.841644   1719.658661   \n",
       "  4    1862.460815  1253.620483  1708.799225  1265.870483   1709.721191   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  302   588.550861  1001.718567   587.566795  1152.891418    589.540005   \n",
       "  303   590.053917   995.501770   578.581680  1151.276550    583.985947   \n",
       "  304   567.780144  1000.020599   569.640409  1153.447327    575.556408   \n",
       "  305   554.612314  1007.411835   564.063829  1148.163361    565.658224   \n",
       "  306   544.000000  1007.383575   559.565327  1161.093048    544.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1132.828247  1701.326019  1387.822754   1704.827393   1370.710815  \n",
       "  1     1158.469696  1678.252350  1434.517151   1685.741150   1410.235168  \n",
       "  2     1178.379059  1646.667526  1459.826233   1650.922318   1443.909607  \n",
       "  3     1218.330444  1639.924500  1495.807800   1642.048141   1489.292297  \n",
       "  4     1238.131104  1632.323730  1524.713684   1629.724182   1521.300903  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  302   1148.351288   670.924820  1314.387329    666.587936   1312.993347  \n",
       "  303   1148.891846   663.691223  1299.153442    656.919220   1296.261169  \n",
       "  304   1152.170319   659.270012  1295.958862    652.669327   1294.808838  \n",
       "  305   1145.264282   632.093376  1275.534424    627.754967   1273.962524  \n",
       "  306   1160.647858   647.813263  1278.028137    603.491535   1281.601807  \n",
       "  \n",
       "  [307 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1690.932770  1137.185173  1710.706177  1136.394753  1685.599030   \n",
       "  1    1750.842010   893.216241  1762.020050   886.000000  1756.768982   \n",
       "  2    1739.953247   895.567719  1751.597961   878.743977  1745.782227   \n",
       "  3    1726.322906   885.842690  1739.699585   870.939972  1736.110474   \n",
       "  4    1710.835632   894.307877  1724.464142   878.492813  1720.140503   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  321   392.612410   650.313812   390.804882   643.074837   387.265932   \n",
       "  322   380.441964   648.790503   385.566179   638.781692   373.326141   \n",
       "  323   373.000000   669.613365   374.356822   660.083635   373.000000   \n",
       "  324   382.000000   718.106013   382.000000   710.375979   382.000000   \n",
       "  325   386.000000   783.328293   389.358353   774.013680   386.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1129.267750  1744.084900  1140.108940  1696.018524  1103.610329  ...   \n",
       "  1     886.000000  1808.738007   899.662647  1811.416901   891.966757  ...   \n",
       "  2     879.160088  1802.895142   895.062828  1810.206726   885.453247  ...   \n",
       "  3     868.667160  1791.744080   899.847862  1809.193634   881.079849  ...   \n",
       "  4     877.614037  1774.652374   901.766373  1800.443970   885.676285  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  321   643.023258   371.598344   658.964327   382.687432   658.784565  ...   \n",
       "  322   638.359705   388.663866   651.426086   370.918183   652.719282  ...   \n",
       "  323   660.608692   375.179114   668.644077   377.920475   670.272003  ...   \n",
       "  324   709.208854   382.000000   710.161422   382.000000   719.120669  ...   \n",
       "  325   772.051754   396.816630   772.621059   386.000000   757.412594  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1677.291077  1208.077988  1680.502106  1220.921310   1637.381439   \n",
       "  1    1810.747009  1284.319397  1650.580994  1202.338776   1640.960175   \n",
       "  2    1802.511292  1243.265778  1638.610870  1226.066650   1644.280121   \n",
       "  3    1834.903748  1288.456390  1636.795898  1256.152100   1651.908325   \n",
       "  4    1827.302948  1259.264465  1631.410736  1295.804138   1650.943176   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  321   430.296490   951.650635   430.716919  1103.753723    443.099388   \n",
       "  322   415.544640   942.145996   429.819138  1096.293701    430.039986   \n",
       "  323   408.589405   940.800049   425.855118  1098.042480    426.352333   \n",
       "  324   382.207141   958.693604   405.943871  1100.730621    393.668682   \n",
       "  325   386.000000   962.039291   394.768870  1108.912384    389.033243   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1223.756607  1602.930634  1470.768616   1606.194855   1473.456909  \n",
       "  1     1189.563812  1579.521866  1475.731750   1570.771378   1466.509033  \n",
       "  2     1196.805359  1561.944382  1489.424011   1562.410095   1466.824280  \n",
       "  3     1229.724823  1576.030212  1520.347290   1587.932495   1506.250061  \n",
       "  4     1268.143372  1583.103653  1559.603577   1591.085724   1546.081177  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  321   1100.704681   512.664078  1255.233643    511.929611   1253.256104  \n",
       "  322   1096.138763   512.416702  1244.233398    508.938370   1246.264832  \n",
       "  323   1098.078552   504.297043  1244.977539    499.423378   1247.575073  \n",
       "  324   1100.571411   485.222740  1222.081299    478.775436   1223.849426  \n",
       "  325   1106.449280   481.314461  1214.219818    468.327423   1214.202209  \n",
       "  \n",
       "  [326 rows x 34 columns],\n",
       "  1),\n",
       " (         nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0   1546.861198  986.468491  1561.553497  973.270798  1550.477310   \n",
       "  1   1498.965019  981.147728  1513.534149  970.248611  1505.287201   \n",
       "  2   1492.222092  980.573547  1502.289024  967.165771  1499.804146   \n",
       "  3   1466.514030  933.886742  1474.273506  920.417000  1472.254341   \n",
       "  4   1453.835960  910.980286  1469.364273  900.132050  1463.941757   \n",
       "  ..          ...         ...          ...         ...          ...   \n",
       "  66   428.030466  721.661560   426.385954  709.268402   429.001696   \n",
       "  67   403.120480  717.904419   402.426434  705.585640   402.782978   \n",
       "  68   390.553383  722.320755   386.271368  710.076332   390.216866   \n",
       "  69   374.603661  726.408318   372.512166  713.918739   374.885923   \n",
       "  70   380.554071  755.847225   389.639721  741.364517   377.531866   \n",
       "  \n",
       "      right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    970.652328  1614.383621  983.962921  1608.587723   966.252800  ...   \n",
       "  1    965.508080  1571.339310  986.393188  1569.944168   958.398941  ...   \n",
       "  2    966.678078  1562.443207  979.405167  1585.099380   946.083817  ...   \n",
       "  3    920.926079  1523.022972  934.609764  1570.641891   900.389656  ...   \n",
       "  4    895.762596  1535.536530  924.518135  1548.588242   874.805504  ...   \n",
       "  ..          ...          ...         ...          ...          ...  ...   \n",
       "  66   710.990486   448.727215  703.345402   503.201675   695.361572  ...   \n",
       "  67   708.098320   421.074417  700.805313   472.474365   693.154121  ...   \n",
       "  68   712.584686   392.544832  709.150757   447.615181   697.201118  ...   \n",
       "  69   713.563274   371.622228  716.876949   407.285229   717.761406  ...   \n",
       "  70   738.574955   404.667011  740.291077   378.711411   734.971439  ...   \n",
       "  \n",
       "      right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0   1736.393829  1220.695587  1617.982025  1287.408325   1632.273575   \n",
       "  1   1745.000000  1177.388397  1609.335449  1277.689972   1642.890533   \n",
       "  2   1739.895050  1198.248505  1602.694061  1279.596497   1618.495712   \n",
       "  3   1717.151764  1128.567596  1592.986252  1276.917450   1610.005798   \n",
       "  4   1631.179321  1142.931702  1615.993164  1284.806549   1619.794220   \n",
       "  ..          ...          ...          ...          ...           ...   \n",
       "  66   593.899338   958.809540   496.279846  1115.339966    584.168808   \n",
       "  67   558.278229   953.472778   455.426292  1122.235199    563.314331   \n",
       "  68   540.871170   962.830811   431.002460  1135.303864    558.524384   \n",
       "  69   472.572830   963.589966   376.493231  1136.792480    498.866135   \n",
       "  70   394.977413   898.656219   414.896111  1105.811584    420.270470   \n",
       "  \n",
       "      right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0    1280.454498  1601.391190  1505.910095   1662.275696   1490.984985  \n",
       "  1    1269.665649  1607.226517  1502.422607   1663.691025   1486.866211  \n",
       "  2    1270.376495  1609.461182  1502.369690   1614.842239   1490.584534  \n",
       "  3    1270.921661  1609.774155  1510.616272   1617.174850   1501.437317  \n",
       "  4    1281.470154  1608.119431  1505.312805   1604.909729   1496.389954  \n",
       "  ..           ...          ...          ...           ...           ...  \n",
       "  66   1117.995300   568.252136  1258.259583    589.637695   1268.776550  \n",
       "  67   1122.176666   480.209587  1276.227112    582.930725   1275.011841  \n",
       "  68   1128.379486   397.161453  1272.526855    587.586182   1272.580383  \n",
       "  69   1122.724854   429.905975  1275.749084    570.160126   1264.470703  \n",
       "  70   1098.944855   525.341873  1229.223633    518.074081   1227.442810  \n",
       "  \n",
       "  [71 rows x 34 columns],\n",
       "  1),\n",
       " (         nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0   1682.829742  929.406605  1692.059174  911.808193  1687.502670   \n",
       "  1   1655.463608  936.461868  1663.472534  922.107552  1658.380615   \n",
       "  2   1639.777206  946.282257  1648.248779  930.662094  1642.350769   \n",
       "  3   1618.102203  955.812332  1629.242859  941.019768  1621.289963   \n",
       "  4   1568.426743  978.017426  1581.057983  962.436089  1570.348175   \n",
       "  ..          ...         ...          ...         ...          ...   \n",
       "  91   471.985310  686.732510   475.893856  676.227909   475.141457   \n",
       "  92   450.467932  687.013977   456.608486  676.480080   453.557398   \n",
       "  93   505.341927  697.890308   498.355179  686.197681   502.305794   \n",
       "  94   472.794056  711.786621   470.724724  702.393749   466.633720   \n",
       "  95   425.299202  699.215260   427.354906  690.762556   422.934149   \n",
       "  \n",
       "      right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    911.963810  1735.683502  921.244373  1743.347382   915.258667  ...   \n",
       "  1    920.852043  1707.919403  924.715668  1721.618805   918.140190  ...   \n",
       "  2    930.226341  1694.894409  935.323471  1687.046417   928.547096  ...   \n",
       "  3    939.137405  1677.096786  948.630562  1668.895264   935.318192  ...   \n",
       "  4    960.647797  1632.660858  968.787216  1627.231522   953.672546  ...   \n",
       "  ..          ...          ...         ...          ...          ...  ...   \n",
       "  91   676.364750   497.412323  689.157906   548.509796   690.534763  ...   \n",
       "  92   676.840668   482.664745  689.149437   532.866768   692.601349  ...   \n",
       "  93   686.248554   456.985207  698.366859   488.501854   693.585392  ...   \n",
       "  94   702.527428   436.591421  707.707329   441.425577   711.213715  ...   \n",
       "  95   690.078619   423.034056  695.288605   419.883927   693.341476  ...   \n",
       "  \n",
       "      right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0   1805.208191  1228.933868  1694.257156  1279.717896   1679.632874   \n",
       "  1   1756.421814  1223.636627  1648.094086  1256.360687   1649.772156   \n",
       "  2   1732.916504  1210.051971  1653.242050  1251.632660   1652.393173   \n",
       "  3   1743.492249  1215.649933  1642.343124  1255.254333   1637.747253   \n",
       "  4   1747.664978  1196.162415  1634.786484  1260.519775   1637.158432   \n",
       "  ..          ...          ...          ...          ...           ...   \n",
       "  91   595.380707   966.042969   551.470535  1119.669312    526.394394   \n",
       "  92   592.384369   960.672333   499.780434  1116.426819    594.174728   \n",
       "  93   540.939568   971.801422   446.211828  1140.911163    563.296204   \n",
       "  94   467.517143   968.824921   451.114361  1134.000366    514.533768   \n",
       "  95   441.167610   948.444794   463.981041  1104.810760    466.687553   \n",
       "  \n",
       "      right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0    1251.972137  1617.576797  1467.313110   1601.624741   1468.170227  \n",
       "  1    1253.742493  1627.249847  1478.145569   1656.843567   1468.600342  \n",
       "  2    1253.609711  1628.959717  1471.642212   1684.787170   1481.756470  \n",
       "  3    1241.727997  1642.219055  1474.979614   1646.348068   1463.689636  \n",
       "  4    1256.114014  1643.794250  1481.926575   1647.339233   1476.267334  \n",
       "  ..           ...          ...          ...           ...           ...  \n",
       "  91   1110.307648   616.524933  1258.350037    615.959854   1247.532288  \n",
       "  92   1125.562653   559.178314  1257.184814    617.513519   1278.558716  \n",
       "  93   1125.674255   438.236969  1285.813782    605.028610   1276.724792  \n",
       "  94   1125.000641   552.476486  1273.164795    593.025574   1265.093994  \n",
       "  95   1101.728760   565.001785  1228.285461    556.544937   1229.629761  \n",
       "  \n",
       "  [96 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1581.190674   981.202408  1589.065582  963.014526  1579.450348   \n",
       "  1    1536.357597  1001.719391  1545.974274  985.392853  1534.870148   \n",
       "  2    1504.461143  1000.080063  1515.049484  982.529488  1501.633514   \n",
       "  3    1485.401245   983.226151  1496.568459  964.821999  1482.970779   \n",
       "  4    1464.677444   950.187714  1466.714092  934.262299  1463.971977   \n",
       "  ..           ...          ...          ...         ...          ...   \n",
       "  99    375.583100   685.068924   371.282259  671.958847   372.430728   \n",
       "  100   357.679635   668.417282   352.783726  655.277046   356.840990   \n",
       "  101   353.650417   641.935158   349.470594  632.011311   354.752163   \n",
       "  102   359.397419   633.973679   354.702404  626.814350   357.194633   \n",
       "  103   372.630399   836.692112   374.844602  831.349867   372.200126   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     963.492447  1645.884247  955.825424  1628.130707   951.175591  ...   \n",
       "  1     984.959900  1601.330688  976.745285  1579.347870   968.809242  ...   \n",
       "  2     980.748413  1575.584457  971.269157  1543.623810   958.607964  ...   \n",
       "  3     965.156120  1560.241669  958.693573  1532.001663   949.838501  ...   \n",
       "  4     934.824608  1517.293793  920.088577  1543.551651   889.182755  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  99    675.327187   384.581957  668.376366   437.644608   658.776894  ...   \n",
       "  100   657.259384   357.263715  656.893417   409.165211   646.612686  ...   \n",
       "  101   632.011768   335.979985  635.139538   372.159557   628.910213  ...   \n",
       "  102   627.497723   329.084415  633.839821   351.414608   631.036736  ...   \n",
       "  103   829.949308   370.864855  828.441784   360.297884   817.568578  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1702.648651  1229.713409  1625.074341  1258.717194   1645.934357   \n",
       "  1    1726.415649  1191.583313  1626.436676  1257.206604   1638.182693   \n",
       "  2    1713.416138  1119.992798  1619.978012  1254.919708   1630.802536   \n",
       "  3    1726.743866  1144.083313  1612.993149  1256.101013   1629.064056   \n",
       "  4    1717.196777  1153.350800  1613.158646  1274.334778   1635.921188   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  99    546.885971   925.298431   442.758705  1074.617279    580.281219   \n",
       "  100   491.660645   912.086182   407.340866  1067.256592    511.320953   \n",
       "  101   437.548149   892.271027   397.616608  1061.502655    436.674538   \n",
       "  102   408.500595   888.267975   376.813263  1050.160706    399.203140   \n",
       "  103   367.907284  1000.696594   383.632259  1100.366486    385.012812   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1275.066528  1607.325439  1484.584717   1673.662094   1485.242249  \n",
       "  1     1250.158264  1609.193146  1484.484741   1643.841660   1471.573608  \n",
       "  2     1247.411713  1611.786392  1477.988953   1618.920227   1470.359619  \n",
       "  3     1248.834106  1609.458878  1474.253723   1622.065460   1469.177490  \n",
       "  4     1268.126129  1618.459122  1485.151184   1630.956741   1470.836914  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  99    1069.819794   423.245846  1200.013794    646.381470   1204.728882  \n",
       "  100   1064.571198   433.509361  1216.963806    607.619934   1184.266113  \n",
       "  101   1041.528656   431.142799  1216.323975    511.045288   1165.359375  \n",
       "  102   1046.811829   423.367805  1187.490662    434.629280   1182.505554  \n",
       "  103   1099.588715   432.639801  1207.021698    427.622757   1207.092651  \n",
       "  \n",
       "  [104 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2057.536526  666.515640  2058.850933  651.623489  2043.861439   \n",
       "  1    2070.410873  686.503662  2071.589844  670.768890  2056.429771   \n",
       "  2    2095.280380  708.725632  2091.625519  689.465927  2081.940781   \n",
       "  3    2095.541199  720.264481  2091.464088  701.946747  2086.438194   \n",
       "  4    2112.247482  700.479095  2107.425179  682.964203  2100.411400   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  400  3407.953346  316.922184  3409.108879  303.575085  3410.553268   \n",
       "  401  3411.319450  315.575413  3413.196098  304.321842  3413.749306   \n",
       "  402  3425.347488  326.696175  3427.660782  315.293133  3423.040939   \n",
       "  403  3424.945587  329.623390  3426.637413  317.402599  3420.519234   \n",
       "  404  3427.124313  330.022255  3427.728394  317.906124  3425.652832   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     654.829819  2041.826702  644.501038  2025.036993   659.137848  ...   \n",
       "  1     672.118881  2056.910267  666.998001  2033.371408   667.638123  ...   \n",
       "  2     692.199631  2071.024479  684.489227  2038.065062   685.766510  ...   \n",
       "  3     702.708008  2060.462959  691.528534  2032.270859   683.959198  ...   \n",
       "  4     685.123489  2068.112953  679.741867  2041.096951   687.303589  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  400   302.922447  3403.444473  308.942669  3425.488235   315.459259  ...   \n",
       "  401   303.216854  3410.983208  310.642307  3426.348602   310.751328  ...   \n",
       "  402   315.013332  3419.978706  319.542549  3417.356888   322.011116  ...   \n",
       "  403   318.097832  3425.910721  317.642498  3416.084572   320.569523  ...   \n",
       "  404   318.678001  3420.516972  318.789555  3423.227764   318.621975  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2030.530798   985.097504  2172.661102   956.194244   2124.366287   \n",
       "  1    2037.874004  1003.639313  2179.758087   989.940674   2113.762848   \n",
       "  2    2055.593029  1062.156433  2212.749786  1019.612976   2139.673828   \n",
       "  3    2038.775482  1076.239197  2223.998032  1041.436462   2130.112305   \n",
       "  4    2062.285202  1042.616943  2227.238113  1079.796631   2157.350754   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  400  3388.703514   631.101044  3362.662300   777.628571   3367.708206   \n",
       "  401  3390.619095   608.095398  3361.995445   765.378815   3366.730522   \n",
       "  402  3385.883614   605.334930  3378.976723   765.764343   3358.305286   \n",
       "  403  3392.395988   603.828094  3382.421371   764.556366   3365.228546   \n",
       "  404  3415.359642   553.352936  3389.280642   549.207123   3391.955208   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      992.771057  2213.101440  1228.425659   2173.065018   1248.811523  \n",
       "  1     1091.802002  2220.976517  1246.787415   2193.770660   1275.335876  \n",
       "  2     1100.526978  2227.269775  1253.316528   2170.949615   1287.026489  \n",
       "  3     1138.349976  2231.014084  1273.664734   2175.882492   1309.408081  \n",
       "  4     1143.624634  2232.304367  1296.654053   2190.980042   1322.562012  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  400    779.005676  3341.754883   905.503235   3338.274158    906.528320  \n",
       "  401    765.794678  3333.716854   910.058655   3328.998329    911.220276  \n",
       "  402    764.007874  3347.822445   911.015442   3323.533239    911.000061  \n",
       "  403    761.269287  3347.313396   923.474976   3331.897890    920.947205  \n",
       "  404    548.922089  3394.387627   553.804596   3391.760871    554.136871  \n",
       "  \n",
       "  [405 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2043.239212  610.362091  2039.800095  586.753204  2029.968185   \n",
       "  1    2058.340637  584.291321  2050.480942  562.833786  2046.726654   \n",
       "  2    2064.002121  599.056168  2058.809341  576.496796  2059.174271   \n",
       "  3    2064.972031  595.895859  2063.696320  574.007614  2060.120041   \n",
       "  4    2075.355026  589.942215  2071.351227  567.316452  2066.750549   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  391  3489.328964  367.943588  3494.465790  359.413738  3491.640945   \n",
       "  392  3489.930984  362.002069  3496.114014  353.569997  3491.837830   \n",
       "  393  3510.783279  371.265499  3512.000000  358.390499  3506.254608   \n",
       "  394  3507.000000  383.483627  3507.000000  368.630083  3506.464478   \n",
       "  395  3504.000000  376.160564  3504.000000  360.429453  3500.878647   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     587.717529  2018.853172  579.177963  1987.857933   589.951797  ...   \n",
       "  1     567.984497  2010.720245  569.843903  1974.140472   581.555206  ...   \n",
       "  2     574.819412  1996.778397  565.699036  2007.203957   570.371384  ...   \n",
       "  3     572.226242  2007.605644  554.398804  2002.703621   555.869263  ...   \n",
       "  4     565.563782  2016.616783  561.408020  2002.957680   556.044647  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  391   358.634321  3488.940384  361.532179  3505.242996   361.812878  ...   \n",
       "  392   352.064619  3494.388443  353.728733  3504.121262   354.365698  ...   \n",
       "  393   356.998383  3509.517914  356.095227  3497.409088   364.174173  ...   \n",
       "  394   366.959229  3507.000000  365.247272  3496.134995   366.771284  ...   \n",
       "  395   359.509892  3504.000000  360.510384  3490.166367   361.001266  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1993.211815  1028.151733  2219.949188   927.320129   2095.894562   \n",
       "  1    2013.206093   992.923950  2239.410553   945.169617   2163.143921   \n",
       "  2    1973.687935  1011.020691  2231.604431   974.767761   2093.648499   \n",
       "  3    1966.707947   963.654663  2230.540314   988.014771   2075.651855   \n",
       "  4    2004.826218   987.562805  2231.370148  1000.667603   2221.972687   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  391  3464.692451   673.134491  3418.578224   846.491821   3465.632263   \n",
       "  392  3460.325485   669.936646  3427.155571   842.114807   3464.977592   \n",
       "  393  3453.237732   693.680298  3467.190498   856.723267   3425.284519   \n",
       "  394  3457.416817   708.874084  3446.733871   854.684937   3439.262978   \n",
       "  395  3460.874588   728.963593  3449.901390   878.856262   3437.064095   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1059.626465  2228.147064  1219.293823   2196.292252   1249.000244  \n",
       "  1      993.456299  2233.262146  1247.453918   2193.687012   1255.601196  \n",
       "  2     1037.344543  2245.104767  1266.752380   2139.134491   1304.691162  \n",
       "  3     1054.146667  2244.945587  1288.542847   2109.520050   1342.708740  \n",
       "  4     1020.021423  2241.300476  1306.752991   2231.412506   1321.510437  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  391    849.919067  3404.321545   996.491394   3446.386482    999.009216  \n",
       "  392    844.456787  3412.367188   989.037781   3443.477753    991.640320  \n",
       "  393    852.019379  3453.184929   989.355652   3405.996445    989.745483  \n",
       "  394    856.751984  3427.145863   987.692993   3413.383160    990.457031  \n",
       "  395    880.353027  3435.165504   999.155518   3415.932274   1000.273376  \n",
       "  \n",
       "  [396 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2057.536526  666.515640  2058.850933  651.623489  2043.861439   \n",
       "  1    2070.410873  686.503662  2071.589844  670.768890  2056.429771   \n",
       "  2    2095.280380  708.725632  2091.625519  689.465927  2081.940781   \n",
       "  3    2095.541199  720.264481  2091.464088  701.946747  2086.438194   \n",
       "  4    2112.247482  700.479095  2107.425179  682.964203  2100.411400   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  400  3407.953346  316.922184  3409.108879  303.575085  3410.553268   \n",
       "  401  3411.319450  315.575413  3413.196098  304.321842  3413.749306   \n",
       "  402  3425.347488  326.696175  3427.660782  315.293133  3423.040939   \n",
       "  403  3424.945587  329.623390  3426.637413  317.402599  3420.519234   \n",
       "  404  3427.124313  330.022255  3427.728394  317.906124  3425.652832   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     654.829819  2041.826702  644.501038  2025.036993   659.137848  ...   \n",
       "  1     672.118881  2056.910267  666.998001  2033.371408   667.638123  ...   \n",
       "  2     692.199631  2071.024479  684.489227  2038.065062   685.766510  ...   \n",
       "  3     702.708008  2060.462959  691.528534  2032.270859   683.959198  ...   \n",
       "  4     685.123489  2068.112953  679.741867  2041.096951   687.303589  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  400   302.922447  3403.444473  308.942669  3425.488235   315.459259  ...   \n",
       "  401   303.216854  3410.983208  310.642307  3426.348602   310.751328  ...   \n",
       "  402   315.013332  3419.978706  319.542549  3417.356888   322.011116  ...   \n",
       "  403   318.097832  3425.910721  317.642498  3416.084572   320.569523  ...   \n",
       "  404   318.678001  3420.516972  318.789555  3423.227764   318.621975  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2030.530798   985.097504  2172.661102   956.194244   2124.366287   \n",
       "  1    2037.874004  1003.639313  2179.758087   989.940674   2113.762848   \n",
       "  2    2055.593029  1062.156433  2212.749786  1019.612976   2139.673828   \n",
       "  3    2038.775482  1076.239197  2223.998032  1041.436462   2130.112305   \n",
       "  4    2062.285202  1042.616943  2227.238113  1079.796631   2157.350754   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  400  3388.703514   631.101044  3362.662300   777.628571   3367.708206   \n",
       "  401  3390.619095   608.095398  3361.995445   765.378815   3366.730522   \n",
       "  402  3385.883614   605.334930  3378.976723   765.764343   3358.305286   \n",
       "  403  3392.395988   603.828094  3382.421371   764.556366   3365.228546   \n",
       "  404  3415.359642   553.352936  3389.280642   549.207123   3391.955208   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      992.771057  2213.101440  1228.425659   2173.065018   1248.811523  \n",
       "  1     1091.802002  2220.976517  1246.787415   2193.770660   1275.335876  \n",
       "  2     1100.526978  2227.269775  1253.316528   2170.949615   1287.026489  \n",
       "  3     1138.349976  2231.014084  1273.664734   2175.882492   1309.408081  \n",
       "  4     1143.624634  2232.304367  1296.654053   2190.980042   1322.562012  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  400    779.005676  3341.754883   905.503235   3338.274158    906.528320  \n",
       "  401    765.794678  3333.716854   910.058655   3328.998329    911.220276  \n",
       "  402    764.007874  3347.822445   911.015442   3323.533239    911.000061  \n",
       "  403    761.269287  3347.313396   923.474976   3331.897890    920.947205  \n",
       "  404    548.922089  3394.387627   553.804596   3391.760871    554.136871  \n",
       "  \n",
       "  [405 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1981.051949  632.976463  1977.869904  617.382591  1971.977005   \n",
       "  1    2001.923508  643.440857  1998.929565  627.512306  1992.000519   \n",
       "  2    2015.124786  650.398010  2013.905350  633.573090  2005.568680   \n",
       "  3    2029.547913  655.641418  2030.153870  639.007935  2019.949783   \n",
       "  4    2043.889450  661.569633  2043.983734  644.378990  2035.006302   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  286  3159.723053  387.718121  3160.692879  378.350662  3163.199608   \n",
       "  287  3152.690323  385.988384  3157.073639  376.318775  3154.245605   \n",
       "  288  3152.476936  398.121407  3151.283257  386.395725  3155.785530   \n",
       "  289  3171.430046  407.959587  3171.249290  395.814369  3173.116318   \n",
       "  290  3173.857170  404.052650  3174.992340  391.420567  3175.916168   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     618.251717  1943.527901  620.941856  1919.772232   620.497116  ...   \n",
       "  1     627.860703  1955.621254  632.870026  1935.605850   631.754898  ...   \n",
       "  2     633.825043  1962.903343  637.882149  1953.300758   640.067520  ...   \n",
       "  3     638.843361  1988.992210  642.337494  1966.427773   641.313873  ...   \n",
       "  4     644.762108  1993.377914  645.431114  1982.280479   646.501984  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  286   378.660973  3160.594315  382.423996  3202.402466   381.679947  ...   \n",
       "  287   377.106129  3171.072258  379.587784  3188.553131   384.257973  ...   \n",
       "  288   385.082920  3165.324074  389.110809  3200.255112   387.075825  ...   \n",
       "  289   395.027954  3181.481720  394.092415  3202.089096   392.833878  ...   \n",
       "  290   390.883854  3186.879639  387.902821  3201.469391   386.898468  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1872.347795   977.491730  2005.665985  1006.188507   1948.224358   \n",
       "  1    1879.215509   993.041687  2009.311905  1007.453735   1931.329330   \n",
       "  2    1877.958647   980.764221  2009.631058  1004.143463   1958.190063   \n",
       "  3    1882.335465   991.910492  2014.959091  1002.223053   1950.267693   \n",
       "  4    1896.073807   951.520203  2012.436081   999.430145   1974.988976   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  286  3188.303024   663.243622  3122.732788   823.174316   3177.395790   \n",
       "  287  3166.031128   657.772858  3131.685623   822.392944   3136.614426   \n",
       "  288  3180.843277   656.998749  3131.811707   821.850677   3159.230484   \n",
       "  289  3182.467972   660.584045  3133.083435   821.099548   3150.730110   \n",
       "  290  3184.378448   649.395935  3141.849518   816.431427   3152.712006   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1050.163818  2006.569809  1248.542603   1975.692329   1259.470947  \n",
       "  1     1073.249512  2007.858643  1248.017090   1951.979095   1265.452026  \n",
       "  2     1045.770325  2005.405029  1245.393066   1961.961472   1257.136292  \n",
       "  3     1055.980347  2006.760986  1248.860779   1950.690781   1266.393066  \n",
       "  4     1023.509308  2007.787704  1244.223755   1991.056450   1256.790039  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  286    826.896790  3101.630058   988.577942   3163.133179    991.864868  \n",
       "  287    820.657410  3105.534893   983.537476   3104.350456    985.304199  \n",
       "  288    822.472382  3099.590172   981.858521   3127.029785    983.492004  \n",
       "  289    822.160828  3105.466408   985.318542   3118.081520    986.588867  \n",
       "  290    816.986389  3107.092419   987.186218   3106.459713    987.319824  \n",
       "  \n",
       "  [291 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1960.983917  611.678268  1955.599991  595.379852  1950.169769   \n",
       "  1    1966.979843  625.909424  1963.266052  610.670715  1962.452713   \n",
       "  2    1983.610306  620.991943  1979.515137  605.028809  1974.126999   \n",
       "  3    1992.995331  635.552696  1990.293564  619.116432  1983.425125   \n",
       "  4    2009.871262  644.808983  2010.534271  629.260811  2000.391174   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  256  3213.827560  262.383095  3213.984726  251.123280  3220.191299   \n",
       "  257  3224.272797  264.996006  3228.643158  256.011665  3229.087204   \n",
       "  258  3241.367493  248.259312  3244.682678  238.171421  3243.973434   \n",
       "  259  3255.567566  229.110716  3264.025650  220.508778  3251.533752   \n",
       "  260  3268.204880  240.416271  3276.000000  227.162416  3262.912109   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     595.558594  1899.730484  605.686890  1903.076569   607.843735  ...   \n",
       "  1     608.003937  1907.185043  618.873657  1920.952820   612.323303  ...   \n",
       "  2     604.014114  1935.328255  612.318939  1928.497543   610.213364  ...   \n",
       "  3     617.233795  1950.758156  621.009758  1931.922958   620.798805  ...   \n",
       "  4     629.016266  1970.504387  635.732841  1944.561066   620.242912  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  256   250.075542  3215.156662  259.373867  3264.000000   271.764420  ...   \n",
       "  257   255.544525  3234.374161  265.193962  3261.981842   268.556412  ...   \n",
       "  258   238.544714  3241.684143  243.142582  3259.993576   248.726067  ...   \n",
       "  259   220.286583  3269.821365  232.395111  3248.253769   236.300339  ...   \n",
       "  260   226.309155  3276.000000  233.312363  3256.853851   233.224747  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1884.292435   885.137085  2023.273315   998.753723   1992.718460   \n",
       "  1    1891.399368   959.408875  2031.405029  1002.372437   1950.017944   \n",
       "  2    1891.543587   959.019714  2033.094498  1010.212646   1940.949234   \n",
       "  3    1884.985228   971.028046  2031.493942  1005.666107   1938.338684   \n",
       "  4    1869.562318   983.390106  2035.954285  1006.670685   1944.689835   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  256  3245.718063   576.137146  3183.243851   746.945740   3230.433167   \n",
       "  257  3242.478485   569.884583  3194.055542   731.690247   3228.500015   \n",
       "  258  3236.929535   568.563263  3201.037170   728.059082   3239.138519   \n",
       "  259  3219.290466   570.658081  3229.658127   739.265015   3224.087219   \n",
       "  260  3241.279572   598.183044  3243.506378   736.833435   3240.585876   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1002.730286  2021.948883  1223.023254   2003.755402   1228.240906  \n",
       "  1     1053.253784  2032.531586  1233.250000   1981.124435   1257.264771  \n",
       "  2     1057.930908  2032.556732  1248.470093   1964.336105   1274.233643  \n",
       "  3     1092.181091  2036.523575  1244.491455   1971.000458   1280.902100  \n",
       "  4     1093.227234  2031.176056  1249.868164   1942.446541   1273.902893  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  256    752.494934  3109.667263   892.920593   3153.724854    903.573181  \n",
       "  257    737.212341  3100.696468   885.042114   3150.893028    896.723145  \n",
       "  258    736.759033  3106.589390   872.057312   3173.039009    891.637207  \n",
       "  259    738.019409  3114.788334   864.036438   3111.655285    864.166443  \n",
       "  260    743.654236  3138.452309   836.969421   3136.835091    841.156860  \n",
       "  \n",
       "  [261 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1981.926910  534.052124  1985.766418  528.295074  1977.157501   \n",
       "  1    2011.340637  554.705498  2019.419445  547.802930  2004.264240   \n",
       "  2    1964.359751  560.178894  1963.561680  544.962357  1956.783091   \n",
       "  3    1978.975388  565.389160  1975.618652  547.085724  1968.146080   \n",
       "  4    1978.914352  568.626251  1979.186150  551.981705  1970.689835   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  260  3310.258698  286.909630  3307.344391  273.701233  3316.150848   \n",
       "  261  3274.411407  276.406105  3274.673920  263.978867  3281.802216   \n",
       "  262  3315.353088  258.711369  3317.920624  249.035019  3315.906082   \n",
       "  263  3317.525925  251.728336  3322.491089  242.472652  3319.161789   \n",
       "  264  3327.029266  282.172501  3337.616669  264.847538  3322.508377   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     530.875854  1972.018845  518.499222  1929.037163   535.943634  ...   \n",
       "  1     546.925749  2028.351761  543.195479  1997.881752   540.406993  ...   \n",
       "  2     546.663338  1951.646640  545.321510  1946.933195   547.698280  ...   \n",
       "  3     548.901230  1940.691406  547.329269  1934.440857   549.500183  ...   \n",
       "  4     550.676270  1951.562519  548.519897  1939.626850   540.967903  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  260   273.539177  3272.214554  274.433647  3318.872101   281.849525  ...   \n",
       "  261   263.246849  3276.692184  270.350334  3325.302124   273.699516  ...   \n",
       "  262   249.940113  3311.727829  254.338085  3323.120575   257.716579  ...   \n",
       "  263   242.556557  3324.314880  249.097694  3332.088379   253.578312  ...   \n",
       "  264   263.940990  3347.000000  266.428619  3315.883377   265.129250  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1954.222473   795.242920  2072.619003   906.651611   2076.597015   \n",
       "  1    1980.959949   777.211105  2076.372551   930.980377   2066.340073   \n",
       "  2    1961.512489   833.613220  2080.119522   937.093567   2082.916534   \n",
       "  3    1927.157261   897.152618  2081.758102   940.032288   2060.920364   \n",
       "  4    1915.464228   891.566833  2078.478775   951.465271   1963.304695   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  260  3283.048141   596.722504  3177.683853   736.405884   3294.841415   \n",
       "  261  3295.644394   578.976410  3206.110260   746.245300   3287.908539   \n",
       "  262  3299.031494   573.714722  3235.108810   736.418640   3272.087509   \n",
       "  263  3298.505707   573.589050  3257.643631   721.635925   3270.311783   \n",
       "  264  3289.900742   582.786163  3292.155746   723.847290   3282.958450   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      914.982300  2079.193542  1139.040466   2076.238708   1147.388245  \n",
       "  1      934.487793  2067.312469  1158.080627   2054.008324   1166.120789  \n",
       "  2      942.447693  2060.654984  1169.706665   2048.600632   1176.077271  \n",
       "  3      967.222229  2057.148270  1176.850098   2024.251114   1203.791016  \n",
       "  4     1027.150696  2052.189896  1184.368774   1963.949852   1223.910706  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  260    761.065552  3093.929817   883.769714   3279.013290    914.803894  \n",
       "  261    756.525513  3104.937252   889.145813   3258.297073    910.432068  \n",
       "  262    753.291626  3110.189758   876.240906   3122.240501    887.224182  \n",
       "  263    725.953857  3136.295692   856.345825   3136.166504    857.886719  \n",
       "  264    725.643066  3167.941231   844.640747   3163.181664    848.134644  \n",
       "  \n",
       "  [265 rows x 34 columns],\n",
       "  1),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1590.477356  1075.428284  1598.445251  1059.405655  1595.140900   \n",
       "  1    1591.324005  1062.382156  1603.282654  1050.508331  1592.792908   \n",
       "  2    1562.608307  1048.559692  1569.004028  1040.015228  1573.218414   \n",
       "  3    1535.605667  1017.273270  1550.896637  1010.583549  1547.591766   \n",
       "  4    1548.068604  1056.123154  1555.299835  1045.296997  1557.011047   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  296   358.602161   738.512989   354.298634   730.705551   359.643108   \n",
       "  297   353.564758   728.911083   349.886738   721.697090   353.136879   \n",
       "  298   346.234818   734.117218   343.816814   726.057922   344.625875   \n",
       "  299   323.847182   727.922874   323.312138   717.709866   324.834927   \n",
       "  300   318.812203   721.415710   319.587463   711.162916   318.993789   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1058.828308  1632.175568  1060.640015  1619.362396  1057.744446  ...   \n",
       "  1    1049.132217  1637.249542  1050.671463  1602.529846  1051.489059  ...   \n",
       "  2    1036.387253  1598.222656  1056.201035  1637.079529  1041.053864  ...   \n",
       "  3    1006.080132  1594.893341  1040.442123  1612.508118  1026.943611  ...   \n",
       "  4    1043.113930  1587.510559  1062.619293  1632.140350  1047.300987  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  296   730.193741   322.907145   738.579823   363.301773   739.641930  ...   \n",
       "  297   720.713207   319.082068   729.797977   354.786827   731.718102  ...   \n",
       "  298   725.577991   320.120645   732.528709   344.224955   734.715267  ...   \n",
       "  299   717.195347   322.680170   725.817001   338.495031   724.715267  ...   \n",
       "  300   710.614475   320.243775   720.656467   330.135481   717.050709  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1669.260559  1389.137146  1562.476471  1258.107910   1549.565353   \n",
       "  1    1651.208435  1357.543030  1543.781250  1345.511658   1542.636139   \n",
       "  2    1606.793243  1342.711884  1517.028641  1438.246155   1519.078217   \n",
       "  3    1605.661072  1341.640137  1514.394104  1446.843567   1511.722504   \n",
       "  4    1595.070496  1324.786285  1528.154282  1441.939453   1519.539429   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  296   383.277405   992.498108   343.931751  1134.549774    407.286774   \n",
       "  297   374.036064   978.489777   329.684504  1115.618988    396.326485   \n",
       "  298   358.558617   976.267242   346.295002  1107.447235    379.020309   \n",
       "  299   347.460077   971.514069   350.084946  1109.625031    365.302471   \n",
       "  300   330.959883   973.089844   343.527922  1100.981567    344.403704   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1250.732361  1444.576126  1569.344910   1441.711685   1569.096191  \n",
       "  1     1323.167542  1440.937866  1583.163818   1435.212761   1577.367126  \n",
       "  2     1424.475281  1431.247086  1591.791260   1431.519699   1587.292053  \n",
       "  3     1435.338745  1425.367935  1600.972595   1424.198822   1597.022339  \n",
       "  4     1424.834839  1425.351929  1610.364014   1415.691544   1599.749390  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  296   1121.020416   409.807365  1244.489197    449.147766   1239.094238  \n",
       "  297   1106.513641   389.081589  1232.788391    444.272980   1228.989075  \n",
       "  298   1101.516083   428.116592  1224.377930    434.731483   1217.881226  \n",
       "  299   1103.713257   422.848381  1217.281311    420.326256   1212.180725  \n",
       "  300   1099.000977   410.161987  1204.429749    404.235588   1203.661865  \n",
       "  \n",
       "  [301 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1585.686035  1087.483139  1595.683350  1070.821114  1574.326691   \n",
       "  1    1550.027252  1077.656845  1564.098389  1059.542595  1548.400818   \n",
       "  2    1537.250732  1087.399475  1545.249725  1067.898941  1530.030060   \n",
       "  3    1536.436279  1080.658005  1543.269531  1062.588806  1537.533264   \n",
       "  4    1489.409149  1062.488472  1499.453125  1046.764015  1495.408630   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  296   336.006355   800.127744   333.556923   792.844869   336.078932   \n",
       "  297   322.834412   804.755783   324.394691   797.421942   322.603599   \n",
       "  298   331.854388   807.269505   333.421263   799.955700   331.242296   \n",
       "  299   333.015747   812.293569   335.778828   804.688474   331.609370   \n",
       "  300   328.858055   828.287310   326.431949   820.498213   329.511021   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1074.619972  1611.076294  1071.192635  1555.431396  1077.715317  ...   \n",
       "  1    1060.131416  1612.226044  1061.421890  1562.953796  1058.135963  ...   \n",
       "  2    1071.047195  1596.507660  1045.300835  1550.896515  1056.148804  ...   \n",
       "  3    1063.082199  1586.635559  1052.511429  1561.680603  1058.655060  ...   \n",
       "  4    1045.128067  1555.114960  1053.181145  1584.757141  1041.790352  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  296   792.403191   331.317991   796.104724   373.913139   802.150110  ...   \n",
       "  297   796.431697   337.268114   801.133166   362.947712   802.760691  ...   \n",
       "  298   798.878218   341.078587   804.558080   348.254366   809.969518  ...   \n",
       "  299   803.551148   344.709221   809.427469   330.925920   815.365255  ...   \n",
       "  300   819.736835   322.810010   822.804352   356.903599   820.220787  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1550.476013  1391.650208  1551.251007  1567.433533   1513.300720   \n",
       "  1    1546.142700  1488.215271  1506.982574  1596.836121   1479.548065   \n",
       "  2    1552.564911  1374.071564  1515.492340  1523.705933   1487.272491   \n",
       "  3    1532.465851  1391.173004  1513.359009  1489.288940   1457.942657   \n",
       "  4    1582.161682  1363.686493  1494.427704  1385.506470   1490.006226   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  296   385.503098  1019.055939   329.590905  1150.337311    371.858299   \n",
       "  297   379.929050  1018.938736   331.316185  1153.545441    368.815308   \n",
       "  298   371.202610  1011.439056   339.830599  1137.248352    365.402405   \n",
       "  299   358.149559  1024.178116   344.123842  1163.996063    353.683388   \n",
       "  300   360.448898  1025.327698   326.869349  1158.538544    352.696648   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1557.561829  1499.564758  1565.785706   1475.531891   1557.343506  \n",
       "  1     1583.972412  1488.640259  1587.277649   1473.438995   1581.627258  \n",
       "  2     1476.867371  1433.168198  1570.472107   1412.609070   1558.848450  \n",
       "  3     1457.838745  1462.860413  1591.194946   1421.952103   1566.725891  \n",
       "  4     1371.839294  1419.065994  1593.846375   1412.598694   1582.975586  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  296   1150.907013   338.586939  1279.187775    375.215542   1279.700592  \n",
       "  297   1152.374695   341.362736  1282.148407    371.246243   1280.197815  \n",
       "  298   1136.464935   355.237019  1262.227570    370.216473   1261.404541  \n",
       "  299   1161.353058   370.968761  1290.582428    371.665905   1287.075348  \n",
       "  300   1156.292816   347.318159  1283.424591    365.941158   1280.127899  \n",
       "  \n",
       "  [301 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1655.622803  1080.326900  1666.941040  1075.878421  1652.115204   \n",
       "  1    1656.787170  1076.145961  1669.385498  1071.000000  1650.903320   \n",
       "  2    1593.356049  1042.529785  1609.243225  1028.331802  1594.374176   \n",
       "  3    1614.415558  1078.205307  1619.175873  1061.568321  1615.955414   \n",
       "  4    1593.647369  1076.861893  1606.172699  1063.577621  1594.576569   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  290   358.629181   819.594105   356.163168   811.958351   361.960428   \n",
       "  291   364.966063   817.870106   361.286276   810.435883   366.920229   \n",
       "  292   351.430056   823.887299   350.512558   816.528591   352.605337   \n",
       "  293   348.090657   829.100250   347.918015   821.924713   348.886542   \n",
       "  294   340.219646   816.958664   341.311215   810.356550   339.974811   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1073.373830  1673.630615  1085.367814  1629.403290  1078.719244  ...   \n",
       "  1    1071.000000  1683.394226  1079.626597  1627.697388  1078.012559  ...   \n",
       "  2    1029.181671  1669.816101  1029.774796  1616.270264  1015.480038  ...   \n",
       "  3    1062.017204  1662.204712  1051.778015  1650.426331  1050.449844  ...   \n",
       "  4    1061.863945  1661.076294  1056.343796  1625.815704  1050.936234  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  290   812.001667   342.198264   819.536819   383.113434   819.000919  ...   \n",
       "  291   810.318260   337.953216   817.275177   375.952606   818.350925  ...   \n",
       "  292   816.489422   340.198529   821.524120   366.128731   823.500645  ...   \n",
       "  293   822.168350   341.415586   827.254494   357.178198   826.369331  ...   \n",
       "  294   810.311934   338.120482   816.602552   351.758015   818.015722  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1579.292480  1317.618347  1558.174530  1437.147339   1531.849121   \n",
       "  1    1589.482635  1311.549606  1543.010132  1438.998230   1520.466141   \n",
       "  2    1590.468475  1312.981964  1530.139847  1442.322174   1519.592041   \n",
       "  3    1603.364136  1300.606537  1531.774353  1445.383698   1525.849808   \n",
       "  4    1598.690430  1300.109528  1532.339767  1447.763977   1513.213669   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  290   404.334572  1066.110992   385.603481  1204.853058    414.046715   \n",
       "  291   397.241947  1066.605194   378.370632  1208.972443    405.716881   \n",
       "  292   381.841011  1057.024597   369.724991  1194.699432    387.719257   \n",
       "  293   372.557114  1049.267883   368.653744  1183.872650    373.383224   \n",
       "  294   363.566198  1055.917236   360.718113  1194.022888    360.156416   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1422.005951  1499.603348  1558.511017   1485.796677   1554.337494  \n",
       "  1     1425.247681  1483.454010  1563.496948   1472.085709   1562.862854  \n",
       "  2     1431.172180  1457.014923  1587.334656   1455.236313   1583.241089  \n",
       "  3     1431.985291  1454.867798  1610.181091   1452.780243   1607.016907  \n",
       "  4     1418.742126  1454.754745  1627.016541   1459.116333   1614.810791  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  290   1199.908905   475.397385  1315.515137    482.407867   1309.796387  \n",
       "  291   1203.302155   472.219879  1305.729248    473.099731   1301.481384  \n",
       "  292   1189.477661   457.025612  1293.102112    457.380730   1290.323853  \n",
       "  293   1179.019348   445.057068  1292.681213    437.452286   1288.455292  \n",
       "  294   1188.797394   431.609657  1294.519897    423.245514   1291.492645  \n",
       "  \n",
       "  [295 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1784.303467  989.440704  1778.737717  973.422798  1784.843018   \n",
       "  1    1802.423340  980.962669  1795.150055  963.948479  1807.854614   \n",
       "  2    1767.228256  989.933212  1762.815186  970.560898  1763.989227   \n",
       "  3    1761.201996  991.977386  1757.290863  972.261063  1756.944962   \n",
       "  4    1755.996902  993.080902  1753.464035  973.482224  1751.847382   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  283   386.009193  790.073799   381.953602  782.160452   388.691071   \n",
       "  284   375.878647  796.887558   372.465042  788.569910   379.213654   \n",
       "  285   372.403793  799.480389   373.134405  790.728566   370.111950   \n",
       "  286   369.446239  861.566261   373.195654  852.320980   364.777655   \n",
       "  287   365.493813  844.667805   368.204199  838.472250   364.514698   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     979.990555  1784.636871  941.526924  1802.966949   948.929146  ...   \n",
       "  1     968.030373  1783.874176  943.270813  1808.929169   941.661285  ...   \n",
       "  2     977.964951  1792.026825  938.635353  1788.757233   955.932625  ...   \n",
       "  3     980.270332  1794.254791  940.908485  1784.583862   959.321800  ...   \n",
       "  4     980.385170  1790.440063  942.830048  1777.511963   960.619003  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  283   782.360456   361.062872  789.022091   404.623177   785.948120  ...   \n",
       "  284   789.000700   359.590420  793.357121   399.751049   788.264591  ...   \n",
       "  285   790.221629   369.423109  792.885768   384.744492   795.379576  ...   \n",
       "  286   852.138668   385.126972  845.985172   365.590134   847.911140  ...   \n",
       "  287   838.081362   377.556837  836.947006   385.848671   835.010068  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1720.256882  1168.206177  1605.370018  1182.459930   1606.459938   \n",
       "  1    1713.719849  1171.870972  1605.425125  1177.107513   1605.746567   \n",
       "  2    1729.631546  1168.625031  1674.077728  1154.287628   1646.622986   \n",
       "  3    1748.869843  1169.700623  1681.833878  1147.827026   1661.819946   \n",
       "  4    1726.055145  1168.342712  1623.203987  1167.258881   1611.162102   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  283   411.093773  1023.064026   387.674606  1180.130249    401.670834   \n",
       "  284   408.546371  1030.354279   382.143982  1184.071289    401.995911   \n",
       "  285   391.845291  1021.168701   385.312424  1164.574860    395.972198   \n",
       "  286   381.672119  1028.418228   384.994694  1153.236420    385.146759   \n",
       "  287   393.743713   977.458572   389.115528  1071.269745    391.916489   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1171.306732  1632.307220  1160.021057   1630.292793   1156.859894  \n",
       "  1     1169.613770  1627.059143  1155.858276   1619.109169   1155.066589  \n",
       "  2     1152.428650  1667.583862  1144.627106   1638.551170   1146.489227  \n",
       "  3     1145.261047  1658.380722  1138.135376   1628.692787   1143.034576  \n",
       "  4     1163.913910  1615.850052  1148.887177   1604.791100   1154.317932  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  283   1174.353119   440.304497  1324.723389    452.727486   1309.155762  \n",
       "  284   1181.188538   425.810135  1323.434875    432.676109   1317.454529  \n",
       "  285   1162.080750   425.021286  1300.570801    424.743134   1297.776855  \n",
       "  286   1152.040161   402.383213  1268.147461    401.600018   1267.018372  \n",
       "  287   1069.928772   389.536514  1155.018463    389.047585   1152.552155  \n",
       "  \n",
       "  [288 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1599.915756  1115.000000  1606.695953  1115.000000  1594.818481   \n",
       "  1    1589.373322  1076.722740  1596.022446  1062.874199  1592.772308   \n",
       "  2    1606.572693  1089.155258  1613.626053  1080.903572  1612.177719   \n",
       "  3    1584.469955  1090.123466  1590.011017  1077.493752  1589.015457   \n",
       "  4    1584.209839  1100.100777  1589.500641  1086.669395  1583.964874   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  287   297.578269   814.360878   299.478083   804.817398   295.000000   \n",
       "  288   304.000000   797.111191   305.619121   791.932602   304.000000   \n",
       "  289   308.037804   796.084972   312.471320   790.372906   306.000000   \n",
       "  290   321.907209   805.618645   326.744759   795.524231   312.339894   \n",
       "  291   317.707126   805.240791   322.829290   795.747654   308.873188   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1115.000000  1625.379318  1115.000000  1599.967682  1115.000000  ...   \n",
       "  1    1063.945736  1633.352615  1061.387405  1643.778687  1079.133869  ...   \n",
       "  2    1078.811562  1640.902466  1088.846092  1660.771149  1082.108421  ...   \n",
       "  3    1074.961487  1620.922577  1081.011475  1659.235107  1077.250351  ...   \n",
       "  4    1084.504372  1634.321655  1080.517662  1651.438995  1074.025856  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  287   805.929821   305.277355   811.942780   295.000000   816.075741  ...   \n",
       "  288   794.060516   312.197044   808.212421   304.000000   812.294735  ...   \n",
       "  289   792.638313   322.431259   807.236610   306.000000   813.128342  ...   \n",
       "  290   797.587608   336.402060   806.785751   305.000000   825.328087  ...   \n",
       "  291   797.265053   333.822380   808.708084   302.000000   825.699295  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1625.209290  1294.972733  1579.121567  1319.110657   1577.217819   \n",
       "  1    1632.130722  1345.123444  1583.005676  1317.067871   1582.337601   \n",
       "  2    1662.746826  1336.542267  1583.681366  1339.682892   1566.597565   \n",
       "  3    1683.984619  1365.777161  1568.503906  1342.669189   1573.020981   \n",
       "  4    1688.369904  1348.643799  1573.982681  1362.292114   1581.393326   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  287   308.143140  1059.291748   361.169525  1196.317169    315.967880   \n",
       "  288   317.289130  1064.847992   364.819218  1195.591675    327.193508   \n",
       "  289   307.940648  1070.123718   364.123001  1206.135986    316.311392   \n",
       "  290   316.458717  1068.365509   365.293079  1205.887024    335.047005   \n",
       "  291   313.738549  1074.776672   371.146591  1216.908203    328.509005   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1293.703827  1508.833855  1538.096130   1498.164207   1520.182465  \n",
       "  1     1309.138489  1502.344200  1545.992676   1500.795723   1532.045929  \n",
       "  2     1337.754791  1500.596138  1561.794373   1506.152504   1556.229736  \n",
       "  3     1326.650574  1495.535583  1570.821289   1496.011398   1557.780457  \n",
       "  4     1343.867249  1480.339005  1588.051941   1518.761490   1577.805786  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  287   1197.905853   351.705975  1340.339111    313.589005   1341.701599  \n",
       "  288   1194.901855   344.449993  1343.426575    315.533885   1341.289978  \n",
       "  289   1206.021393   344.043194  1349.481995    311.238593   1346.250244  \n",
       "  290   1205.282990   342.397743  1349.124573    326.597927   1343.121765  \n",
       "  291   1216.735352   341.981972  1352.456177    320.629528   1343.431396  \n",
       "  \n",
       "  [292 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1687.617096  1038.443481  1689.997162  1016.192162  1684.585571   \n",
       "  1    1673.290405  1031.356140  1678.367706  1012.191032  1671.285706   \n",
       "  2    1685.573059  1068.974548  1681.320160  1047.299446  1678.133728   \n",
       "  3    1675.003601  1063.819138  1669.962830  1041.903313  1668.231812   \n",
       "  4    1646.317810  1036.101601  1650.455231  1017.310425  1644.501740   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  251   353.723661   892.968880   355.871422   883.694244   354.801998   \n",
       "  252   352.546464   891.373474   355.886068   883.235588   352.946902   \n",
       "  253   353.487779   879.304573   354.044750   868.490288   354.846462   \n",
       "  254   359.374028   920.471069   364.496223   915.918694   358.946893   \n",
       "  255   362.581027  1097.978447   371.378482  1093.257488   362.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1021.633469  1739.503937   997.875565  1734.512970  1010.850517  ...   \n",
       "  1    1016.119736  1733.289276   998.915215  1713.895721  1009.992058  ...   \n",
       "  2    1053.972435  1724.322479  1002.757416  1724.292664  1020.270653  ...   \n",
       "  3    1048.635948  1710.174774  1001.983459  1715.120514  1020.366966  ...   \n",
       "  4    1020.782127  1700.793579  1003.006302  1708.657257  1007.771217  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  251   883.878654   363.050731   885.103638   382.325165   884.256424  ...   \n",
       "  252   882.913879   362.691705   888.032578   367.033329   888.267372  ...   \n",
       "  253   867.242886   354.363832   875.733704   359.185339   874.484245  ...   \n",
       "  254   913.946938   367.964442   916.907677   353.111183   903.748837  ...   \n",
       "  255  1091.261265   384.301647  1092.746803   362.000000  1080.462086  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1715.892670  1339.793427  1630.054672  1358.551178   1636.647873   \n",
       "  1    1700.569916  1332.197021  1621.318054  1376.748291   1605.918457   \n",
       "  2    1735.658173  1334.870148  1630.707214  1377.620941   1616.446121   \n",
       "  3    1704.414307  1329.351227  1625.348221  1395.425598   1605.604996   \n",
       "  4    1730.741119  1322.470062  1630.352112  1386.135345   1618.009354   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  251   381.947098  1116.131470   402.941116  1251.114502    400.094982   \n",
       "  252   366.244123  1114.047455   401.881775  1248.896576    399.423897   \n",
       "  253   355.950792  1112.246185   389.675655  1244.115417    387.489353   \n",
       "  254   360.250461  1125.159393   383.013540  1255.925476    378.282732   \n",
       "  255   377.191042  1130.236717   380.728203  1257.797195    368.183687   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1342.823212  1539.799072  1451.896729   1574.879333   1443.000183  \n",
       "  1     1353.348907  1515.677498  1497.443237   1539.158646   1470.497498  \n",
       "  2     1329.055817  1517.944672  1531.631531   1535.639816   1443.980286  \n",
       "  3     1345.240906  1522.937393  1553.130798   1524.337112   1455.135315  \n",
       "  4     1345.670624  1524.801758  1567.234192   1522.683533   1480.548767  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  251   1251.385284   457.365349  1385.851990    454.751656   1381.920288  \n",
       "  252   1250.410187   456.520660  1389.196899    454.743446   1386.258179  \n",
       "  253   1246.786285   452.507767  1382.345032    443.163399   1380.382568  \n",
       "  254   1252.758972   449.669861  1372.609680    443.355728   1365.776428  \n",
       "  255   1246.098328   442.556351  1363.867340    435.164223   1360.640625  \n",
       "  \n",
       "  [256 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2137.887909  859.669922  2128.183151  841.976212  2132.796310   \n",
       "  1    2144.700241  830.760178  2131.468857  814.619598  2137.812805   \n",
       "  2    2140.830948  814.056427  2131.099792  800.758049  2137.797302   \n",
       "  3    2151.425385  814.674583  2139.007065  801.934555  2147.608490   \n",
       "  4    2151.442657  825.831139  2135.439819  812.096252  2145.551956   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  133  3401.371849  519.513924  3399.082771  512.948322  3406.795204   \n",
       "  134  3401.456169  522.752689  3400.036194  514.545456  3405.406944   \n",
       "  135  3403.412178  528.426922  3401.887039  519.169907  3407.748405   \n",
       "  136  3410.834686  534.561321  3411.136261  526.438606  3414.303177   \n",
       "  137  3451.455437  537.260159  3454.094048  528.474716  3448.445023   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     843.209000  2067.020760  838.701767  2099.449051   845.185013  ...   \n",
       "  1     816.295578  2065.625664  824.131805  2106.182404   833.734818  ...   \n",
       "  2     799.227898  2084.816505  809.932327  2113.082840   827.007095  ...   \n",
       "  3     798.510185  2072.538521  812.722801  2118.576492   820.093384  ...   \n",
       "  4     812.979118  2064.792694  819.112068  2126.236115   826.158211  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  133   512.341766  3390.991158  519.480389  3451.992462   523.662754  ...   \n",
       "  134   513.629559  3400.128708  520.311073  3457.399506   523.422756  ...   \n",
       "  135   518.465687  3408.806519  525.005581  3464.688721   527.794220  ...   \n",
       "  136   525.882847  3421.512009  535.654091  3467.344772   532.862556  ...   \n",
       "  137   528.558023  3457.014236  532.014614  3467.001968   528.979635  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2113.262421  1250.598145  2184.011826  1281.041382   2183.488022   \n",
       "  1    2124.621399  1275.454041  2241.871002  1310.924988   2259.055969   \n",
       "  2    2107.895454  1230.631714  2214.349930  1278.942200   2219.767822   \n",
       "  3    2053.921974  1224.667297  2191.393585  1087.154327   2169.358246   \n",
       "  4    2106.709709  1207.997437  2231.233841  1295.244202   2201.065491   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  133  3450.756607   753.374298  3379.844765   878.217194   3436.873154   \n",
       "  134  3460.097000   746.994263  3397.330688   889.391998   3443.267487   \n",
       "  135  3471.780670   751.526825  3410.686302   886.179199   3451.027603   \n",
       "  136  3473.130356   745.876495  3421.410194   879.983734   3457.662941   \n",
       "  137  3451.414085   736.800537  3448.411179   868.579559   3448.205498   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1311.545288  2206.749451  1351.764771   2194.758347   1360.901794  \n",
       "  1     1346.715393  2267.505707  1351.693115   2277.397675   1365.516907  \n",
       "  2     1302.936829  2262.258453  1278.588562   2267.112579   1280.799744  \n",
       "  3     1130.160187  2307.937775  1366.710876   2321.318298   1402.515076  \n",
       "  4     1326.652222  2335.947205  1453.715027   2329.517426   1462.162903  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  133    889.177246  3368.946819   993.576294   3415.970520   1024.455200  \n",
       "  134    890.834503  3392.284065  1011.602051   3423.498848   1020.937073  \n",
       "  135    890.233582  3413.599274   994.100464   3434.753105   1001.386841  \n",
       "  136    882.698883  3422.710915   999.688965   3441.149689   1004.573944  \n",
       "  137    868.394806  3436.299454   986.090363   3434.785667    987.522614  \n",
       "  \n",
       "  [138 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2191.039536  843.604568  2181.077194  824.792419  2183.552933   \n",
       "  1    2199.626801  848.197861  2190.513199  825.754562  2192.190399   \n",
       "  2    2215.805496  855.918457  2210.106522  832.554108  2203.797150   \n",
       "  3    2231.309982  862.961929  2222.047943  843.580841  2225.509384   \n",
       "  4    2271.809036  883.163208  2265.662109  865.772263  2265.614532   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  104  3502.368568  649.693836  3502.054543  643.011915  3507.814980   \n",
       "  105  3508.643005  651.044926  3509.774063  644.611261  3512.968681   \n",
       "  106  3522.639771  650.290443  3524.926170  644.582664  3526.750862   \n",
       "  107  3554.627632  660.529320  3555.749977  652.282333  3553.337364   \n",
       "  108  3546.360146  645.973583  3548.172554  636.426590  3547.907356   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     826.187759  2126.435158  827.371704  2140.713600   833.996002  ...   \n",
       "  1     826.784348  2125.829536  826.478958  2143.049454   832.564819  ...   \n",
       "  2     834.560364  2160.050903  829.006615  2154.709938   838.044678  ...   \n",
       "  3     844.418488  2142.778816  843.330627  2190.088058   850.217514  ...   \n",
       "  4     864.318192  2191.995644  870.867645  2218.796021   873.064468  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  104   642.129118  3499.254913  648.553574  3551.211151   649.059574  ...   \n",
       "  105   643.697884  3510.529984  650.011574  3551.642509   648.683950  ...   \n",
       "  106   643.861546  3524.629372  651.497484  3554.339325   651.073446  ...   \n",
       "  107   651.130848  3547.661499  656.828155  3550.417038   658.349625  ...   \n",
       "  108   635.565716  3550.948524  640.912434  3556.131638   644.359779  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2129.746239  1202.499695  2288.578003  1283.510742   2282.614853   \n",
       "  1    2146.329315  1199.908691  2302.683075  1285.004089   2265.800476   \n",
       "  2    2156.413116  1218.241394  2297.584702  1295.188049   2261.447433   \n",
       "  3    2144.835922  1227.836670  2293.380737  1320.178162   2270.471710   \n",
       "  4    2135.297997  1280.031860  2296.614410  1347.893982   2226.619751   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  104  3553.946777   902.041809  3487.885925  1037.395264   3546.543335   \n",
       "  105  3555.513145   890.697784  3521.493820  1034.949799   3547.995117   \n",
       "  106  3553.845642   889.600281  3535.694016  1032.848846   3550.181023   \n",
       "  107  3549.388908   918.909302  3551.801575  1056.480194   3548.088783   \n",
       "  108  3562.532478   888.455200  3561.248726   989.162781   3560.008240   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1297.159485  2394.009979  1445.773254   2397.453247   1443.945312  \n",
       "  1     1313.428650  2391.465271  1465.078552   2381.832031   1470.981995  \n",
       "  2     1315.279175  2389.251434  1499.635498   2383.642761   1505.686768  \n",
       "  3     1331.740662  2378.880371  1548.545532   2371.911163   1550.920532  \n",
       "  4     1388.109619  2354.084106  1590.377625   2304.709000   1608.236572  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  104   1045.709686  3471.878559  1153.876160   3532.782753   1182.384644  \n",
       "  105   1034.617035  3519.625504  1161.435486   3532.540405   1162.504883  \n",
       "  106   1031.667236  3538.306740  1166.542480   3540.180504   1166.110657  \n",
       "  107   1056.322968  3549.608429  1163.735962   3545.457619   1165.572021  \n",
       "  108    991.352600  3556.392273  1032.660461   3550.601967   1033.151337  \n",
       "  \n",
       "  [109 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2135.298218   999.078156  2138.162796  979.807953  2132.689575   \n",
       "  1    2150.379951  1006.822144  2150.395576  988.834549  2152.739731   \n",
       "  2    2170.226379  1004.630417  2163.010315  985.595322  2173.007019   \n",
       "  3    2193.131042  1001.771103  2184.326599  979.556061  2191.694275   \n",
       "  4    2210.753769   990.929031  2204.141068  969.750061  2207.716415   \n",
       "  ..           ...          ...          ...         ...          ...   \n",
       "  144  3456.250877   532.022854  3455.800705  525.656088  3461.078285   \n",
       "  145  3464.625877   540.740448  3465.925148  534.552361  3468.554703   \n",
       "  146  3475.067184   544.462952  3474.938599  537.636551  3480.717445   \n",
       "  147  3496.588715   547.620895  3497.991394  540.932281  3496.594650   \n",
       "  148  3507.460426   548.117828  3509.496040  540.419945  3506.106606   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     977.453232  2107.130783  950.289993  2101.108109   941.287926  ...   \n",
       "  1     985.683609  2104.117958  957.281738  2128.925911   956.424835  ...   \n",
       "  2     983.956161  2111.639801  962.404938  2146.035339   956.582764  ...   \n",
       "  3     983.158783  2120.636688  970.971603  2154.497597   969.799377  ...   \n",
       "  4     970.766830  2141.035255  963.194519  2157.867332   966.159363  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  144   524.700977  3453.060257  531.811268  3502.964119   529.248768  ...   \n",
       "  145   533.347891  3465.970406  540.656734  3504.368439   538.085045  ...   \n",
       "  146   537.484291  3474.734436  549.827438  3514.847076   546.523666  ...   \n",
       "  147   540.473946  3489.589088  548.379990  3494.879860   549.971306  ...   \n",
       "  148   540.261559  3505.337410  543.850342  3499.554443   546.355591  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2080.566742  1275.843018  2202.246841  1335.348389   2180.180786   \n",
       "  1    2079.400944  1320.467804  2203.169250  1334.514191   2144.233124   \n",
       "  2    2105.727379  1326.738281  2216.553848  1374.940369   2196.079086   \n",
       "  3    2117.007896  1342.319702  2217.306931  1342.975037   2205.611496   \n",
       "  4    2116.017082  1329.585693  2211.780396  1431.875977   2210.845245   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  144  3509.929199   767.336395  3470.553658   909.965393   3495.070244   \n",
       "  145  3503.474251   769.830780  3484.920403   908.214417   3496.624283   \n",
       "  146  3515.000000   769.389954  3489.478859   883.937836   3515.000000   \n",
       "  147  3499.842110   805.163452  3509.245399   855.522186   3512.000000   \n",
       "  148  3495.939972   778.867737  3502.107201   832.878021   3493.319870   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1348.241394  2312.241974  1572.279785   2282.583893   1579.035950  \n",
       "  1     1365.917969  2299.219208  1578.873413   2238.325226   1606.402283  \n",
       "  2     1400.089355  2296.728394  1592.848022   2265.695358   1610.626465  \n",
       "  3     1378.191101  2280.563049  1593.481323   2265.273544   1603.780579  \n",
       "  4     1455.683899  2284.082733  1611.727783   2264.465942   1618.820435  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  144    906.855072  3463.964920  1042.716125   3473.761444   1041.917175  \n",
       "  145    910.116394  3467.684082  1034.721863   3469.063858   1039.708313  \n",
       "  146    883.164215  3484.919037   931.000000   3512.220528    927.635712  \n",
       "  147    858.087433  3495.601463   855.568115   3493.721931    858.998016  \n",
       "  148    833.176117  3491.188057   821.647827   3473.335979    825.246246  \n",
       "  \n",
       "  [149 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2108.382244  794.144272  2115.954273  783.468224  2113.273453   \n",
       "  1    2174.128311  822.280960  2180.751923  807.261757  2168.750595   \n",
       "  2    2177.521408  823.446358  2179.022659  807.516891  2183.745895   \n",
       "  3    2183.350296  826.129578  2188.890396  808.119461  2178.817329   \n",
       "  4    2209.540985  809.366554  2213.580154  790.442680  2198.896683   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  303  3521.316399  505.570034  3522.831551  496.933773  3524.826981   \n",
       "  304  3546.565018  508.196274  3551.082245  499.281086  3541.928444   \n",
       "  305  3553.517738  513.322952  3556.000000  505.233120  3550.829994   \n",
       "  306  3548.312592  508.353661  3551.716423  500.299229  3546.080925   \n",
       "  307  3552.568787  506.831345  3553.000000  498.875557  3551.365837   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     777.935410  2151.260452  795.445518  2167.283234   787.556786  ...   \n",
       "  1     802.789543  2163.195961  796.689026  2128.769112   787.653656  ...   \n",
       "  2     802.183159  2133.761154  784.792564  2189.701324   788.548546  ...   \n",
       "  3     805.814026  2163.804977  791.646759  2139.609901   785.447372  ...   \n",
       "  4     788.864296  2189.079323  787.043701  2154.583954   782.314819  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  303   494.832781  3524.011658  505.584637  3553.955513   506.208313  ...   \n",
       "  304   498.269566  3550.515831  502.307430  3538.811119   507.059399  ...   \n",
       "  305   503.182606  3556.000000  506.901131  3542.268669   507.012760  ...   \n",
       "  306   498.904259  3551.560699  504.684048  3546.220825   504.818363  ...   \n",
       "  307   497.388851  3550.571404  503.332424  3546.951225   503.722366  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2169.532211  1011.803864  2314.949509  1001.317261   2292.872757   \n",
       "  1    2092.038116  1163.287506  2196.063675  1148.437042   2147.435318   \n",
       "  2    2094.640347  1170.799347  2187.530701  1199.124298   2138.092690   \n",
       "  3    2101.739643  1184.639740  2206.912415  1228.959412   2163.846031   \n",
       "  4    2115.669567  1202.070770  2188.783829  1267.034851   2165.977470   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  303  3538.014290   754.583099  3490.538441   861.811768   3531.470573   \n",
       "  304  3515.641037   745.821320  3537.158951   868.800659   3502.836426   \n",
       "  305  3516.242294   746.158875  3530.324661   862.856415   3498.209763   \n",
       "  306  3525.711967   766.590698  3524.852631   887.431335   3508.089470   \n",
       "  307  3535.791519   758.574036  3522.518700   883.873322   3522.216705   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      995.436920  2330.736328   987.172974   2308.388809    971.755066  \n",
       "  1     1165.383057  2195.206802  1157.482422   2160.358299   1165.676483  \n",
       "  2     1212.362762  2199.801865  1201.997284   2172.106667   1206.250275  \n",
       "  3     1249.120361  2256.038177  1239.485413   2212.974136   1244.805542  \n",
       "  4     1284.702515  2247.617004  1275.549805   2222.261887   1278.193481  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  303    869.264404  3474.052330   971.405548   3511.533554    982.469910  \n",
       "  304    865.732666  3513.584694   979.599915   3480.119659    976.140411  \n",
       "  305    858.313019  3497.311668   966.442139   3478.078262    961.375092  \n",
       "  306    887.172729  3498.621738   983.048279   3484.306732    979.948364  \n",
       "  307    886.138641  3497.588390   991.230591   3504.458584    996.011108  \n",
       "  \n",
       "  [308 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2177.042816  808.194931  2172.767593  793.574692  2166.936188   \n",
       "  1    2199.200745  813.855957  2194.785965  797.729568  2189.819412   \n",
       "  2    2207.560471  825.312592  2206.634247  809.439850  2200.914505   \n",
       "  3    2222.779648  831.884369  2221.778290  813.946541  2214.682159   \n",
       "  4    2237.532364  840.093307  2236.626007  822.290176  2230.821945   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  304  3520.805382  543.998566  3520.435646  535.295631  3526.639832   \n",
       "  305  3522.447289  535.634884  3523.947205  527.719799  3527.292152   \n",
       "  306  3544.351471  548.643387  3544.319267  539.781288  3548.534569   \n",
       "  307  3551.065231  539.276600  3553.607628  530.805027  3552.074287   \n",
       "  308  3556.235603  555.205437  3559.281082  546.042603  3552.844391   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     792.056900  2135.796364  799.328781  2114.617363   795.512695  ...   \n",
       "  1     798.730637  2139.152069  797.454956  2128.083214   799.707062  ...   \n",
       "  2     807.646927  2157.906860  806.154030  2144.271355   803.882820  ...   \n",
       "  3     814.035553  2176.724037  811.381660  2156.148338   805.193329  ...   \n",
       "  4     822.118279  2181.996208  814.487076  2172.358871   807.701393  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  304   533.136806  3523.667000  545.881130  3558.000000   545.183041  ...   \n",
       "  305   525.666622  3529.334732  539.970634  3556.161736   535.593067  ...   \n",
       "  306   537.317604  3539.192200  544.902943  3553.283813   542.818039  ...   \n",
       "  307   528.639839  3549.209877  537.168854  3549.656052   530.773212  ...   \n",
       "  308   544.978470  3556.468895  546.496731  3550.478767   551.082569  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2104.795273  1149.943329  2216.260773  1241.924133   2140.257401   \n",
       "  1    2098.856392  1168.299408  2214.189178  1239.688354   2136.847977   \n",
       "  2    2099.109867  1173.291229  2216.049026  1244.638000   2136.380692   \n",
       "  3    2111.373070  1173.358307  2216.090897  1252.287903   2148.436272   \n",
       "  4    2117.642349  1164.273468  2205.708237  1249.313599   2154.642891   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  304  3552.782112   793.127899  3500.611362   931.227814   3537.203369   \n",
       "  305  3545.731674   781.550812  3502.086063   923.055603   3540.709679   \n",
       "  306  3540.359299   785.488159  3510.016254   918.084106   3538.688507   \n",
       "  307  3531.601921   789.219238  3521.984444   921.095459   3521.636696   \n",
       "  308  3534.537300   784.047852  3534.200134   915.578094   3517.928680   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1317.654114  2294.839691  1390.222168   2160.418701   1432.286072  \n",
       "  1     1325.043701  2287.862183  1396.473328   2152.836327   1440.923523  \n",
       "  2     1326.668762  2276.834137  1410.177795   2210.354187   1432.877747  \n",
       "  3     1319.942566  2270.260590  1425.041748   2228.012741   1438.056641  \n",
       "  4     1312.532959  2257.217255  1440.761230   2221.089737   1452.990723  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  304    932.577972  3500.504292  1064.225281   3518.546234   1065.380188  \n",
       "  305    927.105927  3495.546082  1062.439270   3528.843018   1068.871826  \n",
       "  306    921.813354  3499.576580  1051.544556   3521.474419   1057.364502  \n",
       "  307    921.397614  3511.409073  1054.067078   3510.452358   1054.647949  \n",
       "  308    913.989807  3520.220718  1044.141296   3512.609722   1044.684082  \n",
       "  \n",
       "  [309 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2185.471161  804.083359  2201.588104  785.730148  2171.919525   \n",
       "  1    2272.911514  778.713516  2258.798889  765.696594  2267.520523   \n",
       "  2    2280.825348  781.690903  2270.170898  767.522690  2274.128693   \n",
       "  3    2301.237640  791.537109  2296.619858  775.997551  2295.657745   \n",
       "  4    2283.098862  815.593979  2281.911140  798.220657  2275.874657   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  312  3509.278885  523.238800  3508.530220  513.396046  3510.084808   \n",
       "  313  3543.834427  532.123974  3545.000000  522.343468  3540.385017   \n",
       "  314  3543.000000  529.735977  3543.000000  520.009258  3543.000000   \n",
       "  315  3537.090805  536.161415  3537.405327  525.079006  3537.085594   \n",
       "  316  3529.000000  551.626587  3529.000000  541.143394  3529.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     787.829422  2235.828842  780.691498  2160.956596   783.659546  ...   \n",
       "  1     766.455765  2175.414368  777.779358  2237.551682   787.174240  ...   \n",
       "  2     769.684464  2210.627312  767.201950  2220.885788   777.386230  ...   \n",
       "  3     776.968353  2221.968719  776.856918  2245.861031   784.006256  ...   \n",
       "  4     798.453415  2255.841049  791.978600  2224.242393   784.526146  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  312   512.576641  3502.986687  518.566944  3546.706497   523.033695  ...   \n",
       "  313   521.484924  3530.403076  524.368828  3527.248047   528.259018  ...   \n",
       "  314   518.738987  3526.011414  522.336349  3534.794342   523.146168  ...   \n",
       "  315   524.421474  3522.569801  526.399963  3534.955544   527.999748  ...   \n",
       "  316   542.198378  3529.000000  539.819784  3523.887268   534.609251  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2183.583206  1131.289307  2295.271469  1208.134033   2236.203705   \n",
       "  1    2202.512756  1146.503845  2295.446640  1225.330078   2247.087555   \n",
       "  2    2196.379242  1151.424652  2296.611588  1217.550171   2245.617409   \n",
       "  3    2190.713892  1163.403839  2286.127388  1228.652405   2217.816658   \n",
       "  4    2197.139683  1129.272888  2288.406448  1237.183472   2246.158600   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  312  3535.718109   775.644928  3483.569443   918.620789   3515.377357   \n",
       "  313  3506.787117   777.168640  3504.290352   909.765198   3493.243011   \n",
       "  314  3516.461754   773.427521  3498.116535   909.667450   3505.057297   \n",
       "  315  3518.570602   790.879028  3491.820034   908.206696   3513.655899   \n",
       "  316  3515.583466   774.046814  3494.352043   893.658539   3508.998543   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1264.878784  2383.435852  1347.741943   2328.370773   1370.430603  \n",
       "  1     1283.096619  2373.367676  1367.222473   2336.358643   1385.398926  \n",
       "  2     1279.634888  2350.736023  1372.328369   2322.639038   1394.417847  \n",
       "  3     1325.640808  2342.414536  1388.698486   2290.682755   1410.162537  \n",
       "  4     1280.647827  2329.133331  1397.899841   2293.049873   1412.190918  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  312    919.365356  3478.869507  1053.650269   3498.651920   1055.649902  \n",
       "  313    909.528595  3488.548325  1049.002747   3480.801220   1052.199219  \n",
       "  314    911.218658  3482.078728  1052.423462   3487.477722   1055.555176  \n",
       "  315    922.786407  3468.125183  1035.213745   3487.674343   1057.423462  \n",
       "  316    905.290802  3464.669312  1022.619720   3485.212303   1040.200012  \n",
       "  \n",
       "  [317 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1697.459900  1095.105652  1687.093719  1072.289047  1694.217346   \n",
       "  1    1693.914124  1103.713318  1683.854370  1080.480499  1690.470642   \n",
       "  2    1676.717682  1102.298325  1673.204315  1081.223450  1680.478149   \n",
       "  3    1680.399384  1114.011581  1675.169067  1092.378876  1681.064148   \n",
       "  4    1669.119659  1101.506973  1666.427368  1082.791855  1671.061401   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  236   341.737879   759.658541   339.516335   751.285785   339.647601   \n",
       "  237   335.684533   764.990173   334.761361   755.990166   334.381402   \n",
       "  238   346.283202   816.203011   347.471530   811.008419   344.898346   \n",
       "  239   366.534844   822.885551   367.063602   816.783402   365.021917   \n",
       "  240   368.298020   845.973623   370.079410   842.549451   367.514225   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1078.439056  1714.149597  1039.057343  1769.861237  1041.610397  ...   \n",
       "  1    1085.811142  1713.945404  1046.403824  1767.546112  1053.669601  ...   \n",
       "  2    1081.663193  1705.948608  1051.208168  1756.866577  1060.350952  ...   \n",
       "  3    1099.121780  1702.798401  1058.132767  1740.936066  1068.078339  ...   \n",
       "  4    1084.603714  1701.525818  1069.198532  1739.628540  1063.872726  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  236   751.273738   325.368988   757.772810   349.942879   757.367809  ...   \n",
       "  237   755.725067   327.944268   761.153664   340.956251   760.494831  ...   \n",
       "  238   810.234730   347.840268   808.780933   355.258311   801.392712  ...   \n",
       "  239   816.345982   363.012243   815.915126   362.354540   812.240982  ...   \n",
       "  240   841.789729   372.026533   841.598226   366.255150   836.484787  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1759.514954  1448.202637  1642.305695  1531.763733   1680.165710   \n",
       "  1    1762.149841  1481.854187  1651.171600  1570.922424   1704.935577   \n",
       "  2    1742.360992  1501.374207  1620.129303  1568.942322   1643.072205   \n",
       "  3    1719.491241  1474.703186  1645.501251  1579.592957   1655.440399   \n",
       "  4    1712.468933  1480.463379  1661.095459  1581.329163   1670.273071   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  236   372.770226  1043.543854   347.706575  1162.007782    351.050463   \n",
       "  237   354.425842  1034.874451   336.354951  1135.158173    344.048365   \n",
       "  238   357.139699  1035.519409   353.596473  1114.040375    355.234632   \n",
       "  239   356.000000  1042.181686   366.103150  1060.186371    367.762457   \n",
       "  240   371.253330  1028.251236   389.867920  1042.696106    382.555805   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1510.432373  1665.754578  1546.617920   1669.669800   1521.593323  \n",
       "  1     1557.742371  1662.239227  1590.062012   1681.232086   1565.817383  \n",
       "  2     1542.146973  1629.653809  1545.184082   1649.112335   1533.624390  \n",
       "  3     1560.226501  1645.324158  1546.491394   1628.880615   1524.595886  \n",
       "  4     1550.415344  1620.836304  1546.351440   1608.086609   1520.154297  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  236   1154.748260   400.434456  1270.613037    387.924255   1265.091431  \n",
       "  237   1137.356903   351.038731  1154.316162    347.650927   1163.437042  \n",
       "  238   1113.723663   376.927425  1103.154022    376.360516   1096.180084  \n",
       "  239   1058.647964   385.486616  1063.935181    388.157406   1058.058716  \n",
       "  240   1034.895508   410.327991  1056.952179    409.641018   1050.786346  \n",
       "  \n",
       "  [241 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1695.872833  1032.132599  1690.897797  1013.259644  1691.193726   \n",
       "  1    1695.189545  1043.319244  1685.767303  1021.631302  1686.978851   \n",
       "  2    1691.368530  1047.861542  1681.390137  1026.634842  1684.752258   \n",
       "  3    1687.126495  1048.589340  1677.528625  1026.626251  1679.914093   \n",
       "  4    1680.075134  1042.027252  1673.815720  1021.023285  1675.898911   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  248   346.972946   727.811939   343.342571   718.183842   346.888828   \n",
       "  249   343.087296   715.614811   338.736420   706.541647   342.947594   \n",
       "  250   335.138977   723.356658   329.039520   711.451308   333.928993   \n",
       "  251   326.510675   783.941303   328.955142   778.704620   325.000000   \n",
       "  252   340.851275   791.465370   344.379871   787.093580   338.756332   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1016.033905  1728.601807  988.705612  1732.033691   994.616806  ...   \n",
       "  1    1026.589417  1717.959534  987.188499  1749.040344   986.200844  ...   \n",
       "  2    1031.846695  1713.702484  988.923889  1729.149719   988.620377  ...   \n",
       "  3    1030.349792  1710.792236  995.621590  1771.492462   979.399353  ...   \n",
       "  4    1024.209259  1717.216461  996.193039  1761.445038   990.802856  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  248   718.626823   318.181372  722.815029   356.354519   724.973698  ...   \n",
       "  249   706.826916   311.633336  713.080915   348.392246   713.021219  ...   \n",
       "  250   711.912858   309.271003  718.375117   337.811474   716.856382  ...   \n",
       "  251   777.724689   332.966709  776.254787   336.693709   768.913776  ...   \n",
       "  252   786.280087   349.630784  786.800491   341.995367   782.412546  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1692.696442  1385.420532  1703.983429  1465.270325   1686.083252   \n",
       "  1    1766.375061  1381.695923  1764.144745  1570.054077   1747.471436   \n",
       "  2    1749.026733  1405.228210  1738.504242  1532.833435   1715.033875   \n",
       "  3    1804.946594  1397.247742  1714.254791  1603.067810   1773.430298   \n",
       "  4    1782.632935  1385.064697  1641.569778  1247.867737   1649.767105   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  248   375.473244  1012.260620   343.433266  1161.027771    364.097725   \n",
       "  249   370.446701  1011.247070   334.618998  1148.785889    349.758682   \n",
       "  250   361.394554  1013.963196   320.380997  1135.000702    327.393745   \n",
       "  251   332.190237  1008.618683   331.844355  1073.468201    330.064849   \n",
       "  252   337.000000  1009.590378   366.633299   998.155609    351.046361   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1457.370239  1682.629822  1471.002808   1666.936523   1457.179626  \n",
       "  1     1558.352173  1749.199280  1572.579346   1706.699554   1551.931763  \n",
       "  2     1522.371155  1726.543060  1523.474060   1694.179504   1512.629883  \n",
       "  3     1588.847046  1705.293488  1619.072632   1745.813324   1592.563110  \n",
       "  4     1231.447205  1629.155212  1425.711914   1618.550613   1369.642578  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  248   1152.761169   433.515884  1260.496155    434.944260   1251.518494  \n",
       "  249   1140.157867   387.890556  1277.376038    403.630623   1256.041443  \n",
       "  250   1128.534332   366.345375  1251.858154    361.590496   1244.205200  \n",
       "  251   1069.562469   356.605839  1061.315979    356.159845   1052.181183  \n",
       "  252    992.889938   389.389126  1031.130432    383.453545   1028.380936  \n",
       "  \n",
       "  [253 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1735.395508  1045.918198  1732.208130  1027.920166  1731.367676   \n",
       "  1    1734.293793  1051.160843  1727.950439  1031.063889  1729.821808   \n",
       "  2    1729.309814  1048.319183  1724.606323  1029.979935  1724.518738   \n",
       "  3    1724.034607  1049.530869  1721.735931  1030.691238  1722.553528   \n",
       "  4    1727.521362  1044.838852  1724.047516  1024.261871  1723.300293   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  234   357.902188   811.318172   354.528022   802.633750   357.920503   \n",
       "  235   364.425051   823.000000   360.860687   823.000000   365.430950   \n",
       "  236   364.484680   838.684178   361.599598   837.000000   365.571190   \n",
       "  237   357.516756   859.170187   356.542795   848.664289   357.675691   \n",
       "  238   361.193739  1184.198235   363.198314  1180.744934   359.539036   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1029.184906  1781.032043   999.271713  1784.538177  1002.173126  ...   \n",
       "  1    1033.702652  1774.054901  1002.268082  1812.372040   996.209717  ...   \n",
       "  2    1031.926270  1771.039764  1002.800827  1796.265320   997.088165  ...   \n",
       "  3    1032.548935  1765.047943   995.270737  1790.417084  1002.442581  ...   \n",
       "  4    1027.321533  1776.865540   998.381454  1795.377289   999.490326  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  234   801.972678   347.678838   802.947477   361.126749   804.201385  ...   \n",
       "  235   823.000000   348.000000   823.000000   357.772796   823.000000  ...   \n",
       "  236   837.000000   350.000000   837.000000   358.160078   837.000000  ...   \n",
       "  237   847.448901   346.868129   854.700286   358.863353   842.000000  ...   \n",
       "  238  1180.300156   359.383098  1180.436134   359.881049  1171.465668  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1746.886292  1376.408020  1741.279785  1490.971130   1735.852814   \n",
       "  1    1782.194000  1335.868286  1623.876724  1416.570496   1658.455521   \n",
       "  2    1781.338562  1376.759155  1715.739960  1439.981445   1718.693054   \n",
       "  3    1746.833252  1372.485596  1728.925690  1461.777954   1713.727386   \n",
       "  4    1775.964996  1373.042664  1660.120041  1428.358337   1679.043167   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  234   378.503895  1047.078293   377.975445  1190.278442    387.044163   \n",
       "  235   371.878466  1043.766907   375.131290  1190.995453    382.045746   \n",
       "  236   365.279471  1050.761154   369.471516  1189.034515    373.492414   \n",
       "  237   351.143066  1045.182617   362.081734  1182.807983    362.884506   \n",
       "  238   353.000000  1380.242249   369.863726  1378.504059    368.470220   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1479.078430  1728.953674  1484.782288   1708.651001   1470.441956  \n",
       "  1     1365.107697  1635.023026  1447.522461   1655.955032   1416.940369  \n",
       "  2     1389.222046  1707.477692  1466.536865   1699.079742   1425.950684  \n",
       "  3     1437.982971  1729.166748  1441.446838   1705.391678   1438.254028  \n",
       "  4     1335.169525  1720.335815  1474.610474   1714.995758   1416.936951  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  234   1187.912933   407.049187  1331.711182    406.777374   1329.384338  \n",
       "  235   1188.812073   406.079849  1332.676361    405.462788   1330.578003  \n",
       "  236   1187.675018   405.977592  1327.855530    404.638428   1325.973785  \n",
       "  237   1183.106354   400.597260  1322.425293    399.452362   1320.977997  \n",
       "  238   1375.060211   355.295900  1400.937256    365.936812   1395.171326  \n",
       "  \n",
       "  [239 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1392.663727  927.210648  1391.923645  907.590378  1388.440536   \n",
       "  1    1375.225082  929.240707  1376.712357  909.059349  1370.822464   \n",
       "  2    1357.731812  932.545990  1358.499298  912.157082  1352.944534   \n",
       "  3    1335.793365  933.940994  1337.227005  913.643173  1331.757004   \n",
       "  4    1320.513947  931.683670  1323.332901  911.157715  1316.880295   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  218   140.197666  677.953190   140.295533  668.962696   139.158596   \n",
       "  219   140.514012  670.942530   141.629642  662.343066   138.277930   \n",
       "  220   147.303409  731.100573   146.035789  722.435417   146.445353   \n",
       "  221   170.735170  738.168409   170.384747  729.820889   171.196904   \n",
       "  222   172.000000  776.889515   172.000000  766.749456   172.000000   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     911.137596  1442.917419  887.968163  1418.492004   897.588120  ...   \n",
       "  1     912.060127  1430.778290  892.063293  1401.296204   900.092934  ...   \n",
       "  2     915.172920  1411.921906  894.062309  1387.935883   902.085213  ...   \n",
       "  3     916.462418  1390.317291  898.651100  1372.924789   902.619850  ...   \n",
       "  4     912.708191  1378.149765  899.239937  1359.430817   900.321907  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  218   669.281496   135.174534  674.890877   156.375099   672.562162  ...   \n",
       "  219   662.387142   138.602328  667.863413   147.775002   666.762094  ...   \n",
       "  220   721.750854   144.466431  720.752518   157.033465   719.426161  ...   \n",
       "  221   728.678753   163.157372  731.432403   173.682879   728.425216  ...   \n",
       "  222   765.499084   172.000000  773.474977   175.444067   755.288234  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1365.886642  1116.711243  1212.077675  1123.671814   1226.433403   \n",
       "  1    1366.126724  1118.317200  1287.643127  1115.712524   1288.143875   \n",
       "  2    1365.283813  1119.822723  1275.583130  1120.417999   1281.675537   \n",
       "  3    1353.635773  1120.180298  1257.006668  1132.368195   1255.571259   \n",
       "  4    1352.649094  1130.789093  1257.966873  1138.432068   1249.883095   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  218   202.747116   951.015991   166.716915  1077.798645    208.739609   \n",
       "  219   182.392624   957.057831   194.459827  1082.833313    200.569855   \n",
       "  220   179.775471   951.153152   188.414261  1078.945526    194.691357   \n",
       "  221   176.363989   955.640854   186.811039  1082.181580    189.546967   \n",
       "  222   172.000000   930.842285   175.952348  1069.970215    172.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1104.118134  1251.844765  1109.115601   1250.389046   1105.736450  \n",
       "  1     1098.615570  1246.735802  1107.981812   1237.901596   1105.107147  \n",
       "  2     1100.645844  1228.234085  1113.549225   1219.306252   1109.675476  \n",
       "  3     1114.662323  1264.655792  1108.311646   1257.770744   1115.240967  \n",
       "  4     1123.556458  1258.515686  1112.540497   1260.862869   1118.432068  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  218   1075.463776   208.980972  1223.726379    228.960770   1219.973389  \n",
       "  219   1080.940796   230.628708  1225.640930    227.725601   1221.907104  \n",
       "  220   1077.701935   231.278275  1218.450562    228.662994   1215.618958  \n",
       "  221   1082.717651   225.297707  1220.947998    222.985111   1220.139923  \n",
       "  222   1072.414795   218.751091  1201.681091    212.745148   1201.036407  \n",
       "  \n",
       "  [223 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x       nose-y   left-eye-x   left-eye-y  right-eye-x  \\\n",
       "  0    1472.535400  1065.297150  1467.694885  1048.467468  1473.114136   \n",
       "  1    1456.668488  1073.558105  1452.429382  1053.814163  1456.196533   \n",
       "  2    1443.402924  1076.935257  1440.844879  1057.365814  1438.270782   \n",
       "  3    1425.502350  1080.023972  1421.551071  1058.535065  1420.965302   \n",
       "  4    1404.018555  1081.072586  1398.982056  1060.101761  1400.253571   \n",
       "  ..           ...          ...          ...          ...          ...   \n",
       "  251   174.567366   668.619171   191.852077   656.587959   166.057566   \n",
       "  252   178.468254   667.748165   196.338833   656.953247   169.556507   \n",
       "  253   180.291843   667.575115   196.987587   656.206806   170.628162   \n",
       "  254   183.768127   666.560619   202.072815   655.562878   173.485794   \n",
       "  255   185.268841   665.581284   203.398499   654.760971   174.608175   \n",
       "  \n",
       "       right-eye-y   left-ear-x   left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    1052.192459  1508.453217  1025.751503  1499.671997  1028.980942  ...   \n",
       "  1    1061.215378  1493.784851  1026.510300  1483.701202  1039.405579  ...   \n",
       "  2    1063.536316  1483.081573  1027.105179  1465.776672  1041.375137  ...   \n",
       "  3    1064.667679  1460.207214  1028.448685  1456.128693  1041.502281  ...   \n",
       "  4    1064.671371  1448.201752  1035.447113  1442.147583  1043.574387  ...   \n",
       "  ..           ...          ...          ...          ...          ...  ...   \n",
       "  251   656.057732   223.084358   669.017380   156.877093   666.363014  ...   \n",
       "  252   655.287603   225.108597   672.621861   157.677132   666.184334  ...   \n",
       "  253   655.452950   225.842697   668.060493   158.750292   664.882454  ...   \n",
       "  254   652.969090   228.420288   671.520859   159.139173   661.984646  ...   \n",
       "  255   651.920284   228.500298   671.290314   159.775989   661.932270  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1410.199860  1242.497375  1277.771851  1260.093811   1282.420708   \n",
       "  1    1419.754639  1248.130005  1253.653427  1268.111359   1281.900658   \n",
       "  2    1406.308487  1257.413879  1316.794861  1260.054047   1323.935349   \n",
       "  3    1400.916870  1260.805389  1270.993645  1265.195709   1287.398468   \n",
       "  4    1395.158447  1263.183685  1325.669327  1253.773834   1323.846390   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  251   174.578327   932.346283   219.853905  1054.220093    173.595253   \n",
       "  252   159.019845   942.383087   226.248466  1073.455719    157.765895   \n",
       "  253   157.697875   943.655823   229.047417  1077.652374    156.603213   \n",
       "  254   158.159041   937.615356   234.034592  1071.633087    153.711504   \n",
       "  255   157.345932   940.121002   238.575592  1073.776123    151.000000   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1243.950012  1300.095001  1236.082977   1287.688408   1233.074585  \n",
       "  1     1233.197205  1278.970764  1246.439178   1260.372536   1245.340576  \n",
       "  2     1234.296234  1308.875061  1255.797241   1287.066010   1250.532990  \n",
       "  3     1246.235474  1302.880707  1256.577606   1298.325287   1254.887817  \n",
       "  4     1244.193878  1280.590508  1256.528656   1250.982872   1265.282471  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  251   1051.191223   227.882149  1167.105530    201.532021   1153.414673  \n",
       "  252   1073.024048   224.685707  1182.426147    162.653095   1174.886169  \n",
       "  253   1075.501038   224.195953  1192.371582    161.571907   1184.750732  \n",
       "  254   1069.445404   229.944717  1180.155884    157.138751   1172.696777  \n",
       "  255   1072.247711   225.756866  1184.708618    153.511273   1179.132080  \n",
       "  \n",
       "  [256 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1519.992126  836.557373  1519.451599  819.340652  1522.816589   \n",
       "  1    1508.988831  847.523956  1508.003143  825.826614  1511.466492   \n",
       "  2    1496.505737  844.774063  1495.713867  825.079163  1493.529907   \n",
       "  3    1483.081207  842.324951  1482.088013  822.335144  1480.896729   \n",
       "  4    1466.581024  847.747269  1463.951141  825.819946  1465.188385   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  224   210.696566  467.669014   209.258240  459.371883   209.989697   \n",
       "  225   202.456961  468.501625   202.318097  460.383427   201.407505   \n",
       "  226   196.838780  468.681553   198.756922  459.915882   196.000000   \n",
       "  227   214.129019  508.044594   212.940913  499.294050   213.054852   \n",
       "  228   216.289368  511.606055   216.027470  501.838942   215.178999   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     821.204361  1559.621796  809.601288  1556.007202   806.222046  ...   \n",
       "  1     831.626526  1557.081543  806.745514  1544.700684   811.664764  ...   \n",
       "  2     831.114456  1543.826202  801.046906  1525.153717   810.292679  ...   \n",
       "  3     827.170212  1533.907837  801.779869  1522.507019   806.036583  ...   \n",
       "  4     831.021698  1509.202606  801.412506  1508.517609   808.178497  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  224   459.214054   194.931605  467.600498   223.380564   467.355335  ...   \n",
       "  225   460.178436   195.000000  469.571106   213.975496   469.376972  ...   \n",
       "  226   459.964283   197.298004  466.639709   213.400814   468.027767  ...   \n",
       "  227   499.339321   204.931737  497.590084   222.460108   502.452360  ...   \n",
       "  228   501.621106   207.970696  501.274822   220.966131   502.466550  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1451.400330  1001.500854  1345.439636  1003.146881   1352.007240   \n",
       "  1    1453.808884  1000.023071  1375.508377   993.347565   1379.844208   \n",
       "  2    1440.898621  1015.262421  1313.883553  1022.282257   1338.833267   \n",
       "  3    1439.884491  1022.652618  1338.660156  1022.245178   1350.328049   \n",
       "  4    1427.711624  1023.893951  1330.455780  1017.986511   1341.211349   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  224   270.305679   742.943726   228.732460   876.890656    291.848633   \n",
       "  225   259.717583   735.265839   237.005016   865.855804    278.333359   \n",
       "  226   249.071880   741.859802   245.830154   867.894836    272.657280   \n",
       "  227   239.987942   746.428345   251.499252   865.183319    259.440521   \n",
       "  228   223.682896   741.351135   235.778873   862.147736    241.661316   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      990.776489  1374.493454   989.719482   1350.387360    994.920166  \n",
       "  1      986.408813  1371.900162   992.928986   1342.819870    998.192535  \n",
       "  2     1000.294952  1341.864090  1007.044098   1335.718277   1010.150299  \n",
       "  3     1008.683136  1335.109436  1015.068176   1327.057465   1020.327545  \n",
       "  4     1010.007568  1339.273514  1012.902039   1319.322052   1017.470886  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  224    868.063293   281.810593  1003.730713    329.669403    993.530701  \n",
       "  225    859.302856   303.079666   992.410706    322.556389    984.370605  \n",
       "  226    862.522064   305.995544   978.438904    312.557671    972.590271  \n",
       "  227    860.593323   301.403107   968.297882    300.119865    963.601318  \n",
       "  228    858.061279   285.073532   966.849884    287.016052    962.309479  \n",
       "  \n",
       "  [229 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2194.564598  886.368065  2187.364304  866.680145  2191.306198   \n",
       "  1    2245.748856  885.223557  2236.986893  866.601501  2241.678726   \n",
       "  2    2215.156952  919.856247  2212.623108  902.083847  2217.433929   \n",
       "  3    2245.422195  919.479599  2241.562958  899.716873  2244.803436   \n",
       "  4    2295.311050  911.949890  2301.485504  890.634232  2291.197296   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  170  3275.626934  656.482685  3288.545303  646.954666  3271.101360   \n",
       "  171  3252.916458  660.349113  3265.541061  651.402874  3247.674919   \n",
       "  172  3241.071541  662.897110  3253.568764  654.104805  3236.542824   \n",
       "  173  3222.037712  665.008781  3235.786316  656.012520  3217.745338   \n",
       "  174  3195.849213  660.808640  3208.394760  651.332436  3189.977013   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     868.924828  2132.826626  858.059494  2133.618271   857.529854  ...   \n",
       "  1     866.503952  2171.845634  864.350410  2147.155693   858.175888  ...   \n",
       "  2     898.710876  2166.939934  866.604355  2166.842216   859.968979  ...   \n",
       "  3     898.803406  2203.925941  876.548035  2202.123077   862.176765  ...   \n",
       "  4     886.589279  2281.138306  871.168457  2239.221039   852.250885  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  170   646.336010  3313.977814  656.697319  3266.172302   652.792057  ...   \n",
       "  171   650.777164  3289.877556  661.640175  3241.357521   658.855347  ...   \n",
       "  172   653.223671  3277.535767  664.517105  3230.303711   661.236923  ...   \n",
       "  173   654.384281  3261.828735  666.768814  3210.976601   660.342819  ...   \n",
       "  174   650.612663  3233.847839  659.406219  3182.459290   657.276592  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2140.712219  1211.052399  2237.093887  1326.706970   2205.046677   \n",
       "  1    2136.188606  1203.105652  2246.558167  1314.071594   2197.566971   \n",
       "  2    2179.806641  1204.389343  2229.421951  1338.283386   2224.502090   \n",
       "  3    2178.048027  1215.163147  2228.219666  1352.375427   2231.571884   \n",
       "  4    2178.690369  1185.803864  2229.442413  1346.107605   2237.708817   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  170  3275.008251   863.437897  3329.052780   972.291473   3234.395050   \n",
       "  171  3248.517426   863.281342  3318.118454   969.120728   3207.163193   \n",
       "  172  3238.161522   866.944550  3308.929428   969.693512   3200.963577   \n",
       "  173  3216.505386   872.895782  3278.944183   967.675446   3177.882839   \n",
       "  174  3173.277855   871.052216  3228.729584   985.378296   3153.142982   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1356.104980  2295.918045  1476.852905   2272.247375   1494.892761  \n",
       "  1     1374.604980  2308.160873  1490.453308   2282.178314   1507.941956  \n",
       "  2     1348.588196  2304.951233  1509.841797   2287.315887   1517.616577  \n",
       "  3     1385.645081  2320.180679  1532.832031   2302.936066   1545.929626  \n",
       "  4     1359.863525  2298.678558  1571.783325   2303.659515   1582.685181  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  170    959.479034  3360.350418  1064.199860   3265.327656   1046.680573  \n",
       "  171    957.569885  3355.480682  1060.635223   3179.233881   1058.605103  \n",
       "  172    958.333832  3350.102768  1059.187195   3167.886749   1060.195374  \n",
       "  173    963.041809  3343.566681  1056.522125   3159.537910   1067.309235  \n",
       "  174    971.701874  3301.866821  1043.127228   3158.303558   1074.719543  \n",
       "  \n",
       "  [175 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2130.121811  870.829010  2134.115875  851.328262  2125.870987   \n",
       "  1    2157.107162  871.631989  2162.689728  852.152557  2150.963196   \n",
       "  2    2184.821564  874.591354  2188.721863  851.803238  2178.145081   \n",
       "  3    2217.768158  870.467072  2226.238098  852.037949  2215.166931   \n",
       "  4    2257.268524  858.410263  2266.099487  837.155640  2256.561096   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  208  3479.371590  405.354279  3486.243294  397.007557  3481.319313   \n",
       "  209  3487.819683  401.116375  3495.561028  392.202785  3489.725666   \n",
       "  210  3495.631641  400.086685  3503.632805  390.969540  3497.798016   \n",
       "  211  3503.210678  398.141651  3513.205212  389.795687  3503.191839   \n",
       "  212  3523.622520  395.715357  3536.031498  386.579000  3518.709946   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     847.466995  2116.706299  835.425461  2064.759842   811.285995  ...   \n",
       "  1     849.032425  2147.025497  833.210663  2086.548088   816.368309  ...   \n",
       "  2     850.066742  2159.010788  835.340469  2118.946732   818.273865  ...   \n",
       "  3     843.486145  2169.578827  833.650650  2147.377655   819.198883  ...   \n",
       "  4     826.842468  2186.334885  818.575638  2187.063873   798.189346  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  208   396.742970  3514.433105  402.062050  3528.641922   396.714409  ...   \n",
       "  209   392.050465  3525.319824  398.182781  3537.447411   393.275585  ...   \n",
       "  210   390.740318  3536.266884  395.688332  3536.744766   389.382616  ...   \n",
       "  211   389.724817  3546.413124  392.676956  3520.509941   388.814833  ...   \n",
       "  212   387.169640  3564.983986  390.205338  3519.288055   388.879242  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2042.837040  1192.110443  2162.462906  1295.869568   2133.220856   \n",
       "  1    2065.136909  1170.387665  2165.273773  1305.830933   2127.175079   \n",
       "  2    2074.381371  1167.947784  2151.473572  1291.615845   2120.834007   \n",
       "  3    2038.772373  1158.319641  2163.147018  1306.806335   2102.260681   \n",
       "  4    2027.188656  1159.644043  2160.702423  1298.721924   2114.614136   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  208  3555.864563   607.949463  3486.570023   711.694672   3543.459785   \n",
       "  209  3562.605103   599.625854  3497.693096   699.147522   3546.881401   \n",
       "  210  3551.519035   595.648239  3526.638741   704.312439   3528.140694   \n",
       "  211  3545.033249   600.417694  3537.769951   716.923248   3547.876038   \n",
       "  212  3549.131237   599.229248  3568.438210   708.716400   3551.197220   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1332.609070  2230.297791  1537.316589   2203.856461   1544.140747  \n",
       "  1     1339.925293  2232.721985  1552.689819   2183.852722   1565.205444  \n",
       "  2     1324.317932  2234.024521  1564.638794   2143.470459   1597.496155  \n",
       "  3     1345.883545  2227.404739  1575.016296   2114.312836   1634.435364  \n",
       "  4     1335.890076  2216.913193  1582.929321   2115.351250   1650.820740  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  208    713.535309  3513.302353   808.264618   3545.681732    809.094635  \n",
       "  209    702.227722  3545.182915   787.662903   3555.219917    791.739319  \n",
       "  210    699.534607  3556.050262   784.357574   3541.068565    779.359741  \n",
       "  211    713.858734  3540.258232   791.877563   3549.905167    786.065094  \n",
       "  212    704.018585  3560.283096   795.322083   3545.203079    792.233459  \n",
       "  \n",
       "  [213 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2106.662384  965.874634  2104.393982  946.190506  2103.551514   \n",
       "  1    2140.414261  969.124069  2141.337311  949.343201  2139.029877   \n",
       "  2    2165.101212  970.643555  2176.452469  953.263840  2165.231125   \n",
       "  3    2205.137329  961.773071  2216.427261  943.116913  2208.410248   \n",
       "  4    2226.188446  957.947418  2234.733582  936.308670  2222.054031   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  206  3457.074913  446.960236  3467.491730  436.438091  3454.121391   \n",
       "  207  3464.733734  443.820827  3475.105118  433.391266  3462.641830   \n",
       "  208  3478.232826  437.070747  3492.175446  426.139019  3476.468739   \n",
       "  209  3501.015564  434.669178  3511.940369  422.282225  3496.555626   \n",
       "  210  3514.783859  429.005184  3524.208862  418.789057  3508.961182   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     943.189911  2076.718773  933.301025  2039.317238   896.736801  ...   \n",
       "  1     945.369110  2111.475037  929.980499  2079.728561   898.052223  ...   \n",
       "  2     942.180634  2139.626129  926.672394  2098.376984   899.475655  ...   \n",
       "  3     933.155106  2140.294296  912.261795  2146.679489   899.801109  ...   \n",
       "  4     931.675690  2183.182755  910.202637  2159.124878   891.893990  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  206   437.767265  3499.488297  436.874287  3462.963318   437.612152  ...   \n",
       "  207   435.210445  3506.166412  436.369652  3471.902679   438.326694  ...   \n",
       "  208   425.603241  3522.205437  432.415833  3478.693794   427.850319  ...   \n",
       "  209   422.979156  3528.000000  425.401049  3495.661766   426.696907  ...   \n",
       "  210   418.489454  3525.000000  423.378473  3502.299461   422.340282  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2016.664810  1298.558044  2161.535141  1388.529114   2093.023712   \n",
       "  1    2045.088608  1274.450867  2168.616974  1417.431335   2115.608841   \n",
       "  2    2011.708244  1235.239227  2154.911392  1412.342529   2079.256767   \n",
       "  3    2019.029739  1252.974121  2150.217010  1392.454224   2095.008995   \n",
       "  4    2019.517216  1245.881714  2142.184860  1380.227478   2117.681259   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  206  3490.631828   640.737305  3440.038246   740.864655   3472.576706   \n",
       "  207  3491.932106   636.776535  3474.471085   742.251678   3463.210327   \n",
       "  208  3496.574196   645.484879  3490.650513   754.300903   3493.214767   \n",
       "  209  3513.123093   656.626587  3521.325699   742.855591   3514.731964   \n",
       "  210  3516.091743   634.242416  3513.024429   721.956696   3507.793404   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1439.032227  2223.389679  1614.391785   2126.410431   1642.348511  \n",
       "  1     1447.838684  2212.357803  1647.365295   2152.821030   1661.565857  \n",
       "  2     1450.449524  2230.216980  1664.122742   2119.117798   1705.308655  \n",
       "  3     1430.659058  2221.482056  1660.929749   2120.336609   1706.749634  \n",
       "  4     1407.997253  2214.665207  1667.837646   2152.101257   1692.674744  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  206    742.436951  3435.677841   836.458008   3475.519005    832.189331  \n",
       "  207    736.965057  3467.799721   834.902679   3475.895912    833.551300  \n",
       "  208    752.341461  3484.315506   795.222656   3494.016754    792.784119  \n",
       "  209    740.596283  3510.166656   771.286926   3507.008400    775.857788  \n",
       "  210    722.102112  3495.163338   731.030762   3487.144741    740.853546  \n",
       "  \n",
       "  [211 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2259.872360  789.331390  2261.262909  770.886406  2256.530991   \n",
       "  1    2295.378113  790.142197  2298.014069  772.105286  2292.009918   \n",
       "  2    2325.371613  772.519394  2327.072937  754.710266  2322.177917   \n",
       "  3    2354.764587  739.381653  2362.335693  721.049210  2357.537506   \n",
       "  4    2381.676697  693.772682  2386.908966  674.385910  2378.640137   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  101  3503.456970  465.974068  3495.121796  451.857609  3507.786942   \n",
       "  102  3488.987930  462.218452  3482.355072  449.322506  3494.053345   \n",
       "  103  3474.392517  466.364216  3472.069656  456.759487  3480.952682   \n",
       "  104  3480.204102  448.761303  3479.652267  438.837969  3484.984894   \n",
       "  105  3498.392540  460.076809  3496.309456  450.366581  3501.298721   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     767.776199  2189.073883  761.311279  2198.682861   760.614326  ...   \n",
       "  1     767.456024  2217.234497  758.366989  2231.313126   757.032501  ...   \n",
       "  2     750.627792  2236.406235  738.736160  2262.909164   743.976593  ...   \n",
       "  3     714.015579  2267.180756  695.600502  2289.991760   711.013901  ...   \n",
       "  4     669.191566  2293.701447  657.554039  2321.317963   668.395264  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  101   452.993206  3437.065247  457.543915  3502.549286   462.041428  ...   \n",
       "  102   450.269707  3447.741425  458.514889  3517.074768   465.379166  ...   \n",
       "  103   456.385311  3462.122986  463.601036  3526.799103   457.641388  ...   \n",
       "  104   438.664976  3473.461708  444.703398  3531.530685   442.751081  ...   \n",
       "  105   449.584463  3491.114395  455.677761  3544.450790   446.007641  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2126.974869  1161.514099  2163.828293  1311.448669   2168.516891   \n",
       "  1    2107.453796  1113.349213  2163.463196  1242.686584   2162.563309   \n",
       "  2    2086.534538  1077.769531  2151.403488  1246.461304   2157.441788   \n",
       "  3    2070.007637  1114.595734  2161.487930  1411.617981   2150.616196   \n",
       "  4    2064.175385  1068.413483  2145.585953  1319.213806   2150.258896   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  101  3489.429474   797.944214  3456.469498   954.794800   3462.841934   \n",
       "  102  3508.057602   794.061798  3477.053223   952.290771   3481.351349   \n",
       "  103  3527.169235   780.673706  3472.144638   931.418030   3514.139450   \n",
       "  104  3526.260147   766.627411  3478.378059   910.449036   3512.956238   \n",
       "  105  3530.986343   760.370422  3491.854446   915.442993   3515.365570   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1390.344482  2210.519211  1427.329468   2188.598114   1586.863525  \n",
       "  1     1317.540344  2205.759842  1411.344910   2185.802765   1499.541504  \n",
       "  2     1290.102600  2198.423553  1416.831543   2191.257126   1470.689880  \n",
       "  3     1435.339905  2197.685226  1548.923401   2188.834641   1584.661926  \n",
       "  4     1372.420166  2203.891296  1503.126648   2189.421585   1656.501831  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  101    973.727417  3455.510895  1099.385315   3402.457458   1108.622009  \n",
       "  102    961.758179  3482.088989  1091.979736   3411.735031   1088.720947  \n",
       "  103    934.750000  3473.676010  1051.050964   3483.431839   1050.733093  \n",
       "  104    913.498749  3484.122269  1038.169556   3501.556900   1047.190735  \n",
       "  105    919.524353  3490.286812  1044.785767   3501.136185   1049.373230  \n",
       "  \n",
       "  [106 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2290.263199  863.980774  2294.638763  847.653458  2288.108307   \n",
       "  1    2317.412155  854.921509  2326.974701  839.468399  2321.823822   \n",
       "  2    2337.767090  850.790924  2341.479309  833.176895  2336.099625   \n",
       "  3    2376.156738  806.009277  2383.751678  789.891876  2376.031403   \n",
       "  4    2403.555725  763.075012  2411.156555  745.785248  2401.695984   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  103  3423.142639  428.449432  3415.734238  413.936272  3429.073792   \n",
       "  104  3391.337311  429.113148  3386.523346  416.236271  3397.032425   \n",
       "  105  3399.427834  422.110508  3399.169868  412.764713  3405.414955   \n",
       "  106  3410.226814  419.510277  3411.136658  409.850630  3415.179604   \n",
       "  107  3426.454948  417.053301  3429.875481  408.027127  3429.143845   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     845.351715  2265.958847  828.849846  2260.153122   818.400459  ...   \n",
       "  1     831.603394  2270.634171  804.706795  2284.812744   795.275017  ...   \n",
       "  2     831.009247  2302.034119  813.725449  2300.259766   797.031937  ...   \n",
       "  3     785.372482  2332.397858  761.446037  2334.058167   753.969139  ...   \n",
       "  4     741.519287  2354.588013  721.822586  2355.567841   715.937027  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  103   414.982445  3368.482864  417.303783  3438.089203   423.684601  ...   \n",
       "  104   416.057632  3380.670486  422.527004  3447.489502   422.651287  ...   \n",
       "  105   411.685593  3394.359062  419.925964  3457.790268   415.830135  ...   \n",
       "  106   409.034121  3408.769028  414.675850  3457.161804   412.233723  ...   \n",
       "  107   407.112741  3431.835938  412.037050  3462.793320   411.626322  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2196.070156  1114.434875  2255.284706  1252.120911   2243.475197   \n",
       "  1    2192.390366  1104.544067  2248.228386  1260.063354   2233.637321   \n",
       "  2    2191.474106  1090.875092  2244.996490  1236.364441   2240.610138   \n",
       "  3    2173.542732  1036.274750  2217.838295  1218.433411   2235.439484   \n",
       "  4    2141.487305  1002.325623  2208.804993  1212.959351   2224.271332   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  103  3424.498383   760.553314  3385.906693   923.863831   3398.359421   \n",
       "  104  3461.883209   736.125458  3392.043060   890.056519   3446.516190   \n",
       "  105  3460.228897   732.773193  3393.986526   897.609680   3450.866516   \n",
       "  106  3451.467361   726.792145  3396.875374   889.855011   3439.421730   \n",
       "  107  3457.454811   728.072662  3403.141884   880.132568   3452.200874   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1283.008667  2292.116272  1431.194214   2267.520149   1462.304688  \n",
       "  1     1281.010620  2290.540665  1433.991760   2267.607780   1464.396973  \n",
       "  2     1264.066467  2279.934082  1425.218872   2258.281433   1454.909180  \n",
       "  3     1238.459656  2268.783966  1430.684814   2267.939331   1451.843323  \n",
       "  4     1233.460144  2261.077911  1427.484253   2265.122604   1449.985291  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  103    933.832031  3377.679092  1060.184631   3343.214737   1059.197937  \n",
       "  104    886.871277  3404.205246  1015.398682   3431.473099   1013.943481  \n",
       "  105    893.275330  3398.560997  1029.615112   3445.756546   1026.878235  \n",
       "  106    887.627930  3395.612564  1009.042664   3438.909988   1007.912292  \n",
       "  107    880.911346  3398.608231   990.220215   3446.933868    991.242859  \n",
       "  \n",
       "  [108 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2240.514732  865.716354  2239.962967  849.095428  2235.271896   \n",
       "  1    2267.706528  860.388794  2267.916519  842.487732  2262.220886   \n",
       "  2    2304.483734  854.173706  2307.268219  836.675858  2302.445221   \n",
       "  3    2337.577087  851.953522  2339.332489  833.121376  2331.496185   \n",
       "  4    2369.080307  835.895828  2369.486862  815.243156  2360.822372   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  201  3411.105453  456.590725  3424.099327  447.198025  3401.686371   \n",
       "  202  3407.834007  459.243565  3420.071396  449.430958  3396.831841   \n",
       "  203  3402.786942  463.754520  3415.649239  454.438572  3393.250458   \n",
       "  204  3404.820305  464.891388  3418.059540  455.631550  3394.321320   \n",
       "  205  3407.713257  465.023403  3420.462540  455.661972  3397.461678   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     844.392769  2206.886230  836.588470  2191.985142   819.943977  ...   \n",
       "  1     840.896233  2241.353210  833.279411  2215.261925   815.232620  ...   \n",
       "  2     832.116295  2237.089149  819.592018  2251.545219   814.133331  ...   \n",
       "  3     832.666962  2293.766144  822.714767  2277.149101   815.367172  ...   \n",
       "  4     816.775505  2304.970734  805.360741  2302.207947   803.902863  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  201   446.756378  3444.516678  458.260643  3386.890289   459.307480  ...   \n",
       "  202   449.589584  3437.785744  461.844711  3381.402473   461.979660  ...   \n",
       "  203   453.829624  3434.860184  467.026905  3379.282837   465.087700  ...   \n",
       "  204   454.713491  3436.283348  469.043365  3378.825611   466.451332  ...   \n",
       "  205   454.676517  3438.142494  467.985771  3381.992569   464.790760  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2188.506302  1223.994873  2294.676117  1303.280945   2260.260406   \n",
       "  1    2207.149788  1192.968384  2290.289459  1290.643738   2277.171097   \n",
       "  2    2219.775425  1169.320068  2275.085732  1295.791992   2274.518265   \n",
       "  3    2227.316025  1156.250946  2260.336937  1296.813416   2278.455559   \n",
       "  4    2222.982300  1144.867706  2272.674583  1285.909607   2280.202805   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  201  3379.755440   704.226532  3454.493866   817.815216   3383.395226   \n",
       "  202  3379.336716   705.244934  3446.484131   821.917206   3386.342125   \n",
       "  203  3374.043064   705.632050  3434.311699   821.991577   3383.433571   \n",
       "  204  3370.133369   711.647369  3433.391930   828.353149   3379.937622   \n",
       "  205  3369.715378   712.200806  3427.524841   833.807098   3380.327423   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1344.325928  2318.512894  1431.645752   2288.193054   1469.838135  \n",
       "  1     1337.921570  2330.949585  1449.624084   2308.811874   1481.866211  \n",
       "  2     1320.865967  2316.055710  1459.181580   2300.167130   1492.645996  \n",
       "  3     1324.325684  2308.688812  1468.823547   2308.727676   1483.768066  \n",
       "  4     1324.663147  2330.885086  1464.657288   2296.193451   1491.493530  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  201    822.711304  3451.347351   912.738495   3393.723747    931.339539  \n",
       "  202    820.464203  3438.042290   928.372437   3395.040825    929.862793  \n",
       "  203    820.459747  3427.944023   931.810608   3391.959991    933.397461  \n",
       "  204    828.727478  3417.755898   935.551758   3394.681671    937.611328  \n",
       "  205    826.549072  3403.671707   946.979858   3400.094704    937.581787  \n",
       "  \n",
       "  [206 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2086.950806  938.239372  2093.635925  923.813354  2097.005341   \n",
       "  1    2074.541870  953.740326  2079.849487  939.768066  2082.927856   \n",
       "  2    2062.810669  953.589584  2072.995911  940.595222  2076.236664   \n",
       "  3    2052.267822  948.241714  2062.036743  933.057297  2061.346039   \n",
       "  4    2045.101715  957.285843  2053.184662  941.707458  2054.472351   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  219   850.536514  670.380093   840.625217  660.084492   850.661629   \n",
       "  220   825.363358  668.723293   820.741413  660.204685   828.266731   \n",
       "  221   847.075790  660.423550   840.215721  650.723431   846.153091   \n",
       "  222   840.966789  661.113010   836.107780  649.942623   839.249626   \n",
       "  223   832.617722  649.511469   828.046440  640.392662   831.770874   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     922.856827  2141.948029  941.813950  2165.281464   922.274467  ...   \n",
       "  1     937.895401  2133.142670  954.799789  2161.565613   930.341125  ...   \n",
       "  2     932.415573  2117.038239  952.753098  2186.453156   937.711792  ...   \n",
       "  3     932.193993  2115.016479  956.039886  2135.519745   934.645119  ...   \n",
       "  4     940.471596  2105.093079  960.693588  2137.046448   940.208466  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  219   660.439461   794.974702  669.565578   853.166962   669.867245  ...   \n",
       "  220   660.047291   788.368167  664.766258   844.758209   666.891315  ...   \n",
       "  221   650.665897   788.198444  658.406631   838.984138   657.236256  ...   \n",
       "  222   649.173393   782.663204  654.020702   828.574505   652.846882  ...   \n",
       "  223   639.671082   780.426677  646.883753   824.175228   647.589197  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2175.540253  1302.582214  2023.950104  1191.726349   2033.823059   \n",
       "  1    2153.893341  1240.767853  2053.115326  1224.049744   2039.819641   \n",
       "  2    2173.144928  1296.906647  2025.341766  1195.820557   2033.295258   \n",
       "  3    2132.389099  1282.768127  2007.232300  1289.554718   2011.632599   \n",
       "  4    2145.843811  1303.823944  1991.384369  1339.084167   2014.972382   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  219   865.438026   926.621185   823.780464  1081.408081    844.340561   \n",
       "  220   843.448318   920.742065   813.060772  1082.447449    818.894897   \n",
       "  221   840.411179   915.816559   802.943062  1062.206696    827.689133   \n",
       "  222   832.094460   918.609406   802.875229  1064.655060    819.348995   \n",
       "  223   817.484543   899.014557   794.186325  1016.228027    820.579388   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1174.763977  1970.798767  1435.056824   1993.629379   1418.898987  \n",
       "  1     1176.690918  1951.602188  1471.351440   1961.156845   1430.828674  \n",
       "  2     1182.396362  1947.173401  1493.635193   1940.067810   1473.352295  \n",
       "  3     1272.450134  1931.532166  1526.595032   1931.567032   1522.190308  \n",
       "  4     1328.581329  1936.997879  1559.144409   1939.463364   1552.045532  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  219   1076.160034   923.466934  1214.058960    918.454453   1210.958130  \n",
       "  220   1082.366608   920.426193  1198.481812    836.503605   1224.935120  \n",
       "  221   1059.348572   884.030495  1182.598755    878.529732   1183.465759  \n",
       "  222   1063.986023   861.626945  1169.244934    845.819992   1177.292725  \n",
       "  223   1020.942993   808.614395  1114.838867    819.558960   1119.561005  \n",
       "  \n",
       "  [224 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1971.859375  887.785149  1983.592804  873.905739  1979.967957   \n",
       "  1    1963.099487  888.601997  1976.966217  876.296371  1974.996979   \n",
       "  2    1950.704468  892.148514  1966.028748  880.156837  1960.873871   \n",
       "  3    1939.637054  897.919189  1957.815216  886.206291  1948.696198   \n",
       "  4    1921.317688  901.037674  1938.293915  889.023674  1931.029999   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  227   652.354455  652.852047   651.902160  645.112673   654.880427   \n",
       "  228   674.817692  647.469435   669.059448  637.486295   676.119850   \n",
       "  229   706.280243  643.921810   702.158318  637.331035   705.308941   \n",
       "  230   670.726181  650.815773   671.878513  644.679506   669.494294   \n",
       "  231   685.550486  658.649067   688.497356  647.867968   683.760696   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     871.957802  2030.280640  900.370560  2044.955902   887.065681  ...   \n",
       "  1     869.185883  2017.655823  906.115082  2056.978943   890.600609  ...   \n",
       "  2     874.851540  2013.233856  902.881149  2030.822296   891.952988  ...   \n",
       "  3     880.139740  2002.851471  910.515457  2020.752777   897.719444  ...   \n",
       "  4     883.521477  1987.621307  916.492828  2010.340942   895.114105  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  227   645.528182   653.447865  650.997974   696.597340   649.482832  ...   \n",
       "  228   637.275947   653.657918  644.443913   698.503227   646.908318  ...   \n",
       "  229   636.466798   660.674396  645.009779   689.792885   645.769415  ...   \n",
       "  230   643.627571   659.148041  650.618950   668.621934   651.373024  ...   \n",
       "  231   644.982029   674.929247  646.482735   679.253325   646.267776  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2058.305847  1264.550293  1869.532318  1180.016327   1884.801483   \n",
       "  1    2042.762848  1298.122986  1868.478271  1184.722137   1885.529480   \n",
       "  2    2076.577789  1269.093048  1882.198669  1218.890442   1889.839081   \n",
       "  3    2074.828552  1287.562469  1867.566559  1254.330841   1884.214752   \n",
       "  4    2061.173126  1279.406433  1876.434235  1301.001038   1882.309448   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  227   730.972229   906.976990   660.597852  1057.048615    731.378273   \n",
       "  228   720.297188   908.490814   663.925180  1059.724365    729.847183   \n",
       "  229   702.912277   910.881226   667.500868  1065.377777    725.256279   \n",
       "  230   697.745949   907.952087   684.619524  1062.755341    717.719112   \n",
       "  231   663.704137   979.844391   704.040707  1063.844666    708.531971   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1141.250824  1784.032486  1444.614441   1792.132751   1404.426392  \n",
       "  1     1166.309326  1778.434875  1481.843811   1788.373154   1443.892517  \n",
       "  2     1202.023590  1777.635910  1506.722961   1788.479462   1496.692627  \n",
       "  3     1243.716675  1804.763626  1534.135559   1812.038513   1524.497742  \n",
       "  4     1286.885010  1797.503036  1567.648071   1791.553040   1559.681091  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  227   1060.116119   681.570412  1192.251770    745.344688   1196.515503  \n",
       "  228   1059.357300   678.023703  1196.903320    743.342514   1202.289734  \n",
       "  229   1067.411163   679.071615  1205.016785    742.707687   1210.042725  \n",
       "  230   1062.540253   727.140312  1213.011658    743.103172   1213.008179  \n",
       "  231   1065.666290   745.418861  1224.602783    741.816696   1224.438110  \n",
       "  \n",
       "  [232 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1881.067352  898.922745  1892.831696  886.966141  1890.136902   \n",
       "  1    1871.274139  893.866394  1882.921875  883.435852  1882.544891   \n",
       "  2    1858.929443  892.651733  1870.294067  879.287224  1869.801392   \n",
       "  3    1851.545471  896.326492  1862.994324  882.565239  1861.300507   \n",
       "  4    1836.871490  893.229202  1850.052063  879.815926  1847.591797   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  243   550.097816  579.540092   550.860520  567.542898   551.413496   \n",
       "  244   619.618324  562.732992   614.569855  553.218779   620.441261   \n",
       "  245   601.990963  586.981571   601.478954  575.652264   601.729576   \n",
       "  246   599.695354  574.128361   598.742172  563.355396   597.221916   \n",
       "  247   587.802902  585.829403   589.571732  576.846317   583.744469   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     882.263832  1939.419037  900.077164  1961.764160   884.735085  ...   \n",
       "  1     875.691994  1930.875061  905.185120  1964.869812   894.421982  ...   \n",
       "  2     878.077484  1918.395142  907.436768  1944.517151   887.416779  ...   \n",
       "  3     881.775360  1908.079559  909.947342  1946.119781   889.236023  ...   \n",
       "  4     877.777962  1897.216644  911.393265  1917.367432   890.670456  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  243   566.917562   557.540596  570.604248   595.014816   563.969933  ...   \n",
       "  244   552.003881   555.392756  554.661240   603.979805   556.566957  ...   \n",
       "  245   575.095085   563.596325  564.521198   586.860641   565.436127  ...   \n",
       "  246   561.299431   563.515869  563.138966   578.232834   561.496487  ...   \n",
       "  247   573.174496   570.124683  578.868656   565.166801   576.343376  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1971.530090  1219.647491  1814.583603  1136.565033   1814.935867   \n",
       "  1    1914.372070  1186.340851  1798.532104  1178.039368   1804.345459   \n",
       "  2    1974.115936  1253.906921  1791.366364  1184.370850   1805.751129   \n",
       "  3    1984.318420  1268.453888  1777.677704  1218.724518   1800.805908   \n",
       "  4    1946.307892  1252.157318  1775.039459  1258.489136   1787.891144   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  243   625.586380   828.944855   557.144064   996.420380    628.051636   \n",
       "  244   604.481659   836.227539   576.870331  1000.419189    620.763725   \n",
       "  245   586.757545   832.629425   593.636955   992.198334    612.288185   \n",
       "  246   580.191170   827.804382   598.725586   991.939667    607.341694   \n",
       "  247   563.581130   832.142548   601.755470   993.013214    606.677692   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1115.551697  1741.107269  1390.199097   1747.317963   1377.315002  \n",
       "  1     1151.979218  1725.351028  1425.602539   1718.372711   1397.291931  \n",
       "  2     1140.051941  1699.578857  1455.468262   1723.408997   1420.439026  \n",
       "  3     1190.583710  1697.428314  1487.215637   1726.524658   1475.116455  \n",
       "  4     1217.255493  1691.663406  1523.965820   1698.734726   1513.656677  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  243    994.547699   571.368454  1125.200256    648.001755   1128.173035  \n",
       "  244   1000.067291   609.438103  1126.812012    645.657104   1131.393616  \n",
       "  245    992.952576   643.104233  1134.945068    644.138031   1135.546936  \n",
       "  246    992.256348   650.853699  1139.331970    643.955177   1138.620605  \n",
       "  247    994.159180   655.445908  1144.474609    648.449287   1144.394531  \n",
       "  \n",
       "  [248 rows x 34 columns],\n",
       "  2),\n",
       " (         nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0   1669.533630  995.910538  1674.700562  983.177528  1673.518921   \n",
       "  1   1645.716309  996.039818  1651.238083  983.385117  1649.334763   \n",
       "  2   1625.289886  998.012321  1631.237518  985.940277  1630.294159   \n",
       "  3   1608.979355  993.131203  1616.073364  982.523613  1614.862854   \n",
       "  4   1589.231201  990.035164  1595.789413  979.927605  1595.291290   \n",
       "  ..          ...         ...          ...         ...          ...   \n",
       "  71   525.577423  778.032745   522.986919  767.456326   527.929085   \n",
       "  72   494.068058  780.214909   491.725782  769.370895   494.494736   \n",
       "  73   458.724160  769.989685   463.462803  760.380581   463.531870   \n",
       "  74   457.160869  776.407482   457.626072  766.652016   460.225346   \n",
       "  75   501.440430  753.127032   495.285316  744.162881   498.835213   \n",
       "  \n",
       "      right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    982.627907  1707.393036  986.177048  1729.474182   974.402359  ...   \n",
       "  1    982.085541  1687.717773  992.056091  1706.465118   978.031235  ...   \n",
       "  2    984.307632  1666.574860  994.749573  1702.987762   975.315712  ...   \n",
       "  3    980.250648  1651.837051  995.251808  1688.818115   978.846741  ...   \n",
       "  4    976.928177  1630.958435  992.147491  1672.715668   966.881561  ...   \n",
       "  ..          ...          ...         ...          ...          ...  ...   \n",
       "  71   767.665844   531.798153  772.175896   593.024490   767.205372  ...   \n",
       "  72   770.312000   496.521259  775.560303   557.534492   773.143135  ...   \n",
       "  73   761.149483   483.453009  771.055119   529.161690   767.777161  ...   \n",
       "  74   766.889183   468.822094  769.091129   523.083267   770.044949  ...   \n",
       "  75   744.270748   447.244734  752.062013   476.948376   754.270197  ...   \n",
       "  \n",
       "      right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0   1775.431091  1269.821289  1668.710556  1294.762573   1672.000092   \n",
       "  1   1747.523987  1255.644043  1691.108948  1310.702423   1656.367538   \n",
       "  2   1758.606812  1251.509094  1658.282745  1315.421448   1668.889145   \n",
       "  3   1750.451324  1242.137756  1664.911774  1313.321106   1652.157257   \n",
       "  4   1761.988037  1235.061859  1644.395218  1311.970947   1657.786819   \n",
       "  ..          ...          ...          ...          ...           ...   \n",
       "  71   634.662811  1015.925873   576.314697  1168.288849    596.092529   \n",
       "  72   598.333755  1024.288910   563.696365  1179.069550    554.242455   \n",
       "  73   555.897087  1019.400055   509.864361  1162.360382    515.816631   \n",
       "  74   544.932495  1015.929382   490.870834  1163.178833    511.777763   \n",
       "  75   490.081924  1002.641754   444.958776  1136.632080    498.896835   \n",
       "  \n",
       "      right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0    1284.073120  1621.898621  1512.167969   1630.507584   1512.327515  \n",
       "  1    1300.785919  1718.525452  1537.367493   1639.647858   1524.692627  \n",
       "  2    1320.264771  1637.790726  1522.106995   1702.923035   1514.129944  \n",
       "  3    1304.795258  1645.287048  1526.572693   1673.694672   1516.537170  \n",
       "  4    1309.759644  1642.814468  1526.177185   1681.732178   1517.213257  \n",
       "  ..           ...          ...          ...           ...           ...  \n",
       "  71   1160.021301   602.409393  1313.542297    608.582741   1301.408936  \n",
       "  72   1163.990692   617.102234  1321.044373    525.057369   1295.901184  \n",
       "  73   1149.795166   591.262909  1285.450562    545.652328   1277.719360  \n",
       "  74   1160.046234   580.041641  1265.191650    512.988808   1297.735413  \n",
       "  75   1136.087524   456.199465  1265.056213    508.187881   1266.975586  \n",
       "  \n",
       "  [76 rows x 34 columns],\n",
       "  2),\n",
       " (         nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0   1698.828400  841.297333  1710.576157  832.569817  1704.341934   \n",
       "  1   1681.066956  840.272583  1692.349945  830.991478  1689.063095   \n",
       "  2   1662.924454  845.234619  1675.596161  835.223740  1669.717850   \n",
       "  3   1647.857056  842.610847  1659.835938  832.357506  1655.216049   \n",
       "  4   1633.725388  845.393852  1645.751678  835.122932  1640.149200   \n",
       "  ..          ...         ...          ...         ...          ...   \n",
       "  85   462.959613  623.857353   470.303401  614.334743   468.281900   \n",
       "  86   455.010574  620.569824   458.363573  612.015381   455.683918   \n",
       "  87   441.108912  622.201416   442.968878  614.324516   445.320210   \n",
       "  88   512.510269  619.932285   507.588570  611.418397   507.927803   \n",
       "  89   478.234028  775.304281   479.395760  767.907234   478.998135   \n",
       "  \n",
       "      right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0    828.819641  1742.104706  852.908195  1757.583344   842.121254  ...   \n",
       "  1    829.469414  1729.695038  855.819801  1738.005432   843.183296  ...   \n",
       "  2    833.206696  1715.552216  855.688690  1708.914597   845.365593  ...   \n",
       "  3    830.848076  1698.401657  855.813087  1699.323166   847.681572  ...   \n",
       "  4    832.424088  1686.391159  856.259346  1690.704987   840.144424  ...   \n",
       "  ..          ...          ...         ...          ...          ...  ...   \n",
       "  85   614.718575   496.762505  627.298302   531.327538   623.575859  ...   \n",
       "  86   612.211777   465.914997  620.748600   494.794479   618.727039  ...   \n",
       "  87   614.678188   452.702906  626.567677   502.895294   625.889156  ...   \n",
       "  88   610.780189   458.868011  615.185936   477.053185   618.830906  ...   \n",
       "  89   765.614293   469.933048  766.507993   470.789768   781.748417  ...   \n",
       "  \n",
       "      right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0   1776.454620  1117.029297  1707.875732  1175.452209   1688.694595   \n",
       "  1   1735.705872  1135.692993  1715.566330  1264.062317   1664.476852   \n",
       "  2   1733.459900  1142.964020  1697.895248  1202.458801   1663.691101   \n",
       "  3   1737.032776  1135.072266  1684.483124  1178.996704   1672.043213   \n",
       "  4   1743.180664  1123.325958  1679.988220  1182.391449   1665.254425   \n",
       "  ..          ...          ...          ...          ...           ...   \n",
       "  85   586.208794   880.188446   505.825378  1035.297241    577.684608   \n",
       "  86   543.413345   877.787933   499.368111  1039.998718    528.521881   \n",
       "  87   539.170677   876.548218   492.909096  1034.671692    512.144562   \n",
       "  88   477.289307   884.087555   468.017761  1021.579865    471.886009   \n",
       "  89   458.440396   912.268219   468.580934  1061.678162    468.002583   \n",
       "  \n",
       "      right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0    1159.992218  1698.294006  1375.551819   1647.820847   1361.700439  \n",
       "  1    1239.175781  1720.171539  1412.566101   1657.927338   1382.527527  \n",
       "  2    1174.506500  1698.941254  1402.946960   1673.487183   1387.682251  \n",
       "  3    1177.421265  1667.101685  1395.307800   1678.455963   1395.165588  \n",
       "  4    1182.145447  1680.925262  1399.334106   1678.626297   1393.171570  \n",
       "  ..           ...          ...          ...           ...           ...  \n",
       "  85   1018.906738   524.747643  1186.148682    663.996201   1140.302002  \n",
       "  86   1017.603851   524.094872  1194.973145    602.029755   1135.251160  \n",
       "  87   1011.372192   522.266212  1189.946289    578.835785   1136.853821  \n",
       "  88   1020.269470   519.213074  1177.210815    512.555672   1177.816040  \n",
       "  89   1063.008026   514.562752  1178.330475    511.812805   1176.940216  \n",
       "  \n",
       "  [90 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1998.489532  689.945206  1994.203690  673.973633  1992.940079   \n",
       "  1    2013.525940  703.515945  2011.724960  681.606216  2012.140579   \n",
       "  2    2014.751709  701.482178  2013.520172  680.809143  2011.299240   \n",
       "  3    2026.507507  693.876892  2024.665543  674.311661  2024.391464   \n",
       "  4    2046.446121  687.875900  2042.346832  667.461685  2040.180206   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  235  3420.924515  313.875797  3416.830002  303.701847  3415.530258   \n",
       "  236  3305.270439  320.146662  3306.398361  308.098003  3308.056557   \n",
       "  237  3321.754227  316.135990  3324.382042  301.561942  3323.542236   \n",
       "  238  3341.452301  331.060192  3340.234741  314.390444  3344.715462   \n",
       "  239  3368.292778  321.310989  3369.787758  309.042860  3369.340042   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     673.934998  1962.062996  663.751434  1958.512802   661.257950  ...   \n",
       "  1     677.419601  1942.791748  666.009949  1961.095146   658.124634  ...   \n",
       "  2     675.992935  1980.970024  667.203796  1968.594948   653.701462  ...   \n",
       "  3     670.996613  1964.561539  662.694977  1969.594406   656.696625  ...   \n",
       "  4     668.402100  1997.423798  658.891159  1970.051880   656.873779  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  235   303.524115  3367.761963  309.381134  3384.777191   311.061811  ...   \n",
       "  236   309.791990  3324.852730  309.683197  3369.143280   310.517181  ...   \n",
       "  237   301.363192  3346.019188  303.039913  3391.776291   303.677049  ...   \n",
       "  238   316.445847  3351.891090  318.531727  3391.382736   314.049673  ...   \n",
       "  239   309.686005  3378.502731  313.338377  3395.314529   306.343832  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2013.828949   973.128052  2056.571335  1076.066406   2041.976074   \n",
       "  1    1964.809471  1084.775757  2041.648972  1123.809448   2062.992767   \n",
       "  2    1977.828529  1084.610779  2113.417236   989.381897   2068.864090   \n",
       "  3    1980.096031  1077.351929  2157.704254   932.784119   2095.799561   \n",
       "  4    2001.889992  1068.072998  2172.395142   945.545288   2132.335678   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  235  3357.957275   628.100769  3315.925301   797.393616   3363.665268   \n",
       "  236  3365.028778   616.046417  3318.002548   790.859100   3357.338013   \n",
       "  237  3377.534439   631.001984  3322.777092   792.255035   3337.056862   \n",
       "  238  3384.324738   628.878601  3330.417892   781.308990   3345.421127   \n",
       "  239  3378.762680   616.219421  3338.685150   776.207703   3341.357925   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1100.387939  2144.996765  1220.720276   2120.750763   1229.111938  \n",
       "  1     1153.319580  2131.361694  1227.614746   2140.509369   1234.800354  \n",
       "  2     1094.601379  2163.812500  1220.384094   2150.877686   1244.213379  \n",
       "  3     1040.165161  2159.211945  1221.245544   2131.884140   1259.342896  \n",
       "  4     1006.707520  2168.940491  1262.613098   2146.775314   1276.984314  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  235    798.075623  3285.679085   921.551453   3347.871346    921.977661  \n",
       "  236    790.916138  3292.100288   928.299988   3341.173149    928.064392  \n",
       "  237    793.432892  3293.437191   934.588135   3293.789482    933.617126  \n",
       "  238    785.693909  3290.042038   938.236267   3293.031990    938.734070  \n",
       "  239    778.432892  3296.732262   940.911316   3288.155071    940.888611  \n",
       "  \n",
       "  [240 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2011.067169  654.976318  2011.178757  628.203934  1992.251328   \n",
       "  1    2025.857574  653.558197  2026.099854  627.168579  2010.092514   \n",
       "  2    2033.290543  649.450104  2026.664124  627.195435  2020.174637   \n",
       "  3    2042.781937  648.593796  2034.394531  627.425095  2027.659592   \n",
       "  4    2055.706497  643.167465  2043.585037  622.532181  2038.584793   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  221  3391.194702  416.438957  3385.315765  405.616024  3393.778107   \n",
       "  222  3368.138107  425.573689  3364.298706  414.541367  3373.145096   \n",
       "  223  3340.396271  423.710896  3335.852097  410.609516  3344.082542   \n",
       "  224  3350.320267  405.470152  3352.537102  394.400235  3352.545616   \n",
       "  225  3369.042030  418.664898  3369.280319  404.184217  3370.361877   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     632.622665  2003.177628  629.609024  1943.067795   636.039337  ...   \n",
       "  1     627.840256  2006.625641  630.857864  1953.885597   630.130707  ...   \n",
       "  2     628.663589  1999.420067  632.545822  1968.905365   635.480331  ...   \n",
       "  3     626.791290  1991.695251  641.260223  1973.313087   637.015869  ...   \n",
       "  4     629.077881  2007.877029  632.942245  1991.552696   649.742447  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  221   405.720554  3346.086426  408.084927  3404.078461   410.971375  ...   \n",
       "  222   414.693470  3345.375839  409.263302  3408.784439   410.847160  ...   \n",
       "  223   410.145153  3348.770355  411.744217  3406.318054   401.209530  ...   \n",
       "  224   395.943647  3366.880981  395.178967  3393.677505   397.547430  ...   \n",
       "  225   404.738993  3378.007500  407.614960  3406.323639   401.084551  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1959.552521  1048.829895  2174.092743   994.023560   2136.755829   \n",
       "  1    1963.981117  1046.338196  2173.778381  1006.448547   2173.794586   \n",
       "  2    1978.783989  1045.897034  2182.460236  1036.712708   2187.927094   \n",
       "  3    2007.979233  1034.669250  2176.167511  1090.819336   2183.499390   \n",
       "  4    2014.869339  1072.297424  2196.615021  1111.986694   2082.109863   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  221  3388.513641   701.191010  3344.066040   864.523865   3377.122391   \n",
       "  222  3372.484543   703.543671  3345.691208   874.538422   3380.771866   \n",
       "  223  3404.708191   702.803589  3348.268089   873.864777   3395.968735   \n",
       "  224  3385.597153   707.987976  3367.416069   871.480743   3379.501289   \n",
       "  225  3399.278069   733.091766  3365.465683   873.465271   3365.507072   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1042.866638  2242.803833  1275.794678   2239.869537   1291.845093  \n",
       "  1     1036.616272  2246.173126  1301.434692   2239.447266   1318.867432  \n",
       "  2     1054.232300  2247.535187  1336.052490   2243.429718   1345.526123  \n",
       "  3     1103.603516  2238.059814  1374.310852   2241.215729   1391.971985  \n",
       "  4     1140.016968  2239.977448  1397.474976   2190.544769   1411.577515  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  221    870.666382  3295.397453   993.193359   3297.986336    994.854797  \n",
       "  222    873.847321  3314.818069  1025.456177   3358.503365   1026.978943  \n",
       "  223    874.002960  3319.075981  1019.572510   3377.803047   1016.283936  \n",
       "  224    870.369476  3343.202393  1011.307861   3355.395866   1013.440063  \n",
       "  225    874.811829  3333.906204  1011.761108   3316.857853   1010.232117  \n",
       "  \n",
       "  [226 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    2128.201019  579.637531  2130.626236  564.870991  2107.347122   \n",
       "  1    2072.606323  696.824509  2064.106148  676.615517  2055.801582   \n",
       "  2    2087.959229  710.166336  2079.162598  689.615768  2070.316696   \n",
       "  3    2087.209076  701.752533  2089.521103  679.804138  2067.683136   \n",
       "  4    2094.917923  700.012115  2087.280930  679.126419  2090.663727   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  211  3473.428925  394.110069  3467.020844  379.659286  3469.720764   \n",
       "  212  3445.827637  403.607338  3442.023224  385.686867  3447.589813   \n",
       "  213  3415.633453  408.193405  3414.029800  393.226799  3415.731094   \n",
       "  214  3426.462784  396.482964  3426.077393  382.504478  3428.862976   \n",
       "  215  3436.230988  397.794025  3436.572586  379.004799  3434.662125   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     568.695232  2112.777969  568.410591  2070.072029   564.956570  ...   \n",
       "  1     684.987610  2027.294380  683.185013  2027.098557   698.934601  ...   \n",
       "  2     696.218201  2041.358040  695.670181  2013.692726   696.947479  ...   \n",
       "  3     683.128647  2065.374611  668.541229  2017.695847   658.348755  ...   \n",
       "  4     675.223694  2002.070457  686.497879  2040.594582   690.167160  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  211   382.654411  3414.731461  386.924545  3454.985657   395.093452  ...   \n",
       "  212   389.351410  3429.512604  386.135132  3458.610748   395.129852  ...   \n",
       "  213   395.478653  3426.285873  384.480804  3464.639481   386.598351  ...   \n",
       "  214   384.286076  3432.791428  383.150887  3468.174988   383.335457  ...   \n",
       "  215   383.165169  3447.569138  385.283394  3469.889618   377.941658  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    2070.679733   970.418060  2153.766190  1129.204285   2132.891495   \n",
       "  1    2081.051483  1031.513458  2209.947708  1014.339996   2182.278717   \n",
       "  2    2011.915916  1063.907776  2200.870880  1020.032654   2173.247742   \n",
       "  3    2038.929169  1009.168732  2208.020874  1063.940735   2172.520966   \n",
       "  4    1968.971155  1146.612793  2212.131470  1101.439758   2087.386597   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  211  3436.411713   681.189636  3359.676483   838.011780   3434.062378   \n",
       "  212  3440.193604   684.152649  3392.149826   841.132507   3393.886154   \n",
       "  213  3473.285583   715.238037  3406.358185   860.473022   3408.826324   \n",
       "  214  3458.368073   707.083771  3421.685150   861.819031   3425.518921   \n",
       "  215  3472.131348   758.354462  3443.544327   849.611450   3449.669647   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1152.691162  2220.642273  1274.539673   2202.088287   1279.888062  \n",
       "  1     1172.684082  2222.387299  1255.482788   2218.191711   1315.848389  \n",
       "  2     1090.193848  2231.818298  1300.255432   2239.727570   1328.403625  \n",
       "  3     1119.431641  2240.303345  1334.414185   2225.739502   1359.015869  \n",
       "  4     1154.455933  2253.104248  1373.486206   2185.287186   1404.838257  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  211    857.638611  3234.340630   976.933533   3371.310287   1019.697205  \n",
       "  212    837.377136  3266.323982   972.946716   3261.176834    970.946899  \n",
       "  213    861.504944  3284.370941   969.579346   3282.840164    966.596741  \n",
       "  214    862.150940  3338.033737   961.628723   3334.227493    955.677185  \n",
       "  215    853.214600  3376.974907   957.895142   3374.441460    955.763611  \n",
       "  \n",
       "  [216 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1695.973755  543.502747  1686.414597  522.092972  1678.987869   \n",
       "  1    1716.498337  542.076523  1708.847168  523.912094  1698.744095   \n",
       "  2    1733.789169  540.757828  1729.109039  524.112129  1720.187027   \n",
       "  3    1750.177444  537.017044  1745.106949  519.779488  1736.581543   \n",
       "  4    1771.542786  529.088173  1764.196854  511.459610  1753.747040   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  196  2967.564880  343.899799  2970.343658  335.050179  2970.492859   \n",
       "  197  2991.497559  342.674194  2995.482849  334.007168  2990.369141   \n",
       "  198  2999.367813  344.700294  3002.447128  332.560555  3000.072464   \n",
       "  199  2999.448654  351.652332  3001.151047  339.459042  3001.355255   \n",
       "  200  2976.986755  444.883095  2979.000000  433.598312  2974.299210   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     523.524780  1645.290039  533.861008  1619.555374   537.375397  ...   \n",
       "  1     525.075233  1675.665611  530.971413  1639.820892   538.143127  ...   \n",
       "  2     522.893196  1682.038544  535.052765  1663.486687   535.842674  ...   \n",
       "  3     519.499397  1696.631500  528.915924  1670.762337   532.269836  ...   \n",
       "  4     514.247841  1713.634415  518.492630  1692.255569   528.042633  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  196   335.117783  2973.268143  338.258114  2997.606430   343.024765  ...   \n",
       "  197   334.014977  2991.327621  337.322376  2988.107269   341.480911  ...   \n",
       "  198   331.158825  2997.038147  336.140575  2994.594254   340.125607  ...   \n",
       "  199   338.808796  2998.490051  342.240337  2995.278595   342.220802  ...   \n",
       "  200   434.313389  2979.000000  434.444534  2972.464401   436.013779  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1618.695602   913.782776  1775.870789   939.568726   1769.409912   \n",
       "  1    1642.411713   914.631317  1776.733398   958.130798   1777.121765   \n",
       "  2    1657.986191   870.157867  1771.955490   967.277771   1777.478653   \n",
       "  3    1650.835083   923.028992  1783.161133   962.908142   1781.944656   \n",
       "  4    1650.644279   871.353821  1780.447144   968.920532   1786.434219   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  196  2985.774460   620.712952  2909.539642   765.434967   2982.320648   \n",
       "  197  2960.471558   623.203247  2921.305618   767.420746   2926.309204   \n",
       "  198  2970.075699   630.306091  2956.678467   773.572937   2928.346436   \n",
       "  199  2981.582016   618.393311  2938.705971   763.422058   2949.184853   \n",
       "  200  2970.649948   631.816940  2947.234245   756.779449   2950.182831   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0      947.914307  1759.291550  1185.716248   1738.107498   1192.295044  \n",
       "  1      963.094360  1752.621948  1198.450073   1752.209076   1203.188293  \n",
       "  2      973.242554  1751.151855  1189.778625   1758.294373   1203.107422  \n",
       "  3      970.936768  1760.273178  1197.468201   1758.445587   1209.002136  \n",
       "  4      970.760620  1760.277267  1203.526672   1759.099670   1209.585754  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  196    767.460419  2873.863609   922.700256   2958.798187    926.725159  \n",
       "  197    766.827789  2875.185131   924.545410   2874.335979    925.963135  \n",
       "  198    769.221466  2928.016197   928.565247   2875.249805    924.099976  \n",
       "  199    766.482178  2886.988918   918.244995   2894.400879    919.871765  \n",
       "  200    757.073883  2890.109264   903.371826   2889.581532    903.182068  \n",
       "  \n",
       "  [201 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1753.328140  625.882439  1742.987015  611.545898  1741.618378   \n",
       "  1    1777.621262  627.977280  1766.407578  611.643890  1760.287552   \n",
       "  2    1794.015350  626.738884  1784.216705  610.781525  1778.608109   \n",
       "  3    1812.306076  619.736595  1803.719772  603.777260  1800.483444   \n",
       "  4    1824.211731  620.380508  1816.069595  604.325165  1811.351898   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  192  3151.187408  362.305084  3148.882599  351.984589  3152.500992   \n",
       "  193  3135.260803  353.439705  3134.691116  343.674965  3138.767273   \n",
       "  194  3137.776886  359.835693  3137.907898  350.653572  3140.194427   \n",
       "  195  3140.362259  352.485741  3139.515671  339.948025  3142.585953   \n",
       "  196  3161.347931  370.722054  3166.171158  358.054314  3157.373352   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     611.955894  1660.697166  632.489410  1684.753044   636.270111  ...   \n",
       "  1     613.818878  1716.736504  624.228348  1707.502136   638.176666  ...   \n",
       "  2     615.014908  1725.543777  619.380440  1721.511978   632.730576  ...   \n",
       "  3     605.449364  1725.929382  618.571968  1754.198944   625.844597  ...   \n",
       "  4     608.344536  1739.817276  615.402763  1757.637131   627.966217  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  192   352.028435  3111.487991  350.668797  3154.753708   356.068497  ...   \n",
       "  193   344.180218  3122.619965  346.968636  3159.593658   349.973557  ...   \n",
       "  194   351.126228  3130.055695  354.553551  3157.012512   356.216469  ...   \n",
       "  195   340.265038  3132.529556  347.331226  3155.249893   353.424660  ...   \n",
       "  196   358.610191  3167.000000  359.473694  3153.662048   358.279472  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1648.050205  1015.063080  1767.450653  1047.957947   1779.072357   \n",
       "  1    1644.312428  1000.348877  1774.466934  1038.735657   1786.527710   \n",
       "  2    1684.028328   986.179535  1785.974258  1052.572693   1790.876160   \n",
       "  3    1677.939156   991.003967  1789.763489  1049.773193   1792.767212   \n",
       "  4    1668.735725   970.978271  1789.686340  1043.957825   1797.511124   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  192  3114.726151   642.993652  3045.817596   795.709473   3151.318085   \n",
       "  193  3133.233002   635.162720  3057.525368   789.947968   3106.930786   \n",
       "  194  3139.951080   632.770081  3070.908226   790.115173   3117.953751   \n",
       "  195  3135.171402   633.468323  3084.806671   787.074799   3096.777390   \n",
       "  196  3137.742386   665.570099  3105.321167   788.197601   3100.706772   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1047.434814  1805.010956  1286.388062   1822.347748   1297.350708  \n",
       "  1     1049.789551  1813.275986  1281.393555   1825.015366   1295.259460  \n",
       "  2     1053.526367  1806.662735  1281.373718   1818.428421   1292.857605  \n",
       "  3     1052.138245  1814.543533  1285.005554   1815.810196   1297.560364  \n",
       "  4     1049.377991  1810.045517  1283.153748   1817.100082   1295.826355  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  192    801.474670  3008.351318   961.448730   3145.793518    964.662598  \n",
       "  193    792.466309  3007.349819   956.766296   3038.303978    960.432983  \n",
       "  194    793.087402  3006.742718   952.496338   3036.162834    954.862671  \n",
       "  195    790.177155  3008.000690   947.571411   3009.179462    946.538330  \n",
       "  196    786.574890  3010.954765   947.991333   3008.796921    947.587891  \n",
       "  \n",
       "  [197 rows x 34 columns],\n",
       "  2),\n",
       " (          nose-x      nose-y   left-eye-x  left-eye-y  right-eye-x  \\\n",
       "  0    1879.366081  737.369953  1882.743858  731.341593  1871.394073   \n",
       "  1    1884.571056  692.569391  1894.189186  683.305449  1871.242165   \n",
       "  2    1858.909409  681.385727  1850.209320  668.202362  1849.562805   \n",
       "  3    1871.698196  682.325943  1864.092224  667.713943  1859.524384   \n",
       "  4    1888.397186  684.177261  1879.563004  667.563080  1878.905029   \n",
       "  ..           ...         ...          ...         ...          ...   \n",
       "  191  3181.089371  333.597649  3173.842743  321.486046  3179.321686   \n",
       "  192  3184.352234  334.064457  3177.044388  321.080521  3185.158707   \n",
       "  193  3160.665894  334.041088  3152.993164  322.207767  3162.882141   \n",
       "  194  3148.511841  330.104233  3147.398300  318.460331  3150.801064   \n",
       "  195  3121.297665  327.166552  3120.423508  315.651870  3123.458416   \n",
       "  \n",
       "       right-eye-y   left-ear-x  left-ear-y  right-ear-x  right-ear-y  ...  \\\n",
       "  0     730.689323  1878.862164  729.736414  1850.592894   725.389053  ...   \n",
       "  1     680.780854  1899.192459  681.892836  1848.535042   674.723060  ...   \n",
       "  2     669.072662  1810.892273  672.770660  1815.995560   671.986526  ...   \n",
       "  3     669.627853  1827.138428  670.015839  1809.234756   671.746506  ...   \n",
       "  4     670.868149  1825.344551  675.476425  1827.746437   675.673798  ...   \n",
       "  ..           ...          ...         ...          ...          ...  ...   \n",
       "  191   321.871475  3110.052658  332.206093  3164.360062   334.718170  ...   \n",
       "  192   322.415234  3125.769730  328.149303  3178.244781   331.543449  ...   \n",
       "  193   323.124741  3128.912956  329.433010  3183.466614   329.716869  ...   \n",
       "  194   317.512285  3146.165001  320.555279  3179.631851   321.980612  ...   \n",
       "  195   314.969698  3137.515144  322.952328  3183.766548   319.977283  ...   \n",
       "  \n",
       "       right-hip-x  right-hip-y  left-knee-x  left-knee-y  right-knee-x  \\\n",
       "  0    1855.234631   932.366486  1941.949844  1021.425415   1929.336174   \n",
       "  1    1834.683883   919.286148  1941.416672  1034.153351   1919.423935   \n",
       "  2    1826.258240   940.179138  1940.009171  1069.551392   1872.234970   \n",
       "  3    1792.738281   965.561462  1942.584793  1079.648010   1863.256989   \n",
       "  4    1808.124161  1062.872498  1942.051117  1070.641663   1948.570831   \n",
       "  ..           ...          ...          ...          ...           ...   \n",
       "  191  3143.670258   634.162506  3129.840088   793.381470   3146.466080   \n",
       "  192  3165.558228   629.060394  3131.011826   786.534790   3155.676544   \n",
       "  193  3182.450378   615.316498  3123.754723   774.554657   3172.876724   \n",
       "  194  3166.125366   625.138641  3142.518532   781.440979   3143.251442   \n",
       "  195  3184.285408   632.561859  3144.873459   789.463287   3164.358551   \n",
       "  \n",
       "       right-knee-y  left-foot-x  left-foot-y  right-foot-x  right-foot-y  \n",
       "  0     1031.919067  1972.388199  1259.790222   1966.906540   1268.088135  \n",
       "  1     1046.441620  1963.384567  1271.729553   1952.426575   1283.731018  \n",
       "  2     1078.454590  1958.048615  1291.107666   1911.970840   1281.072998  \n",
       "  3     1103.763794  1949.375031  1309.319824   1860.635666   1332.436523  \n",
       "  4     1087.637573  1955.446808  1309.585083   1950.165512   1320.593079  \n",
       "  ..            ...          ...          ...           ...           ...  \n",
       "  191    801.007446  3102.134659   938.500916   3019.827408    889.122070  \n",
       "  192    794.367004  3094.340332   915.144470   3043.057838    888.288513  \n",
       "  193    778.157532  3098.022446   917.679077   3145.654526    919.161377  \n",
       "  194    781.203552  3121.297264   915.557617   3116.815079    913.668945  \n",
       "  195    789.723724  3123.817600   904.312256   3144.134872    901.029480  \n",
       "  \n",
       "  [196 rows x 34 columns],\n",
       "  2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c78bd3-51ac-4b3f-9254-64d734f4aafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 시퀀스와 레이블 분리\n",
    "data, labels = zip(*all_sequences)\n",
    "\n",
    "# # 학습과 테스트 데이터셋으로 분할 (예: 90% 학습, 10% 테스트)\n",
    "# X_temp, X_test, y_temp, y_test = train_test_split(data, labels, test_size=0.1, random_state=0)\n",
    "\n",
    "# # 학습 데이터셋을 다시 학습과 검증 데이터셋으로 분할 (예: 80% 학습, 20% 검증)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=0)\n",
    "\n",
    "# 테스트 셋 없이 학습 데이터셋과 검증 데이터셋만 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73916cc6-74b0-4cae-8790-76f6de7641bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_padded shape: (84, 632, 34)\n",
      "X_val_padded shape: (21, 643, 34)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "# model = Sequential([\n",
    "#     # RNN 레이어\n",
    "#     SimpleRNN(50, return_sequences=True, input_shape=(None, X_train[0].shape[1])),\n",
    "#     SimpleRNN(50),\n",
    "#     # 분류를 위한 Dense 레이어\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(512, return_sequences=True, input_shape=(None, X_train[0].shape[1])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    # BatchNormalization(),\n",
    "    # Dense(3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터를 numpy 배열로 변환\n",
    "X_train = [x.to_numpy() for x in X_train]\n",
    "X_val = [x.to_numpy() for x in X_val]\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 패딩 처리\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', dtype='float32')\n",
    "X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding='post', dtype='float32')\n",
    "\n",
    "print(\"X_train_padded shape:\", X_train_padded.shape)\n",
    "print(\"X_val_padded shape:\", X_val_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6318f4bd-85eb-4514-8ce9-e1338ad74ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1024 - accuracy: 0.2381\n",
      "Epoch 1: loss improved from inf to 1.10243, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 1: accuracy improved from -inf to 0.23810, saving model to best_model_acc.h5\n",
      "3/3 [==============================] - 42s 15s/step - loss: 1.1024 - accuracy: 0.2381 - val_loss: 1.1042 - val_accuracy: 0.1905\n",
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAILAB\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.3810\n",
      "Epoch 2: loss improved from 1.10243 to 1.09392, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 2: accuracy improved from 0.23810 to 0.38095, saving model to best_model_acc.h5\n",
      "3/3 [==============================] - 38s 15s/step - loss: 1.0939 - accuracy: 0.3810 - val_loss: 1.0741 - val_accuracy: 0.4286\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1621 - accuracy: 0.4167 \n",
      "Epoch 3: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 3: accuracy improved from 0.38095 to 0.41667, saving model to best_model_acc.h5\n",
      "3/3 [==============================] - 40s 15s/step - loss: 1.1621 - accuracy: 0.4167 - val_loss: 1.2897 - val_accuracy: 0.4762\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2136 - accuracy: 0.3095 \n",
      "Epoch 4: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 4: accuracy did not improve from 0.41667\n",
      "3/3 [==============================] - 41s 16s/step - loss: 1.2136 - accuracy: 0.3095 - val_loss: 1.2052 - val_accuracy: 0.1905\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1362 - accuracy: 0.3214 \n",
      "Epoch 5: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 5: accuracy did not improve from 0.41667\n",
      "3/3 [==============================] - 43s 16s/step - loss: 1.1362 - accuracy: 0.3214 - val_loss: 1.2193 - val_accuracy: 0.1429\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1324 - accuracy: 0.2976 \n",
      "Epoch 6: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 6: accuracy did not improve from 0.41667\n",
      "3/3 [==============================] - 43s 16s/step - loss: 1.1324 - accuracy: 0.2976 - val_loss: 1.1084 - val_accuracy: 0.4286\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1097 - accuracy: 0.3333 \n",
      "Epoch 7: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 7: accuracy did not improve from 0.41667\n",
      "3/3 [==============================] - 43s 16s/step - loss: 1.1097 - accuracy: 0.3333 - val_loss: 1.0994 - val_accuracy: 0.1905\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1396 - accuracy: 0.2976 \n",
      "Epoch 8: loss did not improve from 1.09392\n",
      "\n",
      "Epoch 8: accuracy did not improve from 0.41667\n",
      "3/3 [==============================] - 43s 16s/step - loss: 1.1396 - accuracy: 0.2976 - val_loss: 1.1029 - val_accuracy: 0.1905\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0877 - accuracy: 0.4286 \n",
      "Epoch 9: loss improved from 1.09392 to 1.08771, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 9: accuracy improved from 0.41667 to 0.42857, saving model to best_model_acc.h5\n",
      "3/3 [==============================] - 44s 16s/step - loss: 1.0877 - accuracy: 0.4286 - val_loss: 1.1018 - val_accuracy: 0.1905\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1180 - accuracy: 0.2619 \n",
      "Epoch 10: loss did not improve from 1.08771\n",
      "\n",
      "Epoch 10: accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 1.1180 - accuracy: 0.2619 - val_loss: 1.0945 - val_accuracy: 0.1905\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1023 - accuracy: 0.3810 \n",
      "Epoch 11: loss did not improve from 1.08771\n",
      "\n",
      "Epoch 11: accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 17s/step - loss: 1.1023 - accuracy: 0.3810 - val_loss: 1.0955 - val_accuracy: 0.1905\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.3690 \n",
      "Epoch 12: loss improved from 1.08771 to 1.07616, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 12: accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 1.0762 - accuracy: 0.3690 - val_loss: 1.1267 - val_accuracy: 0.1905\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "# model = Sequential([\n",
    "#     # RNN 레이어\n",
    "#     SimpleRNN(50, return_sequences=True, input_shape=(None, X_train[0].shape[1])),\n",
    "#     SimpleRNN(50),\n",
    "#     # 분류를 위한 Dense 레이어\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(512, return_sequences=True, input_shape=(None, X_train[0].shape[1])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    # BatchNormalization(),\n",
    "    # Dense(3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터를 numpy 배열로 변환\n",
    "X_train = [x.to_numpy() for x in X_train]\n",
    "X_val = [x.to_numpy() for x in X_val]\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 패딩 처리\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', dtype='float32')\n",
    "X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding='post', dtype='float32')\n",
    "\n",
    "# 검증 손실을 기준으로 최적의 모델 저장\n",
    "checkpoint_loss = ModelCheckpoint('best_model_loss.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# 검증 정확도를 기준으로 최적의 모델 저장\n",
    "checkpoint_acc = ModelCheckpoint('best_model_acc.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train_padded, y_train, validation_data=(X_val_padded, y_val), epochs=1000, batch_size=32, callbacks=[checkpoint_loss, checkpoint_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7db1b58-c855-4b38-8cea-bf2ffe8cc06c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.7619\n",
      "Epoch 1: loss improved from inf to 0.56979, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 1: accuracy improved from -inf to 0.76190, saving model to best_model_acc.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.31645, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.42857, saving model to best_model_val_acc.h5\n",
      "3/3 [==============================] - 40s 15s/step - loss: 0.5698 - accuracy: 0.7619 - val_loss: 1.3165 - val_accuracy: 0.4286\n",
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAILAB\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7381\n",
      "Epoch 2: loss did not improve from 0.56979\n",
      "\n",
      "Epoch 2: accuracy did not improve from 0.76190\n",
      "\n",
      "Epoch 2: val_loss improved from 1.31645 to 1.16764, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 37s 14s/step - loss: 0.6060 - accuracy: 0.7381 - val_loss: 1.1676 - val_accuracy: 0.4286\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7381\n",
      "Epoch 3: loss did not improve from 0.56979\n",
      "\n",
      "Epoch 3: accuracy did not improve from 0.76190\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 39s 15s/step - loss: 0.5810 - accuracy: 0.7381 - val_loss: 1.1998 - val_accuracy: 0.4286\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.7619 \n",
      "Epoch 4: loss did not improve from 0.56979\n",
      "\n",
      "Epoch 4: accuracy did not improve from 0.76190\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.5699 - accuracy: 0.7619 - val_loss: 1.2476 - val_accuracy: 0.4286\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7500 \n",
      "Epoch 5: loss did not improve from 0.56979\n",
      "\n",
      "Epoch 5: accuracy did not improve from 0.76190\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.6012 - accuracy: 0.7500 - val_loss: 1.3329 - val_accuracy: 0.3810\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5602 - accuracy: 0.7857\n",
      "Epoch 6: loss improved from 0.56979 to 0.56018, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 6: accuracy improved from 0.76190 to 0.78571, saving model to best_model_acc.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 40s 15s/step - loss: 0.5602 - accuracy: 0.7857 - val_loss: 1.2802 - val_accuracy: 0.3810\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.7738 \n",
      "Epoch 7: loss did not improve from 0.56018\n",
      "\n",
      "Epoch 7: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.5626 - accuracy: 0.7738 - val_loss: 1.2686 - val_accuracy: 0.3810\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.7262 \n",
      "Epoch 8: loss did not improve from 0.56018\n",
      "\n",
      "Epoch 8: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 15s/step - loss: 0.5842 - accuracy: 0.7262 - val_loss: 1.2237 - val_accuracy: 0.3810\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.7619 \n",
      "Epoch 9: loss improved from 0.56018 to 0.55251, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 9: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5525 - accuracy: 0.7619 - val_loss: 1.2355 - val_accuracy: 0.3810\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.7619 \n",
      "Epoch 10: loss did not improve from 0.55251\n",
      "\n",
      "Epoch 10: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5802 - accuracy: 0.7619 - val_loss: 1.1876 - val_accuracy: 0.4286\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.7500 \n",
      "Epoch 11: loss did not improve from 0.55251\n",
      "\n",
      "Epoch 11: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 41s 16s/step - loss: 0.5668 - accuracy: 0.7500 - val_loss: 1.2126 - val_accuracy: 0.4286\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.7738\n",
      "Epoch 12: loss did not improve from 0.55251\n",
      "\n",
      "Epoch 12: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 41s 16s/step - loss: 0.5678 - accuracy: 0.7738 - val_loss: 1.2227 - val_accuracy: 0.4286\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.7738 \n",
      "Epoch 13: loss did not improve from 0.55251\n",
      "\n",
      "Epoch 13: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5791 - accuracy: 0.7738 - val_loss: 1.2951 - val_accuracy: 0.3810\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.7857 \n",
      "Epoch 14: loss improved from 0.55251 to 0.53693, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 14: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5369 - accuracy: 0.7857 - val_loss: 1.3120 - val_accuracy: 0.3810\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.7857 \n",
      "Epoch 15: loss did not improve from 0.53693\n",
      "\n",
      "Epoch 15: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.5698 - accuracy: 0.7857 - val_loss: 1.2974 - val_accuracy: 0.3810\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.7619 \n",
      "Epoch 16: loss did not improve from 0.53693\n",
      "\n",
      "Epoch 16: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 17s/step - loss: 0.5821 - accuracy: 0.7619 - val_loss: 1.2637 - val_accuracy: 0.4286\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5867 - accuracy: 0.7738 \n",
      "Epoch 17: loss did not improve from 0.53693\n",
      "\n",
      "Epoch 17: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5867 - accuracy: 0.7738 - val_loss: 1.2511 - val_accuracy: 0.4286\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.7738 \n",
      "Epoch 18: loss did not improve from 0.53693\n",
      "\n",
      "Epoch 18: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.5654 - accuracy: 0.7738 - val_loss: 1.2719 - val_accuracy: 0.3810\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.7857 \n",
      "Epoch 19: loss did not improve from 0.53693\n",
      "\n",
      "Epoch 19: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5534 - accuracy: 0.7857 - val_loss: 1.2381 - val_accuracy: 0.3810\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.7500 \n",
      "Epoch 20: loss improved from 0.53693 to 0.53008, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 20: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 42s 16s/step - loss: 0.5301 - accuracy: 0.7500 - val_loss: 1.2644 - val_accuracy: 0.3810\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.7738 \n",
      "Epoch 21: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 21: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.5615 - accuracy: 0.7738 - val_loss: 1.2628 - val_accuracy: 0.3810\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.7738 \n",
      "Epoch 22: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 22: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 17s/step - loss: 0.5514 - accuracy: 0.7738 - val_loss: 1.2399 - val_accuracy: 0.3810\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.7738 \n",
      "Epoch 23: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 23: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.5732 - accuracy: 0.7738 - val_loss: 1.2756 - val_accuracy: 0.3810\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.7619 \n",
      "Epoch 24: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 24: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.5872 - accuracy: 0.7619 - val_loss: 1.2311 - val_accuracy: 0.3810\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.7381 \n",
      "Epoch 25: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 25: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 17s/step - loss: 0.6186 - accuracy: 0.7381 - val_loss: 1.2525 - val_accuracy: 0.3810\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7619 \n",
      "Epoch 26: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 26: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 17s/step - loss: 0.5465 - accuracy: 0.7619 - val_loss: 1.1902 - val_accuracy: 0.4286\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7619 \n",
      "Epoch 27: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 27: accuracy did not improve from 0.78571\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.5683 - accuracy: 0.7619 - val_loss: 1.1802 - val_accuracy: 0.4286\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7976 \n",
      "Epoch 28: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 28: accuracy improved from 0.78571 to 0.79762, saving model to best_model_acc.h5\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.5408 - accuracy: 0.7976 - val_loss: 1.2660 - val_accuracy: 0.3810\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5530 - accuracy: 0.7976 \n",
      "Epoch 29: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 29: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.16764\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5530 - accuracy: 0.7976 - val_loss: 1.4099 - val_accuracy: 0.4286\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.7738 \n",
      "Epoch 30: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 30: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 30: val_loss improved from 1.16764 to 1.14204, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6194 - accuracy: 0.7738 - val_loss: 1.1420 - val_accuracy: 0.3810\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7909 - accuracy: 0.6786 \n",
      "Epoch 31: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 31: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 31: val_loss improved from 1.14204 to 1.10823, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.7909 - accuracy: 0.6786 - val_loss: 1.1082 - val_accuracy: 0.4286\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7497 - accuracy: 0.6786 \n",
      "Epoch 32: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 32: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.10823\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 43s 16s/step - loss: 0.7497 - accuracy: 0.6786 - val_loss: 1.2194 - val_accuracy: 0.4286\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7425 - accuracy: 0.6429 \n",
      "Epoch 33: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 33: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.10823\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.7425 - accuracy: 0.6429 - val_loss: 1.2227 - val_accuracy: 0.4286\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.6786 \n",
      "Epoch 34: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 34: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.10823\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.7178 - accuracy: 0.6786 - val_loss: 1.1445 - val_accuracy: 0.4286\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.6786 \n",
      "Epoch 35: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 35: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 35: val_loss improved from 1.10823 to 1.03876, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.7132 - accuracy: 0.6786 - val_loss: 1.0388 - val_accuracy: 0.4286\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.7024 \n",
      "Epoch 36: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 36: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 36: val_loss improved from 1.03876 to 1.02291, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6570 - accuracy: 0.7024 - val_loss: 1.0229 - val_accuracy: 0.3810\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.6905 \n",
      "Epoch 37: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 37: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 37: val_loss improved from 1.02291 to 0.99420, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 44s 16s/step - loss: 0.6526 - accuracy: 0.6905 - val_loss: 0.9942 - val_accuracy: 0.4286\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.7143 \n",
      "Epoch 38: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 38: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6513 - accuracy: 0.7143 - val_loss: 1.0232 - val_accuracy: 0.4286\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.7024 \n",
      "Epoch 39: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 39: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.6775 - accuracy: 0.7024 - val_loss: 1.0210 - val_accuracy: 0.4286\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.6905 \n",
      "Epoch 40: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 40: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6906 - accuracy: 0.6905 - val_loss: 1.0147 - val_accuracy: 0.4286\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.7262 \n",
      "Epoch 41: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 41: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.6480 - accuracy: 0.7262 - val_loss: 1.0366 - val_accuracy: 0.4286\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.7143 \n",
      "Epoch 42: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 42: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6280 - accuracy: 0.7143 - val_loss: 1.1034 - val_accuracy: 0.4286\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6586 - accuracy: 0.6905 \n",
      "Epoch 43: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 43: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6586 - accuracy: 0.6905 - val_loss: 1.1544 - val_accuracy: 0.4286\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.6786 \n",
      "Epoch 44: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 44: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6704 - accuracy: 0.6786 - val_loss: 1.1924 - val_accuracy: 0.4286\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6332 - accuracy: 0.7143 \n",
      "Epoch 45: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 45: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6332 - accuracy: 0.7143 - val_loss: 1.2719 - val_accuracy: 0.4286\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6654 - accuracy: 0.7262 \n",
      "Epoch 46: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 46: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6654 - accuracy: 0.7262 - val_loss: 1.2389 - val_accuracy: 0.4286\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6568 - accuracy: 0.7143 \n",
      "Epoch 47: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 47: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6568 - accuracy: 0.7143 - val_loss: 1.1585 - val_accuracy: 0.4286\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7500 \n",
      "Epoch 48: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 48: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6093 - accuracy: 0.7500 - val_loss: 1.0627 - val_accuracy: 0.4286\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.7024 \n",
      "Epoch 49: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 49: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6405 - accuracy: 0.7024 - val_loss: 1.1096 - val_accuracy: 0.3810\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.7262 \n",
      "Epoch 50: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 50: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5915 - accuracy: 0.7262 - val_loss: 1.1820 - val_accuracy: 0.4286\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7262 \n",
      "Epoch 51: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 51: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6368 - accuracy: 0.7262 - val_loss: 1.1535 - val_accuracy: 0.4286\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.7619 \n",
      "Epoch 52: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 52: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5794 - accuracy: 0.7619 - val_loss: 1.0903 - val_accuracy: 0.4286\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5812 - accuracy: 0.7738 \n",
      "Epoch 53: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 53: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5812 - accuracy: 0.7738 - val_loss: 1.0593 - val_accuracy: 0.4286\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7738 \n",
      "Epoch 54: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 54: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5630 - accuracy: 0.7738 - val_loss: 1.0738 - val_accuracy: 0.4286\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.7262 \n",
      "Epoch 55: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 55: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6066 - accuracy: 0.7262 - val_loss: 1.0767 - val_accuracy: 0.4286\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.7976 \n",
      "Epoch 56: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 56: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5601 - accuracy: 0.7976 - val_loss: 1.1037 - val_accuracy: 0.4286\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7857 \n",
      "Epoch 57: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 57: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5461 - accuracy: 0.7857 - val_loss: 1.1461 - val_accuracy: 0.4286\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.7738 \n",
      "Epoch 58: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 58: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5585 - accuracy: 0.7738 - val_loss: 1.2633 - val_accuracy: 0.4286\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.7619 \n",
      "Epoch 59: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 59: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5591 - accuracy: 0.7619 - val_loss: 1.3061 - val_accuracy: 0.4286\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6127 - accuracy: 0.7381 \n",
      "Epoch 60: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 60: accuracy did not improve from 0.79762\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.6127 - accuracy: 0.7381 - val_loss: 1.2564 - val_accuracy: 0.4286\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.8095 \n",
      "Epoch 61: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 61: accuracy improved from 0.79762 to 0.80952, saving model to best_model_acc.h5\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5512 - accuracy: 0.8095 - val_loss: 1.1827 - val_accuracy: 0.4286\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7738 \n",
      "Epoch 62: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 62: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5681 - accuracy: 0.7738 - val_loss: 1.1900 - val_accuracy: 0.4286\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7738 \n",
      "Epoch 63: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 63: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5664 - accuracy: 0.7738 - val_loss: 1.2218 - val_accuracy: 0.4286\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7738 \n",
      "Epoch 64: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 64: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5589 - accuracy: 0.7738 - val_loss: 1.1934 - val_accuracy: 0.4286\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.7738 \n",
      "Epoch 65: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 65: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5652 - accuracy: 0.7738 - val_loss: 1.1509 - val_accuracy: 0.4286\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.7857 \n",
      "Epoch 66: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 66: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5828 - accuracy: 0.7857 - val_loss: 1.1537 - val_accuracy: 0.4286\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7619 \n",
      "Epoch 67: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 67: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5822 - accuracy: 0.7619 - val_loss: 1.2188 - val_accuracy: 0.4286\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7857 \n",
      "Epoch 68: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 68: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5408 - accuracy: 0.7857 - val_loss: 1.2400 - val_accuracy: 0.4286\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7738 \n",
      "Epoch 69: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 69: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5372 - accuracy: 0.7738 - val_loss: 1.2159 - val_accuracy: 0.4286\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.7619 \n",
      "Epoch 70: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 70: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5662 - accuracy: 0.7619 - val_loss: 1.1472 - val_accuracy: 0.4286\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.7857 \n",
      "Epoch 71: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 71: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5419 - accuracy: 0.7857 - val_loss: 1.0956 - val_accuracy: 0.4286\n",
      "Epoch 72/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.7500 \n",
      "Epoch 72: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 72: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5476 - accuracy: 0.7500 - val_loss: 1.1053 - val_accuracy: 0.4286\n",
      "Epoch 73/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.7976 \n",
      "Epoch 73: loss did not improve from 0.53008\n",
      "\n",
      "Epoch 73: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5323 - accuracy: 0.7976 - val_loss: 1.1368 - val_accuracy: 0.4286\n",
      "Epoch 74/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.7857 \n",
      "Epoch 74: loss improved from 0.53008 to 0.51432, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 74: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5143 - accuracy: 0.7857 - val_loss: 1.1639 - val_accuracy: 0.4286\n",
      "Epoch 75/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.7500 \n",
      "Epoch 75: loss did not improve from 0.51432\n",
      "\n",
      "Epoch 75: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 45s 17s/step - loss: 0.5666 - accuracy: 0.7500 - val_loss: 1.1614 - val_accuracy: 0.4286\n",
      "Epoch 76/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.7619 \n",
      "Epoch 76: loss did not improve from 0.51432\n",
      "\n",
      "Epoch 76: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5311 - accuracy: 0.7619 - val_loss: 1.1089 - val_accuracy: 0.4286\n",
      "Epoch 77/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.7738 \n",
      "Epoch 77: loss improved from 0.51432 to 0.50632, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 77: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5063 - accuracy: 0.7738 - val_loss: 1.1121 - val_accuracy: 0.4286\n",
      "Epoch 78/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.7738 \n",
      "Epoch 78: loss did not improve from 0.50632\n",
      "\n",
      "Epoch 78: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5306 - accuracy: 0.7738 - val_loss: 1.1429 - val_accuracy: 0.4286\n",
      "Epoch 79/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.7857 \n",
      "Epoch 79: loss improved from 0.50632 to 0.50262, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 79: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5026 - accuracy: 0.7857 - val_loss: 1.1701 - val_accuracy: 0.4286\n",
      "Epoch 80/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.7619 \n",
      "Epoch 80: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 80: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5170 - accuracy: 0.7619 - val_loss: 1.1649 - val_accuracy: 0.3810\n",
      "Epoch 81/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.7738 \n",
      "Epoch 81: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 81: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5661 - accuracy: 0.7738 - val_loss: 1.2155 - val_accuracy: 0.4286\n",
      "Epoch 82/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7738 \n",
      "Epoch 82: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 82: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5545 - accuracy: 0.7738 - val_loss: 1.1931 - val_accuracy: 0.4286\n",
      "Epoch 83/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.7857 \n",
      "Epoch 83: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 83: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5770 - accuracy: 0.7857 - val_loss: 1.1838 - val_accuracy: 0.3810\n",
      "Epoch 84/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.7857 \n",
      "Epoch 84: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 84: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5648 - accuracy: 0.7857 - val_loss: 1.1559 - val_accuracy: 0.3810\n",
      "Epoch 85/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.7857 \n",
      "Epoch 85: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 85: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5406 - accuracy: 0.7857 - val_loss: 1.1935 - val_accuracy: 0.3810\n",
      "Epoch 86/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.7857 \n",
      "Epoch 86: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 86: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5427 - accuracy: 0.7857 - val_loss: 1.2281 - val_accuracy: 0.4286\n",
      "Epoch 87/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.7738 \n",
      "Epoch 87: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 87: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5396 - accuracy: 0.7738 - val_loss: 1.2114 - val_accuracy: 0.4286\n",
      "Epoch 88/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.7857 \n",
      "Epoch 88: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 88: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5544 - accuracy: 0.7857 - val_loss: 1.2493 - val_accuracy: 0.3810\n",
      "Epoch 89/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7738 \n",
      "Epoch 89: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 89: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5496 - accuracy: 0.7738 - val_loss: 1.2609 - val_accuracy: 0.3810\n",
      "Epoch 90/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.7857 \n",
      "Epoch 90: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 90: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5261 - accuracy: 0.7857 - val_loss: 1.2663 - val_accuracy: 0.3810\n",
      "Epoch 91/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.7738 \n",
      "Epoch 91: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 91: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5100 - accuracy: 0.7738 - val_loss: 1.1947 - val_accuracy: 0.3810\n",
      "Epoch 92/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.7857 \n",
      "Epoch 92: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 92: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5259 - accuracy: 0.7857 - val_loss: 1.1871 - val_accuracy: 0.3810\n",
      "Epoch 93/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.7619 \n",
      "Epoch 93: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 93: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5351 - accuracy: 0.7619 - val_loss: 1.1696 - val_accuracy: 0.3810\n",
      "Epoch 94/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7857 \n",
      "Epoch 94: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 94: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.99420\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5380 - accuracy: 0.7857 - val_loss: 1.0444 - val_accuracy: 0.4286\n",
      "Epoch 95/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.7619 \n",
      "Epoch 95: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 95: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 95: val_loss improved from 0.99420 to 0.98147, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5491 - accuracy: 0.7619 - val_loss: 0.9815 - val_accuracy: 0.4286\n",
      "Epoch 96/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7857 \n",
      "Epoch 96: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 96: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 96: val_loss improved from 0.98147 to 0.97902, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5461 - accuracy: 0.7857 - val_loss: 0.9790 - val_accuracy: 0.4286\n",
      "Epoch 97/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.7857 \n",
      "Epoch 97: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 97: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5584 - accuracy: 0.7857 - val_loss: 1.0280 - val_accuracy: 0.4286\n",
      "Epoch 98/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7738 \n",
      "Epoch 98: loss did not improve from 0.50262\n",
      "\n",
      "Epoch 98: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5109 - accuracy: 0.7738 - val_loss: 1.0612 - val_accuracy: 0.3810\n",
      "Epoch 99/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.7738 \n",
      "Epoch 99: loss improved from 0.50262 to 0.49936, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 99: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4994 - accuracy: 0.7738 - val_loss: 1.1092 - val_accuracy: 0.3810\n",
      "Epoch 100/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.7738 \n",
      "Epoch 100: loss improved from 0.49936 to 0.49282, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 100: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4928 - accuracy: 0.7738 - val_loss: 1.1286 - val_accuracy: 0.3810\n",
      "Epoch 101/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.7857 \n",
      "Epoch 101: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 101: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 101: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5171 - accuracy: 0.7857 - val_loss: 1.1630 - val_accuracy: 0.3810\n",
      "Epoch 102/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.7500 \n",
      "Epoch 102: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 102: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 102: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5538 - accuracy: 0.7500 - val_loss: 1.2482 - val_accuracy: 0.3810\n",
      "Epoch 103/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.7738 \n",
      "Epoch 103: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 103: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 103: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5211 - accuracy: 0.7738 - val_loss: 1.2869 - val_accuracy: 0.4286\n",
      "Epoch 104/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.7738 \n",
      "Epoch 104: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 104: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 104: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5381 - accuracy: 0.7738 - val_loss: 1.0910 - val_accuracy: 0.4286\n",
      "Epoch 105/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7500 \n",
      "Epoch 105: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 105: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 105: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5593 - accuracy: 0.7500 - val_loss: 1.0035 - val_accuracy: 0.4286\n",
      "Epoch 106/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.7738 \n",
      "Epoch 106: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 106: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 106: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5129 - accuracy: 0.7738 - val_loss: 1.0310 - val_accuracy: 0.3810\n",
      "Epoch 107/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.7619 \n",
      "Epoch 107: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 107: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.97902\n",
      "\n",
      "Epoch 107: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.5390 - accuracy: 0.7619 - val_loss: 0.9914 - val_accuracy: 0.3810\n",
      "Epoch 108/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.7500 \n",
      "Epoch 108: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 108: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 108: val_loss improved from 0.97902 to 0.96898, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 108: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5473 - accuracy: 0.7500 - val_loss: 0.9690 - val_accuracy: 0.3810\n",
      "Epoch 109/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.7857 \n",
      "Epoch 109: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 109: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 109: val_loss improved from 0.96898 to 0.96524, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 109: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5221 - accuracy: 0.7857 - val_loss: 0.9652 - val_accuracy: 0.4286\n",
      "Epoch 110/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.7619 \n",
      "Epoch 110: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 110: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 110: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5327 - accuracy: 0.7619 - val_loss: 0.9835 - val_accuracy: 0.4286\n",
      "Epoch 111/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.7619 \n",
      "Epoch 111: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 111: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 111: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5327 - accuracy: 0.7619 - val_loss: 1.0212 - val_accuracy: 0.4286\n",
      "Epoch 112/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.7857 \n",
      "Epoch 112: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 112: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 112: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5186 - accuracy: 0.7857 - val_loss: 1.0917 - val_accuracy: 0.3810\n",
      "Epoch 113/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.7857 \n",
      "Epoch 113: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 113: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 113: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5156 - accuracy: 0.7857 - val_loss: 1.1260 - val_accuracy: 0.3810\n",
      "Epoch 114/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.7619 \n",
      "Epoch 114: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 114: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 114: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5459 - accuracy: 0.7619 - val_loss: 1.1247 - val_accuracy: 0.3810\n",
      "Epoch 115/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.7738 \n",
      "Epoch 115: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 115: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 115: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5171 - accuracy: 0.7738 - val_loss: 1.0616 - val_accuracy: 0.3810\n",
      "Epoch 116/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5083 - accuracy: 0.7619 \n",
      "Epoch 116: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 116: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 116: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5083 - accuracy: 0.7619 - val_loss: 1.0248 - val_accuracy: 0.4286\n",
      "Epoch 117/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.7738 \n",
      "Epoch 117: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 117: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 117: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5272 - accuracy: 0.7738 - val_loss: 1.0186 - val_accuracy: 0.4286\n",
      "Epoch 118/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.7738 \n",
      "Epoch 118: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 118: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 118: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5412 - accuracy: 0.7738 - val_loss: 0.9943 - val_accuracy: 0.4286\n",
      "Epoch 119/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.7619 \n",
      "Epoch 119: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 119: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 119: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5163 - accuracy: 0.7619 - val_loss: 0.9794 - val_accuracy: 0.4286\n",
      "Epoch 120/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.7500 \n",
      "Epoch 120: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 120: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 120: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5246 - accuracy: 0.7500 - val_loss: 0.9679 - val_accuracy: 0.4286\n",
      "Epoch 121/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.7738 \n",
      "Epoch 121: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 121: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 121: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5059 - accuracy: 0.7738 - val_loss: 0.9708 - val_accuracy: 0.4286\n",
      "Epoch 122/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.7619 \n",
      "Epoch 122: loss did not improve from 0.49282\n",
      "\n",
      "Epoch 122: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 122: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4957 - accuracy: 0.7619 - val_loss: 0.9763 - val_accuracy: 0.4286\n",
      "Epoch 123/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4862 - accuracy: 0.7976 \n",
      "Epoch 123: loss improved from 0.49282 to 0.48622, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 123: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 123: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4862 - accuracy: 0.7976 - val_loss: 0.9948 - val_accuracy: 0.4286\n",
      "Epoch 124/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.7857 \n",
      "Epoch 124: loss improved from 0.48622 to 0.47600, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 124: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 124: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4760 - accuracy: 0.7857 - val_loss: 1.0085 - val_accuracy: 0.4286\n",
      "Epoch 125/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.7857 \n",
      "Epoch 125: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 125: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 125: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4810 - accuracy: 0.7857 - val_loss: 1.0123 - val_accuracy: 0.4286\n",
      "Epoch 126/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.7857 \n",
      "Epoch 126: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 126: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 126: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5056 - accuracy: 0.7857 - val_loss: 1.0109 - val_accuracy: 0.4286\n",
      "Epoch 127/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.7976 \n",
      "Epoch 127: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 127: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 127: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4886 - accuracy: 0.7976 - val_loss: 1.0254 - val_accuracy: 0.4286\n",
      "Epoch 128/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.7738 \n",
      "Epoch 128: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 128: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 128: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4974 - accuracy: 0.7738 - val_loss: 1.0279 - val_accuracy: 0.4286\n",
      "Epoch 129/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.7857 \n",
      "Epoch 129: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 129: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 129: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5087 - accuracy: 0.7857 - val_loss: 1.0033 - val_accuracy: 0.4286\n",
      "Epoch 130/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.7619 \n",
      "Epoch 130: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 130: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 130: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5267 - accuracy: 0.7619 - val_loss: 0.9834 - val_accuracy: 0.4286\n",
      "Epoch 131/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5435 - accuracy: 0.7738 \n",
      "Epoch 131: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 131: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 131: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5435 - accuracy: 0.7738 - val_loss: 1.0003 - val_accuracy: 0.4286\n",
      "Epoch 132/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.7738 \n",
      "Epoch 132: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 132: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 132: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5212 - accuracy: 0.7738 - val_loss: 1.0822 - val_accuracy: 0.4286\n",
      "Epoch 133/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.7381 \n",
      "Epoch 133: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 133: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 133: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5631 - accuracy: 0.7381 - val_loss: 1.0286 - val_accuracy: 0.4286\n",
      "Epoch 134/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.7619 \n",
      "Epoch 134: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 134: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 134: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5479 - accuracy: 0.7619 - val_loss: 1.0689 - val_accuracy: 0.4286\n",
      "Epoch 135/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.7619 \n",
      "Epoch 135: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 135: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 135: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.6001 - accuracy: 0.7619 - val_loss: 1.1185 - val_accuracy: 0.4286\n",
      "Epoch 136/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.7143 \n",
      "Epoch 136: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 136: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 136: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.6038 - accuracy: 0.7143 - val_loss: 1.1132 - val_accuracy: 0.4286\n",
      "Epoch 137/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7500 \n",
      "Epoch 137: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 137: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 137: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5460 - accuracy: 0.7500 - val_loss: 1.1313 - val_accuracy: 0.4286\n",
      "Epoch 138/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.7619 \n",
      "Epoch 138: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 138: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 138: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5585 - accuracy: 0.7619 - val_loss: 1.2038 - val_accuracy: 0.4286\n",
      "Epoch 139/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.7738 \n",
      "Epoch 139: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 139: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 139: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5114 - accuracy: 0.7738 - val_loss: 1.1911 - val_accuracy: 0.4286\n",
      "Epoch 140/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5307 - accuracy: 0.7857 \n",
      "Epoch 140: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 140: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 140: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5307 - accuracy: 0.7857 - val_loss: 1.1433 - val_accuracy: 0.4286\n",
      "Epoch 141/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.7024 \n",
      "Epoch 141: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 141: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 141: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5206 - accuracy: 0.7024 - val_loss: 1.1014 - val_accuracy: 0.4286\n",
      "Epoch 142/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8095 \n",
      "Epoch 142: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 142: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 142: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5037 - accuracy: 0.8095 - val_loss: 1.0506 - val_accuracy: 0.4286\n",
      "Epoch 143/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.7857 \n",
      "Epoch 143: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 143: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 143: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4927 - accuracy: 0.7857 - val_loss: 1.0144 - val_accuracy: 0.4286\n",
      "Epoch 144/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7857 \n",
      "Epoch 144: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 144: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 144: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4985 - accuracy: 0.7857 - val_loss: 1.0014 - val_accuracy: 0.4286\n",
      "Epoch 145/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.7500 \n",
      "Epoch 145: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 145: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 145: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5087 - accuracy: 0.7500 - val_loss: 1.0049 - val_accuracy: 0.4286\n",
      "Epoch 146/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.7976 \n",
      "Epoch 146: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 146: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 146: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5157 - accuracy: 0.7976 - val_loss: 1.0301 - val_accuracy: 0.4286\n",
      "Epoch 147/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4948 - accuracy: 0.7857 \n",
      "Epoch 147: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 147: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 147: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.4948 - accuracy: 0.7857 - val_loss: 1.0418 - val_accuracy: 0.4286\n",
      "Epoch 148/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.7738 \n",
      "Epoch 148: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 148: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 148: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5243 - accuracy: 0.7738 - val_loss: 1.0474 - val_accuracy: 0.4286\n",
      "Epoch 149/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.7738 \n",
      "Epoch 149: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 149: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 149: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5188 - accuracy: 0.7738 - val_loss: 1.0385 - val_accuracy: 0.4286\n",
      "Epoch 150/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.7976 \n",
      "Epoch 150: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 150: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 150: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4813 - accuracy: 0.7976 - val_loss: 1.0457 - val_accuracy: 0.4286\n",
      "Epoch 151/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.7738 \n",
      "Epoch 151: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 151: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 151: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5034 - accuracy: 0.7738 - val_loss: 1.0425 - val_accuracy: 0.4286\n",
      "Epoch 152/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.7738 \n",
      "Epoch 152: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 152: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 152: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4783 - accuracy: 0.7738 - val_loss: 1.2103 - val_accuracy: 0.4286\n",
      "Epoch 153/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7738 \n",
      "Epoch 153: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 153: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 153: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5300 - accuracy: 0.7738 - val_loss: 1.6676 - val_accuracy: 0.4286\n",
      "Epoch 154/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7976 \n",
      "Epoch 154: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 154: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 154: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5300 - accuracy: 0.7976 - val_loss: 1.4692 - val_accuracy: 0.4286\n",
      "Epoch 155/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.7857 \n",
      "Epoch 155: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 155: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 155: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5354 - accuracy: 0.7857 - val_loss: 1.2497 - val_accuracy: 0.4286\n",
      "Epoch 156/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.7976 \n",
      "Epoch 156: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 156: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 156: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5338 - accuracy: 0.7976 - val_loss: 1.2172 - val_accuracy: 0.4286\n",
      "Epoch 157/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.7857 \n",
      "Epoch 157: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 157: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 157: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5158 - accuracy: 0.7857 - val_loss: 1.2383 - val_accuracy: 0.4286\n",
      "Epoch 158/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.7738 \n",
      "Epoch 158: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 158: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 158: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5060 - accuracy: 0.7738 - val_loss: 1.1824 - val_accuracy: 0.4286\n",
      "Epoch 159/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.7857 \n",
      "Epoch 159: loss did not improve from 0.47600\n",
      "\n",
      "Epoch 159: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 159: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5178 - accuracy: 0.7857 - val_loss: 1.1269 - val_accuracy: 0.4286\n",
      "Epoch 160/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.8095 \n",
      "Epoch 160: loss improved from 0.47600 to 0.46970, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 160: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 160: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4697 - accuracy: 0.8095 - val_loss: 1.0574 - val_accuracy: 0.3810\n",
      "Epoch 161/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.7857 \n",
      "Epoch 161: loss did not improve from 0.46970\n",
      "\n",
      "Epoch 161: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 161: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5100 - accuracy: 0.7857 - val_loss: 1.0625 - val_accuracy: 0.3810\n",
      "Epoch 162/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.7857 \n",
      "Epoch 162: loss did not improve from 0.46970\n",
      "\n",
      "Epoch 162: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 162: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5170 - accuracy: 0.7857 - val_loss: 1.0900 - val_accuracy: 0.3810\n",
      "Epoch 163/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4948 - accuracy: 0.7857 \n",
      "Epoch 163: loss did not improve from 0.46970\n",
      "\n",
      "Epoch 163: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 163: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4948 - accuracy: 0.7857 - val_loss: 1.1427 - val_accuracy: 0.3810\n",
      "Epoch 164/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.7738 \n",
      "Epoch 164: loss improved from 0.46970 to 0.46849, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 164: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 164: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4685 - accuracy: 0.7738 - val_loss: 1.1445 - val_accuracy: 0.4286\n",
      "Epoch 165/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.7976 \n",
      "Epoch 165: loss improved from 0.46849 to 0.46686, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 165: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 165: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4669 - accuracy: 0.7976 - val_loss: 1.1299 - val_accuracy: 0.4286\n",
      "Epoch 166/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.7619 \n",
      "Epoch 166: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 166: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 166: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5238 - accuracy: 0.7619 - val_loss: 1.1219 - val_accuracy: 0.4286\n",
      "Epoch 167/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.7738 \n",
      "Epoch 167: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 167: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 167: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4933 - accuracy: 0.7738 - val_loss: 1.1052 - val_accuracy: 0.3810\n",
      "Epoch 168/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.7738 \n",
      "Epoch 168: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 168: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 168: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4987 - accuracy: 0.7738 - val_loss: 1.0859 - val_accuracy: 0.3810\n",
      "Epoch 169/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.7619 \n",
      "Epoch 169: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 169: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 169: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5000 - accuracy: 0.7619 - val_loss: 1.0514 - val_accuracy: 0.3810\n",
      "Epoch 170/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7738 \n",
      "Epoch 170: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 170: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 170: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4978 - accuracy: 0.7738 - val_loss: 1.1799 - val_accuracy: 0.4286\n",
      "Epoch 171/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.7738 \n",
      "Epoch 171: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 171: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 171: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.4895 - accuracy: 0.7738 - val_loss: 1.2020 - val_accuracy: 0.4286\n",
      "Epoch 172/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7857 \n",
      "Epoch 172: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 172: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 172: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4852 - accuracy: 0.7857 - val_loss: 1.1775 - val_accuracy: 0.3810\n",
      "Epoch 173/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.7857 \n",
      "Epoch 173: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 173: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 173: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4708 - accuracy: 0.7857 - val_loss: 1.1486 - val_accuracy: 0.3810\n",
      "Epoch 174/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.7857 \n",
      "Epoch 174: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 174: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 174: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4895 - accuracy: 0.7857 - val_loss: 1.1199 - val_accuracy: 0.3810\n",
      "Epoch 175/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.7738 \n",
      "Epoch 175: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 175: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 175: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4746 - accuracy: 0.7738 - val_loss: 1.0826 - val_accuracy: 0.3810\n",
      "Epoch 176/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.7738 \n",
      "Epoch 176: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 176: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 176: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4823 - accuracy: 0.7738 - val_loss: 1.0560 - val_accuracy: 0.3810\n",
      "Epoch 177/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.7738 \n",
      "Epoch 177: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 177: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 177: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4711 - accuracy: 0.7738 - val_loss: 1.0688 - val_accuracy: 0.3810\n",
      "Epoch 178/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.7857 \n",
      "Epoch 178: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 178: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 178: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4816 - accuracy: 0.7857 - val_loss: 1.1040 - val_accuracy: 0.3810\n",
      "Epoch 179/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.7738 \n",
      "Epoch 179: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 179: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.96524\n",
      "\n",
      "Epoch 179: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4853 - accuracy: 0.7738 - val_loss: 1.1497 - val_accuracy: 0.4286\n",
      "Epoch 180/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.7857 \n",
      "Epoch 180: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 180: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 180: val_loss improved from 0.96524 to 0.96054, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 180: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4710 - accuracy: 0.7857 - val_loss: 0.9605 - val_accuracy: 0.4286\n",
      "Epoch 181/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.7738 \n",
      "Epoch 181: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 181: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 181: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5102 - accuracy: 0.7738 - val_loss: 0.9950 - val_accuracy: 0.4286\n",
      "Epoch 182/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.7857 \n",
      "Epoch 182: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 182: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 182: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4787 - accuracy: 0.7857 - val_loss: 1.0156 - val_accuracy: 0.4286\n",
      "Epoch 183/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.7857 \n",
      "Epoch 183: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 183: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 183: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4712 - accuracy: 0.7857 - val_loss: 1.0650 - val_accuracy: 0.3810\n",
      "Epoch 184/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.7738 \n",
      "Epoch 184: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 184: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 184: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5039 - accuracy: 0.7738 - val_loss: 1.1015 - val_accuracy: 0.3810\n",
      "Epoch 185/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.7738 \n",
      "Epoch 185: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 185: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 185: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.4878 - accuracy: 0.7738 - val_loss: 1.1006 - val_accuracy: 0.3810\n",
      "Epoch 186/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.7857 \n",
      "Epoch 186: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 186: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 186: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4971 - accuracy: 0.7857 - val_loss: 1.0753 - val_accuracy: 0.3810\n",
      "Epoch 187/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7738 \n",
      "Epoch 187: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 187: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 187: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.4743 - accuracy: 0.7738 - val_loss: 1.0599 - val_accuracy: 0.4286\n",
      "Epoch 188/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.7619 \n",
      "Epoch 188: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 188: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 188: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4755 - accuracy: 0.7619 - val_loss: 1.0615 - val_accuracy: 0.4286\n",
      "Epoch 189/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.7738 \n",
      "Epoch 189: loss did not improve from 0.46686\n",
      "\n",
      "Epoch 189: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 189: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4725 - accuracy: 0.7738 - val_loss: 1.0668 - val_accuracy: 0.4286\n",
      "Epoch 190/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.7738 \n",
      "Epoch 190: loss improved from 0.46686 to 0.45924, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 190: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 190: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4592 - accuracy: 0.7738 - val_loss: 1.0507 - val_accuracy: 0.4286\n",
      "Epoch 191/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4737 - accuracy: 0.7857 \n",
      "Epoch 191: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 191: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 191: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4737 - accuracy: 0.7857 - val_loss: 1.0461 - val_accuracy: 0.4286\n",
      "Epoch 192/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.7500 \n",
      "Epoch 192: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 192: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 192: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4744 - accuracy: 0.7500 - val_loss: 1.0567 - val_accuracy: 0.4286\n",
      "Epoch 193/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.7738 \n",
      "Epoch 193: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 193: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 193: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4769 - accuracy: 0.7738 - val_loss: 1.0568 - val_accuracy: 0.4286\n",
      "Epoch 194/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.7857 \n",
      "Epoch 194: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 194: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 194: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4638 - accuracy: 0.7857 - val_loss: 1.0721 - val_accuracy: 0.3810\n",
      "Epoch 195/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.7738 \n",
      "Epoch 195: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 195: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 195: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4688 - accuracy: 0.7738 - val_loss: 1.0798 - val_accuracy: 0.3810\n",
      "Epoch 196/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.7857 \n",
      "Epoch 196: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 196: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 196: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4701 - accuracy: 0.7857 - val_loss: 1.0859 - val_accuracy: 0.3810\n",
      "Epoch 197/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.7976 \n",
      "Epoch 197: loss did not improve from 0.45924\n",
      "\n",
      "Epoch 197: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 197: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4761 - accuracy: 0.7976 - val_loss: 1.0852 - val_accuracy: 0.4286\n",
      "Epoch 198/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.7619 \n",
      "Epoch 198: loss improved from 0.45924 to 0.45409, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 198: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 198: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4541 - accuracy: 0.7619 - val_loss: 1.0747 - val_accuracy: 0.4286\n",
      "Epoch 199/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.7738 \n",
      "Epoch 199: loss improved from 0.45409 to 0.44910, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 199: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 199: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4491 - accuracy: 0.7738 - val_loss: 1.0799 - val_accuracy: 0.4286\n",
      "Epoch 200/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.7738 \n",
      "Epoch 200: loss did not improve from 0.44910\n",
      "\n",
      "Epoch 200: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 200: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4646 - accuracy: 0.7738 - val_loss: 1.1117 - val_accuracy: 0.4286\n",
      "Epoch 201/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.7976 \n",
      "Epoch 201: loss improved from 0.44910 to 0.44065, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 201: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 201: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.4407 - accuracy: 0.7976 - val_loss: 1.1464 - val_accuracy: 0.4286\n",
      "Epoch 202/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.7857 \n",
      "Epoch 202: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 202: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 202: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4545 - accuracy: 0.7857 - val_loss: 1.1570 - val_accuracy: 0.4286\n",
      "Epoch 203/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7857 \n",
      "Epoch 203: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 203: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 203: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4791 - accuracy: 0.7857 - val_loss: 1.1680 - val_accuracy: 0.4286\n",
      "Epoch 204/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.7738 \n",
      "Epoch 204: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 204: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 204: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4905 - accuracy: 0.7738 - val_loss: 1.1348 - val_accuracy: 0.4286\n",
      "Epoch 205/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.7857 \n",
      "Epoch 205: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 205: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 205: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4921 - accuracy: 0.7857 - val_loss: 1.0959 - val_accuracy: 0.4286\n",
      "Epoch 206/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.7619 \n",
      "Epoch 206: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 206: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 206: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5069 - accuracy: 0.7619 - val_loss: 1.0929 - val_accuracy: 0.3810\n",
      "Epoch 207/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4962 - accuracy: 0.7738 \n",
      "Epoch 207: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 207: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 207: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4962 - accuracy: 0.7738 - val_loss: 1.1256 - val_accuracy: 0.3810\n",
      "Epoch 208/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.7857 \n",
      "Epoch 208: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 208: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 208: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4574 - accuracy: 0.7857 - val_loss: 1.1324 - val_accuracy: 0.3810\n",
      "Epoch 209/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.7976 \n",
      "Epoch 209: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 209: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 209: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4717 - accuracy: 0.7976 - val_loss: 1.1220 - val_accuracy: 0.3810\n",
      "Epoch 210/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.7976 \n",
      "Epoch 210: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 210: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 210: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4444 - accuracy: 0.7976 - val_loss: 1.1075 - val_accuracy: 0.3810\n",
      "Epoch 211/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.7857 \n",
      "Epoch 211: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 211: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 211: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4638 - accuracy: 0.7857 - val_loss: 1.0993 - val_accuracy: 0.3810\n",
      "Epoch 212/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.7619 \n",
      "Epoch 212: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 212: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 212: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4771 - accuracy: 0.7619 - val_loss: 1.1079 - val_accuracy: 0.3810\n",
      "Epoch 213/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.8095 \n",
      "Epoch 213: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 213: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 213: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4598 - accuracy: 0.8095 - val_loss: 1.1164 - val_accuracy: 0.4286\n",
      "Epoch 214/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.7976 \n",
      "Epoch 214: loss did not improve from 0.44065\n",
      "\n",
      "Epoch 214: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 214: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4657 - accuracy: 0.7976 - val_loss: 1.1259 - val_accuracy: 0.4286\n",
      "Epoch 215/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.7976 \n",
      "Epoch 215: loss improved from 0.44065 to 0.43550, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 215: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 215: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.4355 - accuracy: 0.7976 - val_loss: 1.1397 - val_accuracy: 0.4286\n",
      "Epoch 216/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.7857 \n",
      "Epoch 216: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 216: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 216: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4645 - accuracy: 0.7857 - val_loss: 1.1493 - val_accuracy: 0.4286\n",
      "Epoch 217/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.7619 \n",
      "Epoch 217: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 217: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 217: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4810 - accuracy: 0.7619 - val_loss: 1.1721 - val_accuracy: 0.4286\n",
      "Epoch 218/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.7381 \n",
      "Epoch 218: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 218: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 218: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.5258 - accuracy: 0.7381 - val_loss: 1.1260 - val_accuracy: 0.4286\n",
      "Epoch 219/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.7619 \n",
      "Epoch 219: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 219: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 219: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5063 - accuracy: 0.7619 - val_loss: 1.0869 - val_accuracy: 0.3810\n",
      "Epoch 220/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.7500 \n",
      "Epoch 220: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 220: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 220: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5156 - accuracy: 0.7500 - val_loss: 1.0554 - val_accuracy: 0.3810\n",
      "Epoch 221/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.7738 \n",
      "Epoch 221: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 221: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 221: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 46s 17s/step - loss: 0.5309 - accuracy: 0.7738 - val_loss: 1.0439 - val_accuracy: 0.3810\n",
      "Epoch 222/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.7619 \n",
      "Epoch 222: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 222: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 222: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5127 - accuracy: 0.7619 - val_loss: 1.0774 - val_accuracy: 0.3810\n",
      "Epoch 223/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.7976 \n",
      "Epoch 223: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 223: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 223: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.4795 - accuracy: 0.7976 - val_loss: 1.0955 - val_accuracy: 0.3810\n",
      "Epoch 224/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.7738 \n",
      "Epoch 224: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 224: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 224: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.4886 - accuracy: 0.7738 - val_loss: 1.1315 - val_accuracy: 0.3810\n",
      "Epoch 225/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.7857 \n",
      "Epoch 225: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 225: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 225: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4734 - accuracy: 0.7857 - val_loss: 1.1285 - val_accuracy: 0.3810\n",
      "Epoch 226/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.7857 \n",
      "Epoch 226: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 226: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 226: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4655 - accuracy: 0.7857 - val_loss: 1.1409 - val_accuracy: 0.4286\n",
      "Epoch 227/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.7857 \n",
      "Epoch 227: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 227: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 227: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4790 - accuracy: 0.7857 - val_loss: 1.1320 - val_accuracy: 0.4286\n",
      "Epoch 228/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.7857 \n",
      "Epoch 228: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 228: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 228: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4723 - accuracy: 0.7857 - val_loss: 1.1450 - val_accuracy: 0.3810\n",
      "Epoch 229/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.7619 \n",
      "Epoch 229: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 229: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 229: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4582 - accuracy: 0.7619 - val_loss: 1.1440 - val_accuracy: 0.3810\n",
      "Epoch 230/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.7738 \n",
      "Epoch 230: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 230: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 230: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4869 - accuracy: 0.7738 - val_loss: 1.1306 - val_accuracy: 0.4286\n",
      "Epoch 231/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.7619 \n",
      "Epoch 231: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 231: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 231: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 47s 18s/step - loss: 0.5429 - accuracy: 0.7619 - val_loss: 1.0981 - val_accuracy: 0.4286\n",
      "Epoch 232/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7381 \n",
      "Epoch 232: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 232: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 232: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5424 - accuracy: 0.7381 - val_loss: 1.1174 - val_accuracy: 0.3333\n",
      "Epoch 233/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.7619 \n",
      "Epoch 233: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 233: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 233: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5283 - accuracy: 0.7619 - val_loss: 1.1280 - val_accuracy: 0.4286\n",
      "Epoch 234/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7381 \n",
      "Epoch 234: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 234: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 234: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5394 - accuracy: 0.7381 - val_loss: 1.0947 - val_accuracy: 0.3810\n",
      "Epoch 235/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.7262 \n",
      "Epoch 235: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 235: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 235: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5673 - accuracy: 0.7262 - val_loss: 1.1156 - val_accuracy: 0.3810\n",
      "Epoch 236/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.7500 \n",
      "Epoch 236: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 236: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 236: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5176 - accuracy: 0.7500 - val_loss: 1.1212 - val_accuracy: 0.3810\n",
      "Epoch 237/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7381 \n",
      "Epoch 237: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 237: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 237: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.4791 - accuracy: 0.7381 - val_loss: 1.1306 - val_accuracy: 0.3810\n",
      "Epoch 238/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.7500 \n",
      "Epoch 238: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 238: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 238: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 19s/step - loss: 0.5108 - accuracy: 0.7500 - val_loss: 1.2112 - val_accuracy: 0.3333\n",
      "Epoch 239/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.7381 \n",
      "Epoch 239: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 239: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 239: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5405 - accuracy: 0.7381 - val_loss: 1.2364 - val_accuracy: 0.3333\n",
      "Epoch 240/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.7381 \n",
      "Epoch 240: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 240: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 240: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5279 - accuracy: 0.7381 - val_loss: 1.0609 - val_accuracy: 0.3810\n",
      "Epoch 241/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7262 \n",
      "Epoch 241: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 241: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 241: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5288 - accuracy: 0.7262 - val_loss: 1.0725 - val_accuracy: 0.4286\n",
      "Epoch 242/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.7857 \n",
      "Epoch 242: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 242: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 242: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4690 - accuracy: 0.7857 - val_loss: 1.0781 - val_accuracy: 0.4286\n",
      "Epoch 243/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.7500 \n",
      "Epoch 243: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 243: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 243: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.4509 - accuracy: 0.7500 - val_loss: 1.0471 - val_accuracy: 0.3810\n",
      "Epoch 244/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.7976 \n",
      "Epoch 244: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 244: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 244: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4455 - accuracy: 0.7976 - val_loss: 1.0568 - val_accuracy: 0.3810\n",
      "Epoch 245/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.7857 \n",
      "Epoch 245: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 245: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 245: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4587 - accuracy: 0.7857 - val_loss: 1.0309 - val_accuracy: 0.3810\n",
      "Epoch 246/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4765 - accuracy: 0.7857 \n",
      "Epoch 246: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 246: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 246: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4765 - accuracy: 0.7857 - val_loss: 1.0236 - val_accuracy: 0.4286\n",
      "Epoch 247/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.7976 \n",
      "Epoch 247: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 247: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 247: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4412 - accuracy: 0.7976 - val_loss: 1.0391 - val_accuracy: 0.4286\n",
      "Epoch 248/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4433 - accuracy: 0.7976 \n",
      "Epoch 248: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 248: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 248: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4433 - accuracy: 0.7976 - val_loss: 1.0736 - val_accuracy: 0.4286\n",
      "Epoch 249/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.7857 \n",
      "Epoch 249: loss did not improve from 0.43550\n",
      "\n",
      "Epoch 249: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 249: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5047 - accuracy: 0.7857 - val_loss: 1.0845 - val_accuracy: 0.4286\n",
      "Epoch 250/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.7857 \n",
      "Epoch 250: loss improved from 0.43550 to 0.39835, saving model to best_model_loss.h5\n",
      "\n",
      "Epoch 250: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 250: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.3983 - accuracy: 0.7857 - val_loss: 0.9878 - val_accuracy: 0.4286\n",
      "Epoch 251/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4383 - accuracy: 0.7976 \n",
      "Epoch 251: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 251: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 251: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.4383 - accuracy: 0.7976 - val_loss: 0.9780 - val_accuracy: 0.4286\n",
      "Epoch 252/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.7976 \n",
      "Epoch 252: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 252: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 252: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4483 - accuracy: 0.7976 - val_loss: 0.9698 - val_accuracy: 0.3810\n",
      "Epoch 253/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7500 \n",
      "Epoch 253: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 253: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 253: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5645 - accuracy: 0.7500 - val_loss: 1.0544 - val_accuracy: 0.4286\n",
      "Epoch 254/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6527 - accuracy: 0.6905 \n",
      "Epoch 254: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 254: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 254: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6527 - accuracy: 0.6905 - val_loss: 1.5781 - val_accuracy: 0.2857\n",
      "Epoch 255/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.6071 \n",
      "Epoch 255: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 255: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 255: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8272 - accuracy: 0.6071 - val_loss: 1.3022 - val_accuracy: 0.2381\n",
      "Epoch 256/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8457 - accuracy: 0.5833 \n",
      "Epoch 256: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 256: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 256: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8457 - accuracy: 0.5833 - val_loss: 1.0501 - val_accuracy: 0.2857\n",
      "Epoch 257/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.6905 \n",
      "Epoch 257: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 257: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 257: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7041 - accuracy: 0.6905 - val_loss: 0.9817 - val_accuracy: 0.3333\n",
      "Epoch 258/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8520 - accuracy: 0.5714 \n",
      "Epoch 258: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 258: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 258: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8520 - accuracy: 0.5714 - val_loss: 1.0835 - val_accuracy: 0.4286\n",
      "Epoch 259/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8012 - accuracy: 0.6548 \n",
      "Epoch 259: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 259: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 259: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8012 - accuracy: 0.6548 - val_loss: 1.0488 - val_accuracy: 0.3810\n",
      "Epoch 260/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8575 - accuracy: 0.6667 \n",
      "Epoch 260: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 260: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 260: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8575 - accuracy: 0.6667 - val_loss: 1.3829 - val_accuracy: 0.2857\n",
      "Epoch 261/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0112 - accuracy: 0.5238 \n",
      "Epoch 261: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 261: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 261: val_accuracy did not improve from 0.42857\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.0112 - accuracy: 0.5238 - val_loss: 1.0951 - val_accuracy: 0.2857\n",
      "Epoch 262/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0199 - accuracy: 0.4762 \n",
      "Epoch 262: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 262: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 262: val_accuracy improved from 0.42857 to 0.52381, saving model to best_model_val_acc.h5\n",
      "3/3 [==============================] - 50s 19s/step - loss: 1.0199 - accuracy: 0.4762 - val_loss: 0.9972 - val_accuracy: 0.5238\n",
      "Epoch 263/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0541 - accuracy: 0.3333 \n",
      "Epoch 263: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 263: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 263: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.0541 - accuracy: 0.3333 - val_loss: 0.9705 - val_accuracy: 0.5238\n",
      "Epoch 264/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0350 - accuracy: 0.4524 \n",
      "Epoch 264: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 264: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 264: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.0350 - accuracy: 0.4524 - val_loss: 0.9857 - val_accuracy: 0.2857\n",
      "Epoch 265/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0225 - accuracy: 0.4762 \n",
      "Epoch 265: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 265: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 265: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.0225 - accuracy: 0.4762 - val_loss: 1.0145 - val_accuracy: 0.2857\n",
      "Epoch 266/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9629 - accuracy: 0.5119 \n",
      "Epoch 266: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 266: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 266: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.9629 - accuracy: 0.5119 - val_loss: 1.0134 - val_accuracy: 0.2857\n",
      "Epoch 267/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8911 - accuracy: 0.5357 \n",
      "Epoch 267: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 267: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.96054\n",
      "\n",
      "Epoch 267: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8911 - accuracy: 0.5357 - val_loss: 0.9665 - val_accuracy: 0.4286\n",
      "Epoch 268/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.5119 \n",
      "Epoch 268: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 268: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 268: val_loss improved from 0.96054 to 0.91571, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 268: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.9121 - accuracy: 0.5119 - val_loss: 0.9157 - val_accuracy: 0.4762\n",
      "Epoch 269/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0229 - accuracy: 0.4643 \n",
      "Epoch 269: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 269: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 269: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.0229 - accuracy: 0.4643 - val_loss: 1.1266 - val_accuracy: 0.2857\n",
      "Epoch 270/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9380 - accuracy: 0.5238 \n",
      "Epoch 270: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 270: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 270: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.9380 - accuracy: 0.5238 - val_loss: 1.1248 - val_accuracy: 0.3810\n",
      "Epoch 271/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.5119 \n",
      "Epoch 271: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 271: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 271: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.9268 - accuracy: 0.5119 - val_loss: 1.0480 - val_accuracy: 0.4286\n",
      "Epoch 272/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8992 - accuracy: 0.6190 \n",
      "Epoch 272: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 272: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 272: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8992 - accuracy: 0.6190 - val_loss: 1.0805 - val_accuracy: 0.4286\n",
      "Epoch 273/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9033 - accuracy: 0.5833 \n",
      "Epoch 273: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 273: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 273: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.9033 - accuracy: 0.5833 - val_loss: 1.0886 - val_accuracy: 0.4286\n",
      "Epoch 274/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8747 - accuracy: 0.6071 \n",
      "Epoch 274: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 274: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 274: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8747 - accuracy: 0.6071 - val_loss: 1.0381 - val_accuracy: 0.4286\n",
      "Epoch 275/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8381 - accuracy: 0.5714 \n",
      "Epoch 275: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 275: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 275: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8381 - accuracy: 0.5714 - val_loss: 1.0680 - val_accuracy: 0.4286\n",
      "Epoch 276/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8525 - accuracy: 0.6071 \n",
      "Epoch 276: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 276: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 276: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8525 - accuracy: 0.6071 - val_loss: 1.0814 - val_accuracy: 0.4286\n",
      "Epoch 277/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.6071 \n",
      "Epoch 277: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 277: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 277: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8122 - accuracy: 0.6071 - val_loss: 1.0615 - val_accuracy: 0.3810\n",
      "Epoch 278/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.6429 \n",
      "Epoch 278: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 278: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 278: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8259 - accuracy: 0.6429 - val_loss: 0.9678 - val_accuracy: 0.4286\n",
      "Epoch 279/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8433 - accuracy: 0.6071 \n",
      "Epoch 279: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 279: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 279: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8433 - accuracy: 0.6071 - val_loss: 0.9846 - val_accuracy: 0.4286\n",
      "Epoch 280/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8321 - accuracy: 0.6071 \n",
      "Epoch 280: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 280: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 280: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8321 - accuracy: 0.6071 - val_loss: 1.0820 - val_accuracy: 0.4286\n",
      "Epoch 281/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8173 - accuracy: 0.5952 \n",
      "Epoch 281: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 281: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 281: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8173 - accuracy: 0.5952 - val_loss: 1.1561 - val_accuracy: 0.3333\n",
      "Epoch 282/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.5714 \n",
      "Epoch 282: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 282: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 282: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8632 - accuracy: 0.5714 - val_loss: 1.1741 - val_accuracy: 0.3333\n",
      "Epoch 283/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8512 - accuracy: 0.6071 \n",
      "Epoch 283: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 283: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 283: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8512 - accuracy: 0.6071 - val_loss: 1.1374 - val_accuracy: 0.3333\n",
      "Epoch 284/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8506 - accuracy: 0.6071 \n",
      "Epoch 284: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 284: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 284: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8506 - accuracy: 0.6071 - val_loss: 1.0676 - val_accuracy: 0.3810\n",
      "Epoch 285/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.6429 \n",
      "Epoch 285: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 285: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 285: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8103 - accuracy: 0.6429 - val_loss: 1.0085 - val_accuracy: 0.4286\n",
      "Epoch 286/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8237 - accuracy: 0.6429 \n",
      "Epoch 286: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 286: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 286: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8237 - accuracy: 0.6429 - val_loss: 0.9835 - val_accuracy: 0.4286\n",
      "Epoch 287/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.6071 \n",
      "Epoch 287: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 287: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 287: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8306 - accuracy: 0.6071 - val_loss: 1.0098 - val_accuracy: 0.4286\n",
      "Epoch 288/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.6310 \n",
      "Epoch 288: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 288: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 288: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8085 - accuracy: 0.6310 - val_loss: 1.0094 - val_accuracy: 0.4286\n",
      "Epoch 289/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8218 - accuracy: 0.6190 \n",
      "Epoch 289: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 289: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 289: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8218 - accuracy: 0.6190 - val_loss: 0.9934 - val_accuracy: 0.4286\n",
      "Epoch 290/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8138 - accuracy: 0.6190 \n",
      "Epoch 290: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 290: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 290: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8138 - accuracy: 0.6190 - val_loss: 1.0044 - val_accuracy: 0.4286\n",
      "Epoch 291/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7759 - accuracy: 0.6429 \n",
      "Epoch 291: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 291: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 291: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7759 - accuracy: 0.6429 - val_loss: 1.0348 - val_accuracy: 0.4286\n",
      "Epoch 292/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.5595 \n",
      "Epoch 292: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 292: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 292: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.9055 - accuracy: 0.5595 - val_loss: 1.1109 - val_accuracy: 0.4286\n",
      "Epoch 293/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8624 - accuracy: 0.5952 \n",
      "Epoch 293: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 293: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 293: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8624 - accuracy: 0.5952 - val_loss: 1.1688 - val_accuracy: 0.4286\n",
      "Epoch 294/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7920 - accuracy: 0.6429 \n",
      "Epoch 294: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 294: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 294: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7920 - accuracy: 0.6429 - val_loss: 1.1226 - val_accuracy: 0.4286\n",
      "Epoch 295/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.6667 \n",
      "Epoch 295: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 295: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 295: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7436 - accuracy: 0.6667 - val_loss: 1.1297 - val_accuracy: 0.4286\n",
      "Epoch 296/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8025 - accuracy: 0.6667 \n",
      "Epoch 296: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 296: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 296: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8025 - accuracy: 0.6667 - val_loss: 1.1476 - val_accuracy: 0.4286\n",
      "Epoch 297/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8199 - accuracy: 0.6548 \n",
      "Epoch 297: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 297: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 297: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8199 - accuracy: 0.6548 - val_loss: 1.1368 - val_accuracy: 0.4286\n",
      "Epoch 298/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7859 - accuracy: 0.6310 \n",
      "Epoch 298: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 298: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 298: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7859 - accuracy: 0.6310 - val_loss: 1.1151 - val_accuracy: 0.4286\n",
      "Epoch 299/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7869 - accuracy: 0.6786 \n",
      "Epoch 299: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 299: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 299: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7869 - accuracy: 0.6786 - val_loss: 1.1093 - val_accuracy: 0.4286\n",
      "Epoch 300/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.6429 \n",
      "Epoch 300: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 300: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 300: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8222 - accuracy: 0.6429 - val_loss: 1.0305 - val_accuracy: 0.4286\n",
      "Epoch 301/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.6190 \n",
      "Epoch 301: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 301: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 301: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8193 - accuracy: 0.6190 - val_loss: 1.0395 - val_accuracy: 0.4286\n",
      "Epoch 302/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7434 - accuracy: 0.6905 \n",
      "Epoch 302: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 302: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 302: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7434 - accuracy: 0.6905 - val_loss: 1.0537 - val_accuracy: 0.4286\n",
      "Epoch 303/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7727 - accuracy: 0.6548 \n",
      "Epoch 303: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 303: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 303: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7727 - accuracy: 0.6548 - val_loss: 1.0437 - val_accuracy: 0.4286\n",
      "Epoch 304/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9052 - accuracy: 0.5952 \n",
      "Epoch 304: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 304: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 304: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.9052 - accuracy: 0.5952 - val_loss: 1.1113 - val_accuracy: 0.4286\n",
      "Epoch 305/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.6190 \n",
      "Epoch 305: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 305: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 305: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8256 - accuracy: 0.6190 - val_loss: 1.2099 - val_accuracy: 0.3333\n",
      "Epoch 306/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.5952 \n",
      "Epoch 306: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 306: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 306: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8642 - accuracy: 0.5952 - val_loss: 1.1831 - val_accuracy: 0.3333\n",
      "Epoch 307/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.5952 \n",
      "Epoch 307: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 307: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 307: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8676 - accuracy: 0.5952 - val_loss: 1.1269 - val_accuracy: 0.3333\n",
      "Epoch 308/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.5952 \n",
      "Epoch 308: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 308: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 308: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8223 - accuracy: 0.5952 - val_loss: 1.0814 - val_accuracy: 0.3810\n",
      "Epoch 309/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.6190 \n",
      "Epoch 309: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 309: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 309: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8175 - accuracy: 0.6190 - val_loss: 1.0317 - val_accuracy: 0.4286\n",
      "Epoch 310/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7889 - accuracy: 0.6429 \n",
      "Epoch 310: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 310: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 310: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7889 - accuracy: 0.6429 - val_loss: 1.0074 - val_accuracy: 0.4286\n",
      "Epoch 311/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.6667 \n",
      "Epoch 311: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 311: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 311: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7934 - accuracy: 0.6667 - val_loss: 1.0049 - val_accuracy: 0.4286\n",
      "Epoch 312/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7967 - accuracy: 0.6310 \n",
      "Epoch 312: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 312: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 312: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7967 - accuracy: 0.6310 - val_loss: 1.0057 - val_accuracy: 0.4286\n",
      "Epoch 313/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7752 - accuracy: 0.6667 \n",
      "Epoch 313: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 313: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 313: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7752 - accuracy: 0.6667 - val_loss: 1.0198 - val_accuracy: 0.4286\n",
      "Epoch 314/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8001 - accuracy: 0.6429 \n",
      "Epoch 314: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 314: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 314: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8001 - accuracy: 0.6429 - val_loss: 1.0342 - val_accuracy: 0.4286\n",
      "Epoch 315/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8173 - accuracy: 0.6429 \n",
      "Epoch 315: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 315: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 315: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8173 - accuracy: 0.6429 - val_loss: 1.0605 - val_accuracy: 0.4286\n",
      "Epoch 316/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.6429 \n",
      "Epoch 316: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 316: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 316: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7811 - accuracy: 0.6429 - val_loss: 1.0821 - val_accuracy: 0.4286\n",
      "Epoch 317/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8075 - accuracy: 0.6786 \n",
      "Epoch 317: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 317: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 317: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8075 - accuracy: 0.6786 - val_loss: 1.0773 - val_accuracy: 0.4286\n",
      "Epoch 318/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.6667 \n",
      "Epoch 318: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 318: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 318: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8081 - accuracy: 0.6667 - val_loss: 1.0533 - val_accuracy: 0.4286\n",
      "Epoch 319/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.6667 \n",
      "Epoch 319: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 319: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 319: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7579 - accuracy: 0.6667 - val_loss: 1.0244 - val_accuracy: 0.4286\n",
      "Epoch 320/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7641 - accuracy: 0.6667 \n",
      "Epoch 320: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 320: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 320: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7641 - accuracy: 0.6667 - val_loss: 1.0236 - val_accuracy: 0.4286\n",
      "Epoch 321/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.6667 \n",
      "Epoch 321: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 321: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 321: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7649 - accuracy: 0.6667 - val_loss: 1.0169 - val_accuracy: 0.4286\n",
      "Epoch 322/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.6905 \n",
      "Epoch 322: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 322: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 322: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7736 - accuracy: 0.6905 - val_loss: 1.0057 - val_accuracy: 0.4286\n",
      "Epoch 323/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7473 - accuracy: 0.6786 \n",
      "Epoch 323: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 323: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 323: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7473 - accuracy: 0.6786 - val_loss: 0.9983 - val_accuracy: 0.4286\n",
      "Epoch 324/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.6786 \n",
      "Epoch 324: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 324: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 324: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7391 - accuracy: 0.6786 - val_loss: 0.9963 - val_accuracy: 0.4286\n",
      "Epoch 325/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7787 - accuracy: 0.6786 \n",
      "Epoch 325: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 325: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 325: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7787 - accuracy: 0.6786 - val_loss: 0.9938 - val_accuracy: 0.4286\n",
      "Epoch 326/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7500 - accuracy: 0.6667 \n",
      "Epoch 326: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 326: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 326: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.7500 - accuracy: 0.6667 - val_loss: 0.9999 - val_accuracy: 0.4286\n",
      "Epoch 327/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7614 - accuracy: 0.6786 \n",
      "Epoch 327: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 327: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 327: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7614 - accuracy: 0.6786 - val_loss: 1.0025 - val_accuracy: 0.4286\n",
      "Epoch 328/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7787 - accuracy: 0.6905 \n",
      "Epoch 328: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 328: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 328: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7787 - accuracy: 0.6905 - val_loss: 1.0115 - val_accuracy: 0.4286\n",
      "Epoch 329/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7350 - accuracy: 0.6905 \n",
      "Epoch 329: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 329: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 329: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7350 - accuracy: 0.6905 - val_loss: 1.0275 - val_accuracy: 0.4286\n",
      "Epoch 330/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7479 - accuracy: 0.6905 \n",
      "Epoch 330: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 330: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 330: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7479 - accuracy: 0.6905 - val_loss: 1.0348 - val_accuracy: 0.4286\n",
      "Epoch 331/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7580 - accuracy: 0.6786 \n",
      "Epoch 331: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 331: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 331: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7580 - accuracy: 0.6786 - val_loss: 1.0280 - val_accuracy: 0.4286\n",
      "Epoch 332/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7263 - accuracy: 0.6905 \n",
      "Epoch 332: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 332: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 332: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7263 - accuracy: 0.6905 - val_loss: 1.0310 - val_accuracy: 0.4286\n",
      "Epoch 333/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.7024 \n",
      "Epoch 333: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 333: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 333: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7227 - accuracy: 0.7024 - val_loss: 1.0236 - val_accuracy: 0.4286\n",
      "Epoch 334/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7319 - accuracy: 0.7024 \n",
      "Epoch 334: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 334: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 334: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7319 - accuracy: 0.7024 - val_loss: 1.0096 - val_accuracy: 0.4286\n",
      "Epoch 335/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7702 - accuracy: 0.6786 \n",
      "Epoch 335: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 335: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 335: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7702 - accuracy: 0.6786 - val_loss: 1.0208 - val_accuracy: 0.4286\n",
      "Epoch 336/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7963 - accuracy: 0.6548 \n",
      "Epoch 336: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 336: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 336: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7963 - accuracy: 0.6548 - val_loss: 1.0233 - val_accuracy: 0.4286\n",
      "Epoch 337/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7472 - accuracy: 0.6667 \n",
      "Epoch 337: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 337: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 337: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7472 - accuracy: 0.6667 - val_loss: 1.0195 - val_accuracy: 0.4286\n",
      "Epoch 338/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7389 - accuracy: 0.6905 \n",
      "Epoch 338: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 338: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 338: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7389 - accuracy: 0.6905 - val_loss: 1.0225 - val_accuracy: 0.4286\n",
      "Epoch 339/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7485 - accuracy: 0.6905 \n",
      "Epoch 339: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 339: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 339: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7485 - accuracy: 0.6905 - val_loss: 1.0252 - val_accuracy: 0.4286\n",
      "Epoch 340/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.6548 \n",
      "Epoch 340: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 340: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 340: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7721 - accuracy: 0.6548 - val_loss: 1.0273 - val_accuracy: 0.4286\n",
      "Epoch 341/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7680 - accuracy: 0.6548 \n",
      "Epoch 341: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 341: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 341: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7680 - accuracy: 0.6548 - val_loss: 1.0200 - val_accuracy: 0.4286\n",
      "Epoch 342/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.6905 \n",
      "Epoch 342: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 342: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 342: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7424 - accuracy: 0.6905 - val_loss: 1.0082 - val_accuracy: 0.4286\n",
      "Epoch 343/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7467 - accuracy: 0.6905 \n",
      "Epoch 343: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 343: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 343: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7467 - accuracy: 0.6905 - val_loss: 1.0077 - val_accuracy: 0.4286\n",
      "Epoch 344/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7467 - accuracy: 0.6905 \n",
      "Epoch 344: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 344: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 344: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7467 - accuracy: 0.6905 - val_loss: 1.0046 - val_accuracy: 0.4286\n",
      "Epoch 345/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7487 - accuracy: 0.6905 \n",
      "Epoch 345: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 345: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 345: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7487 - accuracy: 0.6905 - val_loss: 1.0048 - val_accuracy: 0.4286\n",
      "Epoch 346/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.6786 \n",
      "Epoch 346: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 346: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 346: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7337 - accuracy: 0.6786 - val_loss: 0.9909 - val_accuracy: 0.4286\n",
      "Epoch 347/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.6905 \n",
      "Epoch 347: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 347: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 347: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7376 - accuracy: 0.6905 - val_loss: 0.9952 - val_accuracy: 0.4286\n",
      "Epoch 348/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7515 - accuracy: 0.6786 \n",
      "Epoch 348: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 348: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 348: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7515 - accuracy: 0.6786 - val_loss: 0.9916 - val_accuracy: 0.4286\n",
      "Epoch 349/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.6905 \n",
      "Epoch 349: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 349: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 349: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7245 - accuracy: 0.6905 - val_loss: 0.9956 - val_accuracy: 0.4286\n",
      "Epoch 350/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.6905 \n",
      "Epoch 350: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 350: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 350: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7148 - accuracy: 0.6905 - val_loss: 1.0049 - val_accuracy: 0.4286\n",
      "Epoch 351/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.6786 \n",
      "Epoch 351: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 351: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 351: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7480 - accuracy: 0.6786 - val_loss: 1.0254 - val_accuracy: 0.4286\n",
      "Epoch 352/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.6905 \n",
      "Epoch 352: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 352: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 352: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7436 - accuracy: 0.6905 - val_loss: 1.0318 - val_accuracy: 0.4286\n",
      "Epoch 353/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.6667 \n",
      "Epoch 353: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 353: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 353: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7337 - accuracy: 0.6667 - val_loss: 1.0296 - val_accuracy: 0.4286\n",
      "Epoch 354/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.6905 \n",
      "Epoch 354: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 354: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 354: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7341 - accuracy: 0.6905 - val_loss: 1.0334 - val_accuracy: 0.4286\n",
      "Epoch 355/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7462 - accuracy: 0.6786 \n",
      "Epoch 355: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 355: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 355: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7462 - accuracy: 0.6786 - val_loss: 1.0381 - val_accuracy: 0.4286\n",
      "Epoch 356/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.6429 \n",
      "Epoch 356: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 356: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 356: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8085 - accuracy: 0.6429 - val_loss: 1.1820 - val_accuracy: 0.3333\n",
      "Epoch 357/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.5357 \n",
      "Epoch 357: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 357: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 357: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.9436 - accuracy: 0.5357 - val_loss: 1.2096 - val_accuracy: 0.2381\n",
      "Epoch 358/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9462 - accuracy: 0.5000 \n",
      "Epoch 358: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 358: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 358: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 53s 20s/step - loss: 0.9462 - accuracy: 0.5000 - val_loss: 1.1595 - val_accuracy: 0.3333\n",
      "Epoch 359/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9293 - accuracy: 0.5119 \n",
      "Epoch 359: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 359: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 359: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.9293 - accuracy: 0.5119 - val_loss: 1.0525 - val_accuracy: 0.3333\n",
      "Epoch 360/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8560 - accuracy: 0.5595 \n",
      "Epoch 360: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 360: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.91571\n",
      "\n",
      "Epoch 360: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8560 - accuracy: 0.5595 - val_loss: 0.9662 - val_accuracy: 0.4286\n",
      "Epoch 361/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8056 - accuracy: 0.6429 \n",
      "Epoch 361: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 361: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 361: val_loss improved from 0.91571 to 0.91272, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 361: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8056 - accuracy: 0.6429 - val_loss: 0.9127 - val_accuracy: 0.4286\n",
      "Epoch 362/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7584 - accuracy: 0.6905 \n",
      "Epoch 362: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 362: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 362: val_loss improved from 0.91272 to 0.90324, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 362: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7584 - accuracy: 0.6905 - val_loss: 0.9032 - val_accuracy: 0.4286\n",
      "Epoch 363/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.6310 \n",
      "Epoch 363: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 363: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 363: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8210 - accuracy: 0.6310 - val_loss: 0.9134 - val_accuracy: 0.4286\n",
      "Epoch 364/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7696 - accuracy: 0.6667 \n",
      "Epoch 364: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 364: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 364: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7696 - accuracy: 0.6667 - val_loss: 0.9310 - val_accuracy: 0.4286\n",
      "Epoch 365/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7612 - accuracy: 0.6905 \n",
      "Epoch 365: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 365: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 365: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7612 - accuracy: 0.6905 - val_loss: 0.9532 - val_accuracy: 0.4286\n",
      "Epoch 366/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7768 - accuracy: 0.6667 \n",
      "Epoch 366: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 366: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 366: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7768 - accuracy: 0.6667 - val_loss: 0.9662 - val_accuracy: 0.4286\n",
      "Epoch 367/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7513 - accuracy: 0.6905 \n",
      "Epoch 367: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 367: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 367: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7513 - accuracy: 0.6905 - val_loss: 0.9669 - val_accuracy: 0.4286\n",
      "Epoch 368/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.6786 \n",
      "Epoch 368: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 368: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 368: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7340 - accuracy: 0.6786 - val_loss: 0.9841 - val_accuracy: 0.4286\n",
      "Epoch 369/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7127 - accuracy: 0.6905 \n",
      "Epoch 369: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 369: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 369: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7127 - accuracy: 0.6905 - val_loss: 0.9759 - val_accuracy: 0.4286\n",
      "Epoch 370/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7734 - accuracy: 0.6548 \n",
      "Epoch 370: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 370: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 370: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7734 - accuracy: 0.6548 - val_loss: 1.0112 - val_accuracy: 0.4286\n",
      "Epoch 371/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.6905 \n",
      "Epoch 371: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 371: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 371: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7392 - accuracy: 0.6905 - val_loss: 1.0367 - val_accuracy: 0.4286\n",
      "Epoch 372/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.7024 \n",
      "Epoch 372: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 372: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 372: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7209 - accuracy: 0.7024 - val_loss: 1.0169 - val_accuracy: 0.4286\n",
      "Epoch 373/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7055 - accuracy: 0.6905 \n",
      "Epoch 373: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 373: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 373: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7055 - accuracy: 0.6905 - val_loss: 1.0191 - val_accuracy: 0.4286\n",
      "Epoch 374/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.6905 \n",
      "Epoch 374: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 374: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 374: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7428 - accuracy: 0.6905 - val_loss: 1.0372 - val_accuracy: 0.4286\n",
      "Epoch 375/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7286 - accuracy: 0.6905 \n",
      "Epoch 375: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 375: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 375: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7286 - accuracy: 0.6905 - val_loss: 1.0306 - val_accuracy: 0.4286\n",
      "Epoch 376/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.6905 \n",
      "Epoch 376: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 376: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 376: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7312 - accuracy: 0.6905 - val_loss: 1.0306 - val_accuracy: 0.4286\n",
      "Epoch 377/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7314 - accuracy: 0.6905 \n",
      "Epoch 377: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 377: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 377: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 53s 20s/step - loss: 0.7314 - accuracy: 0.6905 - val_loss: 1.0313 - val_accuracy: 0.4286\n",
      "Epoch 378/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7299 - accuracy: 0.6905 \n",
      "Epoch 378: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 378: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 378: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 53s 20s/step - loss: 0.7299 - accuracy: 0.6905 - val_loss: 1.0377 - val_accuracy: 0.4286\n",
      "Epoch 379/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.6905 \n",
      "Epoch 379: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 379: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 379: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7420 - accuracy: 0.6905 - val_loss: 1.0422 - val_accuracy: 0.4286\n",
      "Epoch 380/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7237 - accuracy: 0.6905 \n",
      "Epoch 380: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 380: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 380: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7237 - accuracy: 0.6905 - val_loss: 1.0430 - val_accuracy: 0.4286\n",
      "Epoch 381/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.6905 \n",
      "Epoch 381: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 381: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 381: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7408 - accuracy: 0.6905 - val_loss: 1.0430 - val_accuracy: 0.4286\n",
      "Epoch 382/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7599 - accuracy: 0.6905 \n",
      "Epoch 382: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 382: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 382: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7599 - accuracy: 0.6905 - val_loss: 1.0473 - val_accuracy: 0.4286\n",
      "Epoch 383/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7119 - accuracy: 0.6905 \n",
      "Epoch 383: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 383: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 383: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7119 - accuracy: 0.6905 - val_loss: 1.0247 - val_accuracy: 0.4286\n",
      "Epoch 384/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7554 - accuracy: 0.6667 \n",
      "Epoch 384: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 384: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 384: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7554 - accuracy: 0.6667 - val_loss: 1.0308 - val_accuracy: 0.4286\n",
      "Epoch 385/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.6905 \n",
      "Epoch 385: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 385: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 385: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7235 - accuracy: 0.6905 - val_loss: 1.0217 - val_accuracy: 0.4286\n",
      "Epoch 386/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7438 - accuracy: 0.6667 \n",
      "Epoch 386: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 386: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 386: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7438 - accuracy: 0.6667 - val_loss: 1.0142 - val_accuracy: 0.4286\n",
      "Epoch 387/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.6905 \n",
      "Epoch 387: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 387: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 387: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7408 - accuracy: 0.6905 - val_loss: 1.0188 - val_accuracy: 0.4286\n",
      "Epoch 388/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.6905 \n",
      "Epoch 388: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 388: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 388: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7140 - accuracy: 0.6905 - val_loss: 1.0247 - val_accuracy: 0.4286\n",
      "Epoch 389/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.6786 \n",
      "Epoch 389: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 389: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 389: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7073 - accuracy: 0.6786 - val_loss: 1.0150 - val_accuracy: 0.4286\n",
      "Epoch 390/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7189 - accuracy: 0.6905 \n",
      "Epoch 390: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 390: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 390: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7189 - accuracy: 0.6905 - val_loss: 1.0344 - val_accuracy: 0.4286\n",
      "Epoch 391/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7335 - accuracy: 0.6786 \n",
      "Epoch 391: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 391: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 391: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7335 - accuracy: 0.6786 - val_loss: 1.0389 - val_accuracy: 0.4286\n",
      "Epoch 392/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7236 - accuracy: 0.6905 \n",
      "Epoch 392: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 392: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 392: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7236 - accuracy: 0.6905 - val_loss: 1.0380 - val_accuracy: 0.4286\n",
      "Epoch 393/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.7024 \n",
      "Epoch 393: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 393: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 393: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7079 - accuracy: 0.7024 - val_loss: 1.0301 - val_accuracy: 0.4286\n",
      "Epoch 394/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7055 - accuracy: 0.6905 \n",
      "Epoch 394: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 394: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 394: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7055 - accuracy: 0.6905 - val_loss: 1.0317 - val_accuracy: 0.4286\n",
      "Epoch 395/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7519 - accuracy: 0.6667 \n",
      "Epoch 395: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 395: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 395: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7519 - accuracy: 0.6667 - val_loss: 1.0398 - val_accuracy: 0.4286\n",
      "Epoch 396/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7519 - accuracy: 0.6786 \n",
      "Epoch 396: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 396: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 396: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7519 - accuracy: 0.6786 - val_loss: 1.0205 - val_accuracy: 0.4286\n",
      "Epoch 397/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.6786 \n",
      "Epoch 397: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 397: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 397: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7446 - accuracy: 0.6786 - val_loss: 1.0217 - val_accuracy: 0.4286\n",
      "Epoch 398/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.6786 \n",
      "Epoch 398: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 398: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 398: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7397 - accuracy: 0.6786 - val_loss: 1.0334 - val_accuracy: 0.4286\n",
      "Epoch 399/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7268 - accuracy: 0.6786 \n",
      "Epoch 399: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 399: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 399: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7268 - accuracy: 0.6786 - val_loss: 1.0330 - val_accuracy: 0.4286\n",
      "Epoch 400/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.6905 \n",
      "Epoch 400: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 400: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 400: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7209 - accuracy: 0.6905 - val_loss: 1.0151 - val_accuracy: 0.4286\n",
      "Epoch 401/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7375 - accuracy: 0.6905 \n",
      "Epoch 401: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 401: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 401: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7375 - accuracy: 0.6905 - val_loss: 1.0213 - val_accuracy: 0.4286\n",
      "Epoch 402/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7244 - accuracy: 0.6786 \n",
      "Epoch 402: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 402: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 402: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7244 - accuracy: 0.6786 - val_loss: 1.0153 - val_accuracy: 0.4286\n",
      "Epoch 403/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7521 - accuracy: 0.6786 \n",
      "Epoch 403: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 403: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 403: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7521 - accuracy: 0.6786 - val_loss: 1.0106 - val_accuracy: 0.4286\n",
      "Epoch 404/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.6905 \n",
      "Epoch 404: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 404: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 404: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7266 - accuracy: 0.6905 - val_loss: 0.9961 - val_accuracy: 0.4286\n",
      "Epoch 405/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.6905 \n",
      "Epoch 405: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 405: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 405: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 20s/step - loss: 0.7259 - accuracy: 0.6905 - val_loss: 0.9836 - val_accuracy: 0.4286\n",
      "Epoch 406/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7884 - accuracy: 0.6786 \n",
      "Epoch 406: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 406: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 406: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7884 - accuracy: 0.6786 - val_loss: 0.9816 - val_accuracy: 0.4286\n",
      "Epoch 407/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8013 - accuracy: 0.6667 \n",
      "Epoch 407: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 407: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 407: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8013 - accuracy: 0.6667 - val_loss: 0.9924 - val_accuracy: 0.4286\n",
      "Epoch 408/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.6786 \n",
      "Epoch 408: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 408: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 408: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7420 - accuracy: 0.6786 - val_loss: 1.0005 - val_accuracy: 0.4286\n",
      "Epoch 409/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.6548 \n",
      "Epoch 409: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 409: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 409: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7611 - accuracy: 0.6548 - val_loss: 1.0098 - val_accuracy: 0.4286\n",
      "Epoch 410/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7482 - accuracy: 0.6786 \n",
      "Epoch 410: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 410: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 410: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7482 - accuracy: 0.6786 - val_loss: 1.0034 - val_accuracy: 0.4286\n",
      "Epoch 411/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.6905 \n",
      "Epoch 411: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 411: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 411: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7359 - accuracy: 0.6905 - val_loss: 0.9693 - val_accuracy: 0.4286\n",
      "Epoch 412/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.6667 \n",
      "Epoch 412: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 412: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 412: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7369 - accuracy: 0.6667 - val_loss: 0.9983 - val_accuracy: 0.4286\n",
      "Epoch 413/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.6786 \n",
      "Epoch 413: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 413: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 413: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7478 - accuracy: 0.6786 - val_loss: 0.9969 - val_accuracy: 0.4286\n",
      "Epoch 414/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7373 - accuracy: 0.6786 \n",
      "Epoch 414: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 414: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 414: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7373 - accuracy: 0.6786 - val_loss: 0.9917 - val_accuracy: 0.4286\n",
      "Epoch 415/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7228 - accuracy: 0.6786 \n",
      "Epoch 415: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 415: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 415: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7228 - accuracy: 0.6786 - val_loss: 1.0027 - val_accuracy: 0.4286\n",
      "Epoch 416/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.6905 \n",
      "Epoch 416: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 416: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 416: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7480 - accuracy: 0.6905 - val_loss: 1.0059 - val_accuracy: 0.4286\n",
      "Epoch 417/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.7024 \n",
      "Epoch 417: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 417: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 417: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7043 - accuracy: 0.7024 - val_loss: 1.0016 - val_accuracy: 0.4286\n",
      "Epoch 418/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7121 - accuracy: 0.6786 \n",
      "Epoch 418: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 418: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 418: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7121 - accuracy: 0.6786 - val_loss: 0.9981 - val_accuracy: 0.4286\n",
      "Epoch 419/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.6905 \n",
      "Epoch 419: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 419: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 419: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7185 - accuracy: 0.6905 - val_loss: 1.0107 - val_accuracy: 0.4286\n",
      "Epoch 420/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7269 - accuracy: 0.7024 \n",
      "Epoch 420: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 420: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 420: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7269 - accuracy: 0.7024 - val_loss: 1.0433 - val_accuracy: 0.4286\n",
      "Epoch 421/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.6905 \n",
      "Epoch 421: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 421: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 421: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7110 - accuracy: 0.6905 - val_loss: 1.0538 - val_accuracy: 0.4286\n",
      "Epoch 422/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7471 - accuracy: 0.7024 \n",
      "Epoch 422: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 422: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 422: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7471 - accuracy: 0.7024 - val_loss: 1.0529 - val_accuracy: 0.4286\n",
      "Epoch 423/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7118 - accuracy: 0.6905 \n",
      "Epoch 423: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 423: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 423: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7118 - accuracy: 0.6905 - val_loss: 1.0427 - val_accuracy: 0.4286\n",
      "Epoch 424/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.6667 \n",
      "Epoch 424: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 424: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 424: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 20s/step - loss: 0.7313 - accuracy: 0.6667 - val_loss: 1.0119 - val_accuracy: 0.4286\n",
      "Epoch 425/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.7024 \n",
      "Epoch 425: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 425: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 425: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7017 - accuracy: 0.7024 - val_loss: 0.9705 - val_accuracy: 0.5238\n",
      "Epoch 426/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7169 - accuracy: 0.6786 \n",
      "Epoch 426: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 426: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 426: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7169 - accuracy: 0.6786 - val_loss: 0.9746 - val_accuracy: 0.5238\n",
      "Epoch 427/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.6786 \n",
      "Epoch 427: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 427: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 427: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7277 - accuracy: 0.6786 - val_loss: 0.9832 - val_accuracy: 0.5238\n",
      "Epoch 428/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7129 - accuracy: 0.6905 \n",
      "Epoch 428: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 428: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 428: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7129 - accuracy: 0.6905 - val_loss: 0.9780 - val_accuracy: 0.4286\n",
      "Epoch 429/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.6667 \n",
      "Epoch 429: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 429: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 429: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7126 - accuracy: 0.6667 - val_loss: 0.9984 - val_accuracy: 0.4286\n",
      "Epoch 430/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7064 - accuracy: 0.6786 \n",
      "Epoch 430: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 430: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 430: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7064 - accuracy: 0.6786 - val_loss: 1.0039 - val_accuracy: 0.4286\n",
      "Epoch 431/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.6905 \n",
      "Epoch 431: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 431: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 431: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7024 - accuracy: 0.6905 - val_loss: 1.0142 - val_accuracy: 0.4286\n",
      "Epoch 432/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.6905 \n",
      "Epoch 432: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 432: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 432: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7141 - accuracy: 0.6905 - val_loss: 1.0167 - val_accuracy: 0.4286\n",
      "Epoch 433/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.6905 \n",
      "Epoch 433: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 433: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 433: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7146 - accuracy: 0.6905 - val_loss: 1.0469 - val_accuracy: 0.4286\n",
      "Epoch 434/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.6905 \n",
      "Epoch 434: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 434: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 434: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7278 - accuracy: 0.6905 - val_loss: 1.1147 - val_accuracy: 0.3810\n",
      "Epoch 435/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7815 - accuracy: 0.6667 \n",
      "Epoch 435: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 435: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 435: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7815 - accuracy: 0.6667 - val_loss: 1.0329 - val_accuracy: 0.4286\n",
      "Epoch 436/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.7024 \n",
      "Epoch 436: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 436: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 436: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7277 - accuracy: 0.7024 - val_loss: 0.9767 - val_accuracy: 0.4762\n",
      "Epoch 437/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.6667 \n",
      "Epoch 437: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 437: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 437: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7205 - accuracy: 0.6667 - val_loss: 0.9681 - val_accuracy: 0.5238\n",
      "Epoch 438/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7276 - accuracy: 0.6548 \n",
      "Epoch 438: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 438: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 438: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7276 - accuracy: 0.6548 - val_loss: 0.9972 - val_accuracy: 0.4286\n",
      "Epoch 439/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7438 - accuracy: 0.6786 \n",
      "Epoch 439: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 439: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 439: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7438 - accuracy: 0.6786 - val_loss: 1.0064 - val_accuracy: 0.4286\n",
      "Epoch 440/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7289 - accuracy: 0.6786 \n",
      "Epoch 440: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 440: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 440: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7289 - accuracy: 0.6786 - val_loss: 1.0333 - val_accuracy: 0.4286\n",
      "Epoch 441/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7578 - accuracy: 0.6667 \n",
      "Epoch 441: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 441: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 441: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7578 - accuracy: 0.6667 - val_loss: 1.0587 - val_accuracy: 0.4286\n",
      "Epoch 442/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.6905 \n",
      "Epoch 442: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 442: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 442: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7095 - accuracy: 0.6905 - val_loss: 1.0164 - val_accuracy: 0.4286\n",
      "Epoch 443/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.6667 \n",
      "Epoch 443: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 443: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 443: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 53s 20s/step - loss: 0.7638 - accuracy: 0.6667 - val_loss: 1.0209 - val_accuracy: 0.4286\n",
      "Epoch 444/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7589 - accuracy: 0.6786 \n",
      "Epoch 444: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 444: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 444: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7589 - accuracy: 0.6786 - val_loss: 1.0212 - val_accuracy: 0.4286\n",
      "Epoch 445/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7285 - accuracy: 0.6905 \n",
      "Epoch 445: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 445: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 445: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7285 - accuracy: 0.6905 - val_loss: 1.0613 - val_accuracy: 0.4286\n",
      "Epoch 446/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7469 - accuracy: 0.7024 \n",
      "Epoch 446: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 446: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 446: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7469 - accuracy: 0.7024 - val_loss: 1.0514 - val_accuracy: 0.4286\n",
      "Epoch 447/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.6667 \n",
      "Epoch 447: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 447: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 447: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7249 - accuracy: 0.6667 - val_loss: 1.0199 - val_accuracy: 0.4286\n",
      "Epoch 448/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.6667 \n",
      "Epoch 448: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 448: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 448: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 20s/step - loss: 0.7291 - accuracy: 0.6667 - val_loss: 1.0233 - val_accuracy: 0.4286\n",
      "Epoch 449/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.6548 \n",
      "Epoch 449: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 449: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 449: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 20s/step - loss: 0.7409 - accuracy: 0.6548 - val_loss: 1.0373 - val_accuracy: 0.4286\n",
      "Epoch 450/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7360 - accuracy: 0.6667 \n",
      "Epoch 450: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 450: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 450: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7360 - accuracy: 0.6667 - val_loss: 1.0249 - val_accuracy: 0.4286\n",
      "Epoch 451/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7484 - accuracy: 0.6786 \n",
      "Epoch 451: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 451: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 451: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7484 - accuracy: 0.6786 - val_loss: 1.0116 - val_accuracy: 0.4286\n",
      "Epoch 452/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.6667 \n",
      "Epoch 452: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 452: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 452: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7408 - accuracy: 0.6667 - val_loss: 1.0003 - val_accuracy: 0.4286\n",
      "Epoch 453/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.6786 \n",
      "Epoch 453: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 453: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 453: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7374 - accuracy: 0.6786 - val_loss: 0.9931 - val_accuracy: 0.4286\n",
      "Epoch 454/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7486 - accuracy: 0.6786 \n",
      "Epoch 454: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 454: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 454: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7486 - accuracy: 0.6786 - val_loss: 0.9914 - val_accuracy: 0.4286\n",
      "Epoch 455/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8578 - accuracy: 0.6429 \n",
      "Epoch 455: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 455: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 455: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8578 - accuracy: 0.6429 - val_loss: 0.9834 - val_accuracy: 0.4286\n",
      "Epoch 456/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7856 - accuracy: 0.6429 \n",
      "Epoch 456: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 456: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 456: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7856 - accuracy: 0.6429 - val_loss: 1.0737 - val_accuracy: 0.4286\n",
      "Epoch 457/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7970 - accuracy: 0.6190 \n",
      "Epoch 457: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 457: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 457: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7970 - accuracy: 0.6190 - val_loss: 1.0783 - val_accuracy: 0.4286\n",
      "Epoch 458/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8248 - accuracy: 0.6310 \n",
      "Epoch 458: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 458: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 458: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.8248 - accuracy: 0.6310 - val_loss: 0.9994 - val_accuracy: 0.4286\n",
      "Epoch 459/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7654 - accuracy: 0.6786 \n",
      "Epoch 459: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 459: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 459: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7654 - accuracy: 0.6786 - val_loss: 0.9630 - val_accuracy: 0.4286\n",
      "Epoch 460/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7441 - accuracy: 0.6786 \n",
      "Epoch 460: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 460: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 460: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7441 - accuracy: 0.6786 - val_loss: 0.9758 - val_accuracy: 0.4286\n",
      "Epoch 461/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.6310 \n",
      "Epoch 461: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 461: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 461: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7952 - accuracy: 0.6310 - val_loss: 0.9670 - val_accuracy: 0.4286\n",
      "Epoch 462/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.6667 \n",
      "Epoch 462: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 462: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 462: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7549 - accuracy: 0.6667 - val_loss: 0.9332 - val_accuracy: 0.4286\n",
      "Epoch 463/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7533 - accuracy: 0.6429 \n",
      "Epoch 463: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 463: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 463: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7533 - accuracy: 0.6429 - val_loss: 0.9552 - val_accuracy: 0.4286\n",
      "Epoch 464/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.6548 \n",
      "Epoch 464: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 464: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 464: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7510 - accuracy: 0.6548 - val_loss: 0.9783 - val_accuracy: 0.4286\n",
      "Epoch 465/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7754 - accuracy: 0.6310 \n",
      "Epoch 465: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 465: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 465: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7754 - accuracy: 0.6310 - val_loss: 0.9946 - val_accuracy: 0.4286\n",
      "Epoch 466/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7593 - accuracy: 0.6667 \n",
      "Epoch 466: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 466: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 466: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7593 - accuracy: 0.6667 - val_loss: 0.9620 - val_accuracy: 0.4286\n",
      "Epoch 467/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.6905 \n",
      "Epoch 467: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 467: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 467: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7101 - accuracy: 0.6905 - val_loss: 0.9758 - val_accuracy: 0.4286\n",
      "Epoch 468/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.6667 \n",
      "Epoch 468: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 468: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 468: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7085 - accuracy: 0.6667 - val_loss: 0.9904 - val_accuracy: 0.4286\n",
      "Epoch 469/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.6667 \n",
      "Epoch 469: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 469: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 469: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7143 - accuracy: 0.6667 - val_loss: 1.0029 - val_accuracy: 0.4286\n",
      "Epoch 470/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.6905 \n",
      "Epoch 470: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 470: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 470: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7291 - accuracy: 0.6905 - val_loss: 1.0220 - val_accuracy: 0.4286\n",
      "Epoch 471/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.6786 \n",
      "Epoch 471: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 471: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 471: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7253 - accuracy: 0.6786 - val_loss: 1.0192 - val_accuracy: 0.4286\n",
      "Epoch 472/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.6667 \n",
      "Epoch 472: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 472: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 472: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7137 - accuracy: 0.6667 - val_loss: 1.0157 - val_accuracy: 0.4286\n",
      "Epoch 473/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7129 - accuracy: 0.6905 \n",
      "Epoch 473: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 473: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 473: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7129 - accuracy: 0.6905 - val_loss: 1.0189 - val_accuracy: 0.4286\n",
      "Epoch 474/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7105 - accuracy: 0.7024 \n",
      "Epoch 474: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 474: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 474: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7105 - accuracy: 0.7024 - val_loss: 1.0225 - val_accuracy: 0.4286\n",
      "Epoch 475/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7344 - accuracy: 0.6548 \n",
      "Epoch 475: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 475: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 475: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7344 - accuracy: 0.6548 - val_loss: 1.0260 - val_accuracy: 0.4286\n",
      "Epoch 476/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7127 - accuracy: 0.6667 \n",
      "Epoch 476: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 476: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 476: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7127 - accuracy: 0.6667 - val_loss: 1.0354 - val_accuracy: 0.4286\n",
      "Epoch 477/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7274 - accuracy: 0.7143 \n",
      "Epoch 477: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 477: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 477: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7274 - accuracy: 0.7143 - val_loss: 1.0233 - val_accuracy: 0.4286\n",
      "Epoch 478/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.6905 \n",
      "Epoch 478: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 478: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 478: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7192 - accuracy: 0.6905 - val_loss: 1.0151 - val_accuracy: 0.4286\n",
      "Epoch 479/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7880 - accuracy: 0.6548 \n",
      "Epoch 479: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 479: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 479: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7880 - accuracy: 0.6548 - val_loss: 1.0260 - val_accuracy: 0.4286\n",
      "Epoch 480/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7199 - accuracy: 0.6905 \n",
      "Epoch 480: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 480: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 480: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7199 - accuracy: 0.6905 - val_loss: 1.0034 - val_accuracy: 0.4286\n",
      "Epoch 481/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7246 - accuracy: 0.7024 \n",
      "Epoch 481: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 481: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 481: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7246 - accuracy: 0.7024 - val_loss: 1.0173 - val_accuracy: 0.4286\n",
      "Epoch 482/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7315 - accuracy: 0.6786 \n",
      "Epoch 482: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 482: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 482: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7315 - accuracy: 0.6786 - val_loss: 1.0224 - val_accuracy: 0.4286\n",
      "Epoch 483/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7299 - accuracy: 0.7024 \n",
      "Epoch 483: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 483: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 483: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7299 - accuracy: 0.7024 - val_loss: 1.0156 - val_accuracy: 0.4286\n",
      "Epoch 484/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7339 - accuracy: 0.6786 \n",
      "Epoch 484: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 484: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 484: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7339 - accuracy: 0.6786 - val_loss: 1.0142 - val_accuracy: 0.4286\n",
      "Epoch 485/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7231 - accuracy: 0.6786 \n",
      "Epoch 485: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 485: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 485: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7231 - accuracy: 0.6786 - val_loss: 1.0140 - val_accuracy: 0.4286\n",
      "Epoch 486/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7193 - accuracy: 0.6786 \n",
      "Epoch 486: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 486: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 486: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7193 - accuracy: 0.6786 - val_loss: 1.0094 - val_accuracy: 0.4286\n",
      "Epoch 487/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7314 - accuracy: 0.6905 \n",
      "Epoch 487: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 487: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 487: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7314 - accuracy: 0.6905 - val_loss: 0.9869 - val_accuracy: 0.4286\n",
      "Epoch 488/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7326 - accuracy: 0.6786 \n",
      "Epoch 488: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 488: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 488: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7326 - accuracy: 0.6786 - val_loss: 0.9852 - val_accuracy: 0.4286\n",
      "Epoch 489/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.6786 \n",
      "Epoch 489: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 489: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 489: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7178 - accuracy: 0.6786 - val_loss: 0.9813 - val_accuracy: 0.4286\n",
      "Epoch 490/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.7024 \n",
      "Epoch 490: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 490: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 490: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7072 - accuracy: 0.7024 - val_loss: 0.9993 - val_accuracy: 0.4286\n",
      "Epoch 491/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.6786 \n",
      "Epoch 491: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 491: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 491: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7312 - accuracy: 0.6786 - val_loss: 1.0118 - val_accuracy: 0.4286\n",
      "Epoch 492/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7133 - accuracy: 0.6786 \n",
      "Epoch 492: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 492: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 492: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7133 - accuracy: 0.6786 - val_loss: 1.0149 - val_accuracy: 0.4286\n",
      "Epoch 493/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.6786 \n",
      "Epoch 493: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 493: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 493: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7233 - accuracy: 0.6786 - val_loss: 1.0028 - val_accuracy: 0.4286\n",
      "Epoch 494/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.6548 \n",
      "Epoch 494: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 494: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 494: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7297 - accuracy: 0.6548 - val_loss: 1.0030 - val_accuracy: 0.4286\n",
      "Epoch 495/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.6786 \n",
      "Epoch 495: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 495: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 495: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7242 - accuracy: 0.6786 - val_loss: 1.0112 - val_accuracy: 0.4286\n",
      "Epoch 496/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7316 - accuracy: 0.6667 \n",
      "Epoch 496: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 496: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 496: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7316 - accuracy: 0.6667 - val_loss: 1.0102 - val_accuracy: 0.4286\n",
      "Epoch 497/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.7024 \n",
      "Epoch 497: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 497: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 497: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7110 - accuracy: 0.7024 - val_loss: 1.0178 - val_accuracy: 0.4286\n",
      "Epoch 498/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.6905 \n",
      "Epoch 498: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 498: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 498: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7168 - accuracy: 0.6905 - val_loss: 1.0242 - val_accuracy: 0.4286\n",
      "Epoch 499/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7104 - accuracy: 0.6786 \n",
      "Epoch 499: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 499: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 499: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7104 - accuracy: 0.6786 - val_loss: 1.0414 - val_accuracy: 0.4286\n",
      "Epoch 500/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.6786 \n",
      "Epoch 500: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 500: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 500: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7048 - accuracy: 0.6786 - val_loss: 1.0526 - val_accuracy: 0.4286\n",
      "Epoch 501/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.6667 \n",
      "Epoch 501: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 501: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 501: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7114 - accuracy: 0.6667 - val_loss: 1.0468 - val_accuracy: 0.4286\n",
      "Epoch 502/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7181 - accuracy: 0.6905 \n",
      "Epoch 502: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 502: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 502: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7181 - accuracy: 0.6905 - val_loss: 1.0628 - val_accuracy: 0.4286\n",
      "Epoch 503/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7173 - accuracy: 0.6905 \n",
      "Epoch 503: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 503: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 503: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7173 - accuracy: 0.6905 - val_loss: 1.0741 - val_accuracy: 0.4286\n",
      "Epoch 504/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7083 - accuracy: 0.6786 \n",
      "Epoch 504: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 504: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 504: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7083 - accuracy: 0.6786 - val_loss: 1.0567 - val_accuracy: 0.4286\n",
      "Epoch 505/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7003 - accuracy: 0.6905 \n",
      "Epoch 505: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 505: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 505: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7003 - accuracy: 0.6905 - val_loss: 1.0262 - val_accuracy: 0.4286\n",
      "Epoch 506/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7264 - accuracy: 0.6548 \n",
      "Epoch 506: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 506: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 506: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7264 - accuracy: 0.6548 - val_loss: 1.0140 - val_accuracy: 0.4286\n",
      "Epoch 507/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7244 - accuracy: 0.7024 \n",
      "Epoch 507: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 507: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 507: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7244 - accuracy: 0.7024 - val_loss: 0.9963 - val_accuracy: 0.4762\n",
      "Epoch 508/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.6786 \n",
      "Epoch 508: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 508: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 508: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6958 - accuracy: 0.6786 - val_loss: 0.9848 - val_accuracy: 0.4762\n",
      "Epoch 509/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6994 - accuracy: 0.6786 \n",
      "Epoch 509: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 509: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 509: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6994 - accuracy: 0.6786 - val_loss: 0.9847 - val_accuracy: 0.4762\n",
      "Epoch 510/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.7143 \n",
      "Epoch 510: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 510: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 510: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6930 - accuracy: 0.7143 - val_loss: 0.9976 - val_accuracy: 0.5238\n",
      "Epoch 511/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7946 - accuracy: 0.6548 \n",
      "Epoch 511: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 511: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 511: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7946 - accuracy: 0.6548 - val_loss: 1.0107 - val_accuracy: 0.5238\n",
      "Epoch 512/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.6905 \n",
      "Epoch 512: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 512: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 512: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6981 - accuracy: 0.6905 - val_loss: 1.0000 - val_accuracy: 0.4762\n",
      "Epoch 513/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 0.7024 \n",
      "Epoch 513: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 513: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 513: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6763 - accuracy: 0.7024 - val_loss: 0.9927 - val_accuracy: 0.5238\n",
      "Epoch 514/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7439 - accuracy: 0.6429 \n",
      "Epoch 514: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 514: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 514: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7439 - accuracy: 0.6429 - val_loss: 0.9938 - val_accuracy: 0.4286\n",
      "Epoch 515/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.6905 \n",
      "Epoch 515: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 515: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 515: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6940 - accuracy: 0.6905 - val_loss: 0.9964 - val_accuracy: 0.4286\n",
      "Epoch 516/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.6667 \n",
      "Epoch 516: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 516: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 516: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7312 - accuracy: 0.6667 - val_loss: 0.9969 - val_accuracy: 0.4286\n",
      "Epoch 517/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.6786 \n",
      "Epoch 517: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 517: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 517: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6764 - accuracy: 0.6786 - val_loss: 0.9996 - val_accuracy: 0.4286\n",
      "Epoch 518/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.6786 \n",
      "Epoch 518: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 518: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 518: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6862 - accuracy: 0.6786 - val_loss: 1.0036 - val_accuracy: 0.4286\n",
      "Epoch 519/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.6786 \n",
      "Epoch 519: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 519: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 519: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6952 - accuracy: 0.6786 - val_loss: 1.0048 - val_accuracy: 0.4286\n",
      "Epoch 520/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.6667 \n",
      "Epoch 520: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 520: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 520: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6706 - accuracy: 0.6667 - val_loss: 1.0174 - val_accuracy: 0.4286\n",
      "Epoch 521/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.6786 \n",
      "Epoch 521: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 521: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 521: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7052 - accuracy: 0.6786 - val_loss: 1.0286 - val_accuracy: 0.4286\n",
      "Epoch 522/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.6786 \n",
      "Epoch 522: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 522: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 522: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.6878 - accuracy: 0.6786 - val_loss: 1.0173 - val_accuracy: 0.4286\n",
      "Epoch 523/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.6905 \n",
      "Epoch 523: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 523: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 523: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6459 - accuracy: 0.6905 - val_loss: 1.0361 - val_accuracy: 0.4286\n",
      "Epoch 524/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6989 - accuracy: 0.6548 \n",
      "Epoch 524: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 524: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 524: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6989 - accuracy: 0.6548 - val_loss: 1.0501 - val_accuracy: 0.3810\n",
      "Epoch 525/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.6667 \n",
      "Epoch 525: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 525: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 525: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6923 - accuracy: 0.6667 - val_loss: 1.0212 - val_accuracy: 0.4286\n",
      "Epoch 526/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.6548 \n",
      "Epoch 526: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 526: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 526: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6671 - accuracy: 0.6548 - val_loss: 0.9807 - val_accuracy: 0.4286\n",
      "Epoch 527/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.6905 \n",
      "Epoch 527: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 527: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 527: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6259 - accuracy: 0.6905 - val_loss: 0.9668 - val_accuracy: 0.4762\n",
      "Epoch 528/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.7024 \n",
      "Epoch 528: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 528: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 528: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6766 - accuracy: 0.7024 - val_loss: 0.9571 - val_accuracy: 0.4762\n",
      "Epoch 529/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7224 - accuracy: 0.6429 \n",
      "Epoch 529: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 529: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 529: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7224 - accuracy: 0.6429 - val_loss: 0.9726 - val_accuracy: 0.4762\n",
      "Epoch 530/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.6667 \n",
      "Epoch 530: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 530: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 530: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6874 - accuracy: 0.6667 - val_loss: 1.0043 - val_accuracy: 0.4286\n",
      "Epoch 531/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 0.6905 \n",
      "Epoch 531: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 531: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 531: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6750 - accuracy: 0.6905 - val_loss: 1.0095 - val_accuracy: 0.4286\n",
      "Epoch 532/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.7024 \n",
      "Epoch 532: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 532: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 532: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6730 - accuracy: 0.7024 - val_loss: 1.0097 - val_accuracy: 0.4286\n",
      "Epoch 533/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.6667 \n",
      "Epoch 533: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 533: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 533: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6398 - accuracy: 0.6667 - val_loss: 0.9629 - val_accuracy: 0.4286\n",
      "Epoch 534/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9236 - accuracy: 0.5952 \n",
      "Epoch 534: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 534: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 534: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.9236 - accuracy: 0.5952 - val_loss: 0.9686 - val_accuracy: 0.4762\n",
      "Epoch 535/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8215 - accuracy: 0.6786 \n",
      "Epoch 535: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 535: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 535: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8215 - accuracy: 0.6786 - val_loss: 1.2632 - val_accuracy: 0.4286\n",
      "Epoch 536/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9618 - accuracy: 0.6310 \n",
      "Epoch 536: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 536: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 536: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.9618 - accuracy: 0.6310 - val_loss: 1.6151 - val_accuracy: 0.2381\n",
      "Epoch 537/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2624 - accuracy: 0.4881 \n",
      "Epoch 537: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 537: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 537: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.2624 - accuracy: 0.4881 - val_loss: 1.4919 - val_accuracy: 0.2381\n",
      "Epoch 538/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1903 - accuracy: 0.4881 \n",
      "Epoch 538: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 538: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 538: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 1.1903 - accuracy: 0.4881 - val_loss: 1.4456 - val_accuracy: 0.2381\n",
      "Epoch 539/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.5000 \n",
      "Epoch 539: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 539: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 539: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 1.1325 - accuracy: 0.5000 - val_loss: 1.2719 - val_accuracy: 0.2381\n",
      "Epoch 540/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0516 - accuracy: 0.5238 \n",
      "Epoch 540: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 540: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 540: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 1.0516 - accuracy: 0.5238 - val_loss: 1.2092 - val_accuracy: 0.2857\n",
      "Epoch 541/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9706 - accuracy: 0.5238 \n",
      "Epoch 541: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 541: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 541: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.9706 - accuracy: 0.5238 - val_loss: 1.1790 - val_accuracy: 0.2857\n",
      "Epoch 542/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9402 - accuracy: 0.5714 \n",
      "Epoch 542: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 542: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 542: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.9402 - accuracy: 0.5714 - val_loss: 1.1502 - val_accuracy: 0.2857\n",
      "Epoch 543/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9426 - accuracy: 0.5357 \n",
      "Epoch 543: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 543: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 543: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.9426 - accuracy: 0.5357 - val_loss: 1.1243 - val_accuracy: 0.2857\n",
      "Epoch 544/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8999 - accuracy: 0.5833 \n",
      "Epoch 544: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 544: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 544: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.8999 - accuracy: 0.5833 - val_loss: 1.1121 - val_accuracy: 0.2857\n",
      "Epoch 545/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9068 - accuracy: 0.5595 \n",
      "Epoch 545: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 545: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 545: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.9068 - accuracy: 0.5595 - val_loss: 1.0954 - val_accuracy: 0.3333\n",
      "Epoch 546/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8758 - accuracy: 0.5952 \n",
      "Epoch 546: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 546: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 546: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8758 - accuracy: 0.5952 - val_loss: 1.0785 - val_accuracy: 0.3333\n",
      "Epoch 547/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8791 - accuracy: 0.6071 \n",
      "Epoch 547: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 547: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 547: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8791 - accuracy: 0.6071 - val_loss: 1.0616 - val_accuracy: 0.3810\n",
      "Epoch 548/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8489 - accuracy: 0.5952 \n",
      "Epoch 548: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 548: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 548: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.8489 - accuracy: 0.5952 - val_loss: 1.0308 - val_accuracy: 0.3810\n",
      "Epoch 549/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8315 - accuracy: 0.6548 \n",
      "Epoch 549: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 549: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 549: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8315 - accuracy: 0.6548 - val_loss: 1.0117 - val_accuracy: 0.3810\n",
      "Epoch 550/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8307 - accuracy: 0.6190 \n",
      "Epoch 550: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 550: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 550: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.8307 - accuracy: 0.6190 - val_loss: 0.9963 - val_accuracy: 0.3810\n",
      "Epoch 551/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7984 - accuracy: 0.6310 \n",
      "Epoch 551: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 551: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 551: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7984 - accuracy: 0.6310 - val_loss: 0.9915 - val_accuracy: 0.3810\n",
      "Epoch 552/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8153 - accuracy: 0.6310 \n",
      "Epoch 552: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 552: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 552: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8153 - accuracy: 0.6310 - val_loss: 0.9906 - val_accuracy: 0.3810\n",
      "Epoch 553/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7965 - accuracy: 0.6429 \n",
      "Epoch 553: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 553: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 553: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7965 - accuracy: 0.6429 - val_loss: 0.9965 - val_accuracy: 0.3810\n",
      "Epoch 554/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7935 - accuracy: 0.6310 \n",
      "Epoch 554: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 554: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 554: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7935 - accuracy: 0.6310 - val_loss: 1.0009 - val_accuracy: 0.3810\n",
      "Epoch 555/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7689 - accuracy: 0.6429 \n",
      "Epoch 555: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 555: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 555: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7689 - accuracy: 0.6429 - val_loss: 0.9993 - val_accuracy: 0.3810\n",
      "Epoch 556/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7704 - accuracy: 0.6548 \n",
      "Epoch 556: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 556: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 556: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.7704 - accuracy: 0.6548 - val_loss: 0.9699 - val_accuracy: 0.4762\n",
      "Epoch 557/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7508 - accuracy: 0.6905 \n",
      "Epoch 557: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 557: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 557: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7508 - accuracy: 0.6905 - val_loss: 0.9421 - val_accuracy: 0.4762\n",
      "Epoch 558/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7623 - accuracy: 0.6667 \n",
      "Epoch 558: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 558: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 558: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7623 - accuracy: 0.6667 - val_loss: 0.9543 - val_accuracy: 0.4762\n",
      "Epoch 559/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7352 - accuracy: 0.6905 \n",
      "Epoch 559: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 559: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 559: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7352 - accuracy: 0.6905 - val_loss: 0.9707 - val_accuracy: 0.4286\n",
      "Epoch 560/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.6667 \n",
      "Epoch 560: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 560: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 560: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7528 - accuracy: 0.6667 - val_loss: 0.9813 - val_accuracy: 0.4286\n",
      "Epoch 561/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.6667 \n",
      "Epoch 561: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 561: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 561: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7501 - accuracy: 0.6667 - val_loss: 0.9845 - val_accuracy: 0.4286\n",
      "Epoch 562/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7469 - accuracy: 0.6667 \n",
      "Epoch 562: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 562: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 562: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7469 - accuracy: 0.6667 - val_loss: 0.9863 - val_accuracy: 0.4286\n",
      "Epoch 563/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7379 - accuracy: 0.6667 \n",
      "Epoch 563: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 563: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 563: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7379 - accuracy: 0.6667 - val_loss: 0.9788 - val_accuracy: 0.4286\n",
      "Epoch 564/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7134 - accuracy: 0.7143 \n",
      "Epoch 564: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 564: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 564: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7134 - accuracy: 0.7143 - val_loss: 0.9780 - val_accuracy: 0.4286\n",
      "Epoch 565/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7238 - accuracy: 0.6786 \n",
      "Epoch 565: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 565: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 565: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7238 - accuracy: 0.6786 - val_loss: 0.9815 - val_accuracy: 0.4286\n",
      "Epoch 566/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.6905 \n",
      "Epoch 566: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 566: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 566: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7243 - accuracy: 0.6905 - val_loss: 0.9892 - val_accuracy: 0.4286\n",
      "Epoch 567/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7283 - accuracy: 0.6786 \n",
      "Epoch 567: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 567: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 567: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7283 - accuracy: 0.6786 - val_loss: 0.9943 - val_accuracy: 0.4286\n",
      "Epoch 568/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.6786 \n",
      "Epoch 568: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 568: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 568: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7215 - accuracy: 0.6786 - val_loss: 0.9884 - val_accuracy: 0.4286\n",
      "Epoch 569/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7435 - accuracy: 0.6786 \n",
      "Epoch 569: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 569: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 569: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7435 - accuracy: 0.6786 - val_loss: 0.9858 - val_accuracy: 0.4286\n",
      "Epoch 570/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7305 - accuracy: 0.6786 \n",
      "Epoch 570: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 570: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 570: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7305 - accuracy: 0.6786 - val_loss: 0.9927 - val_accuracy: 0.4286\n",
      "Epoch 571/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7142 - accuracy: 0.7024 \n",
      "Epoch 571: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 571: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 571: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7142 - accuracy: 0.7024 - val_loss: 1.0015 - val_accuracy: 0.4286\n",
      "Epoch 572/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7352 - accuracy: 0.6667 \n",
      "Epoch 572: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 572: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 572: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7352 - accuracy: 0.6667 - val_loss: 1.0042 - val_accuracy: 0.4286\n",
      "Epoch 573/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7353 - accuracy: 0.6905 \n",
      "Epoch 573: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 573: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 573: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7353 - accuracy: 0.6905 - val_loss: 0.9970 - val_accuracy: 0.4286\n",
      "Epoch 574/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7189 - accuracy: 0.6905 \n",
      "Epoch 574: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 574: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 574: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7189 - accuracy: 0.6905 - val_loss: 0.9920 - val_accuracy: 0.4286\n",
      "Epoch 575/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7229 - accuracy: 0.6786 \n",
      "Epoch 575: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 575: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 575: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7229 - accuracy: 0.6786 - val_loss: 0.9904 - val_accuracy: 0.4286\n",
      "Epoch 576/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7177 - accuracy: 0.6786 \n",
      "Epoch 576: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 576: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 576: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7177 - accuracy: 0.6786 - val_loss: 0.9924 - val_accuracy: 0.4286\n",
      "Epoch 577/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7289 - accuracy: 0.6905 \n",
      "Epoch 577: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 577: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 577: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7289 - accuracy: 0.6905 - val_loss: 0.9932 - val_accuracy: 0.4286\n",
      "Epoch 578/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.6905 \n",
      "Epoch 578: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 578: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 578: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6927 - accuracy: 0.6905 - val_loss: 0.9916 - val_accuracy: 0.4286\n",
      "Epoch 579/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7129 - accuracy: 0.7024 \n",
      "Epoch 579: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 579: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 579: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7129 - accuracy: 0.7024 - val_loss: 0.9976 - val_accuracy: 0.4286\n",
      "Epoch 580/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.6786 \n",
      "Epoch 580: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 580: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 580: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7272 - accuracy: 0.6786 - val_loss: 1.0005 - val_accuracy: 0.4286\n",
      "Epoch 581/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7103 - accuracy: 0.7143 \n",
      "Epoch 581: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 581: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 581: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7103 - accuracy: 0.7143 - val_loss: 0.9969 - val_accuracy: 0.4286\n",
      "Epoch 582/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.6786 \n",
      "Epoch 582: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 582: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 582: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6913 - accuracy: 0.6786 - val_loss: 1.0010 - val_accuracy: 0.4286\n",
      "Epoch 583/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.7143 \n",
      "Epoch 583: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 583: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 583: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6981 - accuracy: 0.7143 - val_loss: 1.0069 - val_accuracy: 0.4286\n",
      "Epoch 584/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.7024 \n",
      "Epoch 584: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 584: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 584: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6910 - accuracy: 0.7024 - val_loss: 1.0112 - val_accuracy: 0.4286\n",
      "Epoch 585/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.6786 \n",
      "Epoch 585: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 585: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 585: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7023 - accuracy: 0.6786 - val_loss: 1.0183 - val_accuracy: 0.4286\n",
      "Epoch 586/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7384 - accuracy: 0.6429 \n",
      "Epoch 586: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 586: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 586: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7384 - accuracy: 0.6429 - val_loss: 1.0180 - val_accuracy: 0.4286\n",
      "Epoch 587/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6725 - accuracy: 0.7143 \n",
      "Epoch 587: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 587: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 587: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6725 - accuracy: 0.7143 - val_loss: 1.0096 - val_accuracy: 0.4286\n",
      "Epoch 588/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.6548 \n",
      "Epoch 588: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 588: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 588: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7209 - accuracy: 0.6548 - val_loss: 0.9861 - val_accuracy: 0.4286\n",
      "Epoch 589/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.6905 \n",
      "Epoch 589: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 589: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 589: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7047 - accuracy: 0.6905 - val_loss: 0.9956 - val_accuracy: 0.4286\n",
      "Epoch 590/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.6548 \n",
      "Epoch 590: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 590: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 590: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7321 - accuracy: 0.6548 - val_loss: 0.9960 - val_accuracy: 0.4286\n",
      "Epoch 591/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7264 - accuracy: 0.6905 \n",
      "Epoch 591: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 591: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 591: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7264 - accuracy: 0.6905 - val_loss: 0.9804 - val_accuracy: 0.4286\n",
      "Epoch 592/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.6786 \n",
      "Epoch 592: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 592: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 592: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7159 - accuracy: 0.6786 - val_loss: 0.9788 - val_accuracy: 0.4286\n",
      "Epoch 593/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.6786 \n",
      "Epoch 593: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 593: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 593: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7148 - accuracy: 0.6786 - val_loss: 0.9667 - val_accuracy: 0.4286\n",
      "Epoch 594/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.6786 \n",
      "Epoch 594: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 594: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 594: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7248 - accuracy: 0.6786 - val_loss: 0.9630 - val_accuracy: 0.4286\n",
      "Epoch 595/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.6786 \n",
      "Epoch 595: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 595: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 595: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.7128 - accuracy: 0.6786 - val_loss: 0.9802 - val_accuracy: 0.4286\n",
      "Epoch 596/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7077 - accuracy: 0.6905 \n",
      "Epoch 596: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 596: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 596: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7077 - accuracy: 0.6905 - val_loss: 0.9913 - val_accuracy: 0.4286\n",
      "Epoch 597/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.6667 \n",
      "Epoch 597: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 597: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 597: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7195 - accuracy: 0.6667 - val_loss: 0.9751 - val_accuracy: 0.4286\n",
      "Epoch 598/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7591 - accuracy: 0.6548 \n",
      "Epoch 598: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 598: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 598: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7591 - accuracy: 0.6548 - val_loss: 0.9980 - val_accuracy: 0.4286\n",
      "Epoch 599/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.6786 \n",
      "Epoch 599: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 599: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 599: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7015 - accuracy: 0.6786 - val_loss: 1.0208 - val_accuracy: 0.4286\n",
      "Epoch 600/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7151 - accuracy: 0.6667 \n",
      "Epoch 600: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 600: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 600: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7151 - accuracy: 0.6667 - val_loss: 1.0241 - val_accuracy: 0.4762\n",
      "Epoch 601/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6783 - accuracy: 0.7024 \n",
      "Epoch 601: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 601: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 601: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6783 - accuracy: 0.7024 - val_loss: 1.0196 - val_accuracy: 0.4762\n",
      "Epoch 602/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.6786 \n",
      "Epoch 602: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 602: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 602: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6936 - accuracy: 0.6786 - val_loss: 1.0298 - val_accuracy: 0.4286\n",
      "Epoch 603/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7019 - accuracy: 0.7143 \n",
      "Epoch 603: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 603: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 603: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7019 - accuracy: 0.7143 - val_loss: 1.0255 - val_accuracy: 0.4286\n",
      "Epoch 604/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.7024 \n",
      "Epoch 604: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 604: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 604: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6894 - accuracy: 0.7024 - val_loss: 1.0302 - val_accuracy: 0.4286\n",
      "Epoch 605/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7074 - accuracy: 0.6786 \n",
      "Epoch 605: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 605: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 605: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7074 - accuracy: 0.6786 - val_loss: 1.0322 - val_accuracy: 0.4286\n",
      "Epoch 606/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.6786 \n",
      "Epoch 606: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 606: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 606: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7233 - accuracy: 0.6786 - val_loss: 1.0390 - val_accuracy: 0.4286\n",
      "Epoch 607/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.6667 \n",
      "Epoch 607: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 607: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 607: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6919 - accuracy: 0.6667 - val_loss: 1.0445 - val_accuracy: 0.4286\n",
      "Epoch 608/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.7024 \n",
      "Epoch 608: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 608: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 608: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.6838 - accuracy: 0.7024 - val_loss: 1.0333 - val_accuracy: 0.4286\n",
      "Epoch 609/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.6905 \n",
      "Epoch 609: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 609: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 609: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6810 - accuracy: 0.6905 - val_loss: 1.0512 - val_accuracy: 0.4286\n",
      "Epoch 610/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7097 - accuracy: 0.6667 \n",
      "Epoch 610: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 610: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 610: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7097 - accuracy: 0.6667 - val_loss: 1.0326 - val_accuracy: 0.4286\n",
      "Epoch 611/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.6667 \n",
      "Epoch 611: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 611: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 611: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6867 - accuracy: 0.6667 - val_loss: 1.0287 - val_accuracy: 0.4762\n",
      "Epoch 612/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.6548 \n",
      "Epoch 612: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 612: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 612: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6965 - accuracy: 0.6548 - val_loss: 1.0325 - val_accuracy: 0.4762\n",
      "Epoch 613/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.6667 \n",
      "Epoch 613: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 613: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 613: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7073 - accuracy: 0.6667 - val_loss: 0.9920 - val_accuracy: 0.4286\n",
      "Epoch 614/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.6786 \n",
      "Epoch 614: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 614: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 614: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7190 - accuracy: 0.6786 - val_loss: 0.9687 - val_accuracy: 0.4286\n",
      "Epoch 615/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7221 - accuracy: 0.6548 \n",
      "Epoch 615: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 615: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 615: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7221 - accuracy: 0.6548 - val_loss: 0.9684 - val_accuracy: 0.4286\n",
      "Epoch 616/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7104 - accuracy: 0.6548 \n",
      "Epoch 616: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 616: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 616: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7104 - accuracy: 0.6548 - val_loss: 0.9931 - val_accuracy: 0.4762\n",
      "Epoch 617/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.6786 \n",
      "Epoch 617: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 617: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 617: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6983 - accuracy: 0.6786 - val_loss: 1.0037 - val_accuracy: 0.4762\n",
      "Epoch 618/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.6786 \n",
      "Epoch 618: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 618: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 618: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6968 - accuracy: 0.6786 - val_loss: 0.9893 - val_accuracy: 0.4286\n",
      "Epoch 619/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.6786 \n",
      "Epoch 619: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 619: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 619: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7008 - accuracy: 0.6786 - val_loss: 1.0121 - val_accuracy: 0.4286\n",
      "Epoch 620/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6990 - accuracy: 0.6667 \n",
      "Epoch 620: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 620: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 620: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6990 - accuracy: 0.6667 - val_loss: 1.0216 - val_accuracy: 0.4286\n",
      "Epoch 621/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7106 - accuracy: 0.6905 \n",
      "Epoch 621: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 621: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 621: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.7106 - accuracy: 0.6905 - val_loss: 1.0222 - val_accuracy: 0.4286\n",
      "Epoch 622/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.7024 \n",
      "Epoch 622: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 622: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 622: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6782 - accuracy: 0.7024 - val_loss: 1.0186 - val_accuracy: 0.4286\n",
      "Epoch 623/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.7143 \n",
      "Epoch 623: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 623: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 623: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6671 - accuracy: 0.7143 - val_loss: 1.0300 - val_accuracy: 0.4286\n",
      "Epoch 624/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.6548 \n",
      "Epoch 624: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 624: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 624: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7000 - accuracy: 0.6548 - val_loss: 1.0326 - val_accuracy: 0.4286\n",
      "Epoch 625/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.6429 \n",
      "Epoch 625: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 625: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 625: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7235 - accuracy: 0.6429 - val_loss: 1.0497 - val_accuracy: 0.4762\n",
      "Epoch 626/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7714 - accuracy: 0.6786 \n",
      "Epoch 626: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 626: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 626: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7714 - accuracy: 0.6786 - val_loss: 1.0251 - val_accuracy: 0.5238\n",
      "Epoch 627/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.6786 \n",
      "Epoch 627: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 627: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 627: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7141 - accuracy: 0.6786 - val_loss: 1.0617 - val_accuracy: 0.5238\n",
      "Epoch 628/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7163 - accuracy: 0.6667 \n",
      "Epoch 628: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 628: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 628: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7163 - accuracy: 0.6667 - val_loss: 1.0390 - val_accuracy: 0.4286\n",
      "Epoch 629/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.6786 \n",
      "Epoch 629: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 629: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 629: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7088 - accuracy: 0.6786 - val_loss: 1.0376 - val_accuracy: 0.4286\n",
      "Epoch 630/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.6905 \n",
      "Epoch 630: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 630: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 630: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6982 - accuracy: 0.6905 - val_loss: 1.0632 - val_accuracy: 0.4286\n",
      "Epoch 631/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7119 - accuracy: 0.6667 \n",
      "Epoch 631: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 631: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 631: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7119 - accuracy: 0.6667 - val_loss: 1.0544 - val_accuracy: 0.4286\n",
      "Epoch 632/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7668 - accuracy: 0.6548 \n",
      "Epoch 632: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 632: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 632: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7668 - accuracy: 0.6548 - val_loss: 1.0446 - val_accuracy: 0.4286\n",
      "Epoch 633/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7065 - accuracy: 0.6548 \n",
      "Epoch 633: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 633: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 633: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7065 - accuracy: 0.6548 - val_loss: 1.0653 - val_accuracy: 0.4286\n",
      "Epoch 634/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.7024 \n",
      "Epoch 634: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 634: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 634: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6930 - accuracy: 0.7024 - val_loss: 1.0645 - val_accuracy: 0.4286\n",
      "Epoch 635/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.6786 \n",
      "Epoch 635: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 635: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 635: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.7341 - accuracy: 0.6786 - val_loss: 1.0677 - val_accuracy: 0.4286\n",
      "Epoch 636/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.6786 \n",
      "Epoch 636: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 636: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 636: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7092 - accuracy: 0.6786 - val_loss: 1.0432 - val_accuracy: 0.4286\n",
      "Epoch 637/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.6667 \n",
      "Epoch 637: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 637: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 637: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6906 - accuracy: 0.6667 - val_loss: 1.0627 - val_accuracy: 0.4762\n",
      "Epoch 638/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6614 - accuracy: 0.7143 \n",
      "Epoch 638: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 638: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 638: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6614 - accuracy: 0.7143 - val_loss: 1.0391 - val_accuracy: 0.5238\n",
      "Epoch 639/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7152 - accuracy: 0.6429 \n",
      "Epoch 639: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 639: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 639: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7152 - accuracy: 0.6429 - val_loss: 1.0229 - val_accuracy: 0.5238\n",
      "Epoch 640/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6669 - accuracy: 0.7024 \n",
      "Epoch 640: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 640: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 640: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6669 - accuracy: 0.7024 - val_loss: 0.9873 - val_accuracy: 0.5238\n",
      "Epoch 641/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.6548 \n",
      "Epoch 641: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 641: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 641: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6883 - accuracy: 0.6548 - val_loss: 0.9670 - val_accuracy: 0.4762\n",
      "Epoch 642/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.6667 \n",
      "Epoch 642: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 642: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 642: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6608 - accuracy: 0.6667 - val_loss: 1.0157 - val_accuracy: 0.4286\n",
      "Epoch 643/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.6905 \n",
      "Epoch 643: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 643: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 643: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6535 - accuracy: 0.6905 - val_loss: 1.0079 - val_accuracy: 0.4286\n",
      "Epoch 644/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.6905 \n",
      "Epoch 644: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 644: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 644: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6728 - accuracy: 0.6905 - val_loss: 0.9628 - val_accuracy: 0.4286\n",
      "Epoch 645/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.6667 \n",
      "Epoch 645: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 645: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 645: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6646 - accuracy: 0.6667 - val_loss: 0.9380 - val_accuracy: 0.4286\n",
      "Epoch 646/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.6786 \n",
      "Epoch 646: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 646: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 646: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6383 - accuracy: 0.6786 - val_loss: 0.9296 - val_accuracy: 0.4762\n",
      "Epoch 647/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6174 - accuracy: 0.7024 \n",
      "Epoch 647: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 647: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 647: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6174 - accuracy: 0.7024 - val_loss: 0.9246 - val_accuracy: 0.4762\n",
      "Epoch 648/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.6786 \n",
      "Epoch 648: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 648: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 648: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6322 - accuracy: 0.6786 - val_loss: 0.9316 - val_accuracy: 0.4762\n",
      "Epoch 649/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.6190 \n",
      "Epoch 649: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 649: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 649: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6449 - accuracy: 0.6190 - val_loss: 0.9425 - val_accuracy: 0.4762\n",
      "Epoch 650/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.6429 \n",
      "Epoch 650: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 650: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 650: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6183 - accuracy: 0.6429 - val_loss: 0.9438 - val_accuracy: 0.5238\n",
      "Epoch 651/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.6190 \n",
      "Epoch 651: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 651: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 651: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6281 - accuracy: 0.6190 - val_loss: 0.9081 - val_accuracy: 0.5238\n",
      "Epoch 652/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.6905 \n",
      "Epoch 652: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 652: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 652: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6182 - accuracy: 0.6905 - val_loss: 0.9350 - val_accuracy: 0.4762\n",
      "Epoch 653/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6425 - accuracy: 0.6905 \n",
      "Epoch 653: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 653: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 653: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6425 - accuracy: 0.6905 - val_loss: 1.0068 - val_accuracy: 0.4286\n",
      "Epoch 654/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.6667 \n",
      "Epoch 654: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 654: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 654: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6395 - accuracy: 0.6667 - val_loss: 0.9601 - val_accuracy: 0.4286\n",
      "Epoch 655/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.7143 \n",
      "Epoch 655: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 655: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.90324\n",
      "\n",
      "Epoch 655: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5842 - accuracy: 0.7143 - val_loss: 0.9098 - val_accuracy: 0.4762\n",
      "Epoch 656/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.7143 \n",
      "Epoch 656: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 656: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 656: val_loss improved from 0.90324 to 0.89025, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 656: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5881 - accuracy: 0.7143 - val_loss: 0.8902 - val_accuracy: 0.4762\n",
      "Epoch 657/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.6667 \n",
      "Epoch 657: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 657: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.89025\n",
      "\n",
      "Epoch 657: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6021 - accuracy: 0.6667 - val_loss: 0.8903 - val_accuracy: 0.4762\n",
      "Epoch 658/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.6190 \n",
      "Epoch 658: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 658: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 658: val_loss improved from 0.89025 to 0.87873, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 658: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6241 - accuracy: 0.6190 - val_loss: 0.8787 - val_accuracy: 0.4762\n",
      "Epoch 659/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6008 - accuracy: 0.6667 \n",
      "Epoch 659: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 659: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 659: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6008 - accuracy: 0.6667 - val_loss: 0.9548 - val_accuracy: 0.4286\n",
      "Epoch 660/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.6310 \n",
      "Epoch 660: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 660: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 660: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6711 - accuracy: 0.6310 - val_loss: 0.8989 - val_accuracy: 0.4286\n",
      "Epoch 661/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6487 - accuracy: 0.6548 \n",
      "Epoch 661: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 661: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 661: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6487 - accuracy: 0.6548 - val_loss: 0.9281 - val_accuracy: 0.4762\n",
      "Epoch 662/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.6667 \n",
      "Epoch 662: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 662: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 662: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6256 - accuracy: 0.6667 - val_loss: 0.9334 - val_accuracy: 0.4762\n",
      "Epoch 663/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6635 - accuracy: 0.6548 \n",
      "Epoch 663: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 663: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 663: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6635 - accuracy: 0.6548 - val_loss: 1.0749 - val_accuracy: 0.3810\n",
      "Epoch 664/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6699 - accuracy: 0.6667 \n",
      "Epoch 664: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 664: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 664: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6699 - accuracy: 0.6667 - val_loss: 0.9370 - val_accuracy: 0.4286\n",
      "Epoch 665/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.6786 \n",
      "Epoch 665: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 665: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 665: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6630 - accuracy: 0.6786 - val_loss: 0.9346 - val_accuracy: 0.4762\n",
      "Epoch 666/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.6548 \n",
      "Epoch 666: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 666: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 666: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6060 - accuracy: 0.6548 - val_loss: 0.9491 - val_accuracy: 0.5238\n",
      "Epoch 667/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.6905 \n",
      "Epoch 667: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 667: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 667: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6476 - accuracy: 0.6905 - val_loss: 0.9306 - val_accuracy: 0.4762\n",
      "Epoch 668/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.6429 \n",
      "Epoch 668: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 668: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 668: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7116 - accuracy: 0.6429 - val_loss: 1.1025 - val_accuracy: 0.3810\n",
      "Epoch 669/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7766 - accuracy: 0.6429 \n",
      "Epoch 669: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 669: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 669: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7766 - accuracy: 0.6429 - val_loss: 1.1915 - val_accuracy: 0.3810\n",
      "Epoch 670/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7599 - accuracy: 0.6310 \n",
      "Epoch 670: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 670: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 670: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7599 - accuracy: 0.6310 - val_loss: 1.0512 - val_accuracy: 0.4762\n",
      "Epoch 671/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.6905 \n",
      "Epoch 671: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 671: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 671: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6705 - accuracy: 0.6905 - val_loss: 1.0590 - val_accuracy: 0.5238\n",
      "Epoch 672/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7756 - accuracy: 0.6310 \n",
      "Epoch 672: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 672: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 672: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7756 - accuracy: 0.6310 - val_loss: 0.9572 - val_accuracy: 0.4286\n",
      "Epoch 673/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.6667 \n",
      "Epoch 673: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 673: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 673: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6735 - accuracy: 0.6667 - val_loss: 0.9321 - val_accuracy: 0.4286\n",
      "Epoch 674/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6606 - accuracy: 0.6786 \n",
      "Epoch 674: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 674: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 674: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6606 - accuracy: 0.6786 - val_loss: 1.0998 - val_accuracy: 0.4286\n",
      "Epoch 675/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.6667 \n",
      "Epoch 675: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 675: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 675: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6828 - accuracy: 0.6667 - val_loss: 1.0516 - val_accuracy: 0.4286\n",
      "Epoch 676/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.6310 \n",
      "Epoch 676: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 676: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 676: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6484 - accuracy: 0.6310 - val_loss: 0.9533 - val_accuracy: 0.4286\n",
      "Epoch 677/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.6786 \n",
      "Epoch 677: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 677: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 677: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6419 - accuracy: 0.6786 - val_loss: 0.9082 - val_accuracy: 0.4286\n",
      "Epoch 678/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6786 \n",
      "Epoch 678: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 678: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.87873\n",
      "\n",
      "Epoch 678: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6458 - accuracy: 0.6786 - val_loss: 0.9146 - val_accuracy: 0.4286\n",
      "Epoch 679/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.7262 \n",
      "Epoch 679: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 679: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 679: val_loss improved from 0.87873 to 0.87772, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 679: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6456 - accuracy: 0.7262 - val_loss: 0.8777 - val_accuracy: 0.4286\n",
      "Epoch 680/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.6786 \n",
      "Epoch 680: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 680: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 680: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6406 - accuracy: 0.6786 - val_loss: 0.9219 - val_accuracy: 0.4286\n",
      "Epoch 681/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.6190 \n",
      "Epoch 681: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 681: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 681: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6567 - accuracy: 0.6190 - val_loss: 0.9040 - val_accuracy: 0.5238\n",
      "Epoch 682/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.6310 \n",
      "Epoch 682: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 682: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 682: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6716 - accuracy: 0.6310 - val_loss: 0.9095 - val_accuracy: 0.5238\n",
      "Epoch 683/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.6190 \n",
      "Epoch 683: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 683: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 683: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6592 - accuracy: 0.6190 - val_loss: 0.9227 - val_accuracy: 0.5238\n",
      "Epoch 684/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.6905 \n",
      "Epoch 684: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 684: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 684: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6205 - accuracy: 0.6905 - val_loss: 0.9606 - val_accuracy: 0.4286\n",
      "Epoch 685/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.7024 \n",
      "Epoch 685: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 685: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 685: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6120 - accuracy: 0.7024 - val_loss: 1.0045 - val_accuracy: 0.4286\n",
      "Epoch 686/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.6786 \n",
      "Epoch 686: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 686: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 686: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6288 - accuracy: 0.6786 - val_loss: 1.0107 - val_accuracy: 0.4286\n",
      "Epoch 687/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.7143 \n",
      "Epoch 687: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 687: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 687: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6324 - accuracy: 0.7143 - val_loss: 0.9628 - val_accuracy: 0.4286\n",
      "Epoch 688/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.6310 \n",
      "Epoch 688: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 688: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 688: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6728 - accuracy: 0.6310 - val_loss: 0.9636 - val_accuracy: 0.4286\n",
      "Epoch 689/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.6667 \n",
      "Epoch 689: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 689: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 689: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6274 - accuracy: 0.6667 - val_loss: 0.9820 - val_accuracy: 0.4762\n",
      "Epoch 690/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.6310 \n",
      "Epoch 690: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 690: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 690: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.6210 - accuracy: 0.6310 - val_loss: 1.0318 - val_accuracy: 0.4286\n",
      "Epoch 691/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.6905 \n",
      "Epoch 691: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 691: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 691: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6245 - accuracy: 0.6905 - val_loss: 0.9676 - val_accuracy: 0.4286\n",
      "Epoch 692/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.6905 \n",
      "Epoch 692: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 692: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 692: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5920 - accuracy: 0.6905 - val_loss: 0.9091 - val_accuracy: 0.4762\n",
      "Epoch 693/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.6905 \n",
      "Epoch 693: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 693: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 693: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6309 - accuracy: 0.6905 - val_loss: 0.9052 - val_accuracy: 0.4762\n",
      "Epoch 694/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.6310 \n",
      "Epoch 694: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 694: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.87772\n",
      "\n",
      "Epoch 694: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6892 - accuracy: 0.6310 - val_loss: 0.8949 - val_accuracy: 0.4762\n",
      "Epoch 695/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.7024 \n",
      "Epoch 695: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 695: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 695: val_loss improved from 0.87772 to 0.87561, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 695: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6216 - accuracy: 0.7024 - val_loss: 0.8756 - val_accuracy: 0.5238\n",
      "Epoch 696/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.6429 \n",
      "Epoch 696: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 696: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 696: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7249 - accuracy: 0.6429 - val_loss: 0.9209 - val_accuracy: 0.5238\n",
      "Epoch 697/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.6667 \n",
      "Epoch 697: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 697: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 697: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5911 - accuracy: 0.6667 - val_loss: 1.0159 - val_accuracy: 0.4286\n",
      "Epoch 698/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.6667 \n",
      "Epoch 698: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 698: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 698: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6629 - accuracy: 0.6667 - val_loss: 1.0588 - val_accuracy: 0.4286\n",
      "Epoch 699/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.6548 \n",
      "Epoch 699: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 699: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 699: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6899 - accuracy: 0.6548 - val_loss: 1.0069 - val_accuracy: 0.4286\n",
      "Epoch 700/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.6548 \n",
      "Epoch 700: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 700: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 700: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6265 - accuracy: 0.6548 - val_loss: 0.9136 - val_accuracy: 0.4286\n",
      "Epoch 701/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.6905 \n",
      "Epoch 701: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 701: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 701: val_loss did not improve from 0.87561\n",
      "\n",
      "Epoch 701: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6288 - accuracy: 0.6905 - val_loss: 0.8929 - val_accuracy: 0.4762\n",
      "Epoch 702/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.6310 \n",
      "Epoch 702: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 702: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 702: val_loss improved from 0.87561 to 0.84795, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 702: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6356 - accuracy: 0.6310 - val_loss: 0.8479 - val_accuracy: 0.4762\n",
      "Epoch 703/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.5952 \n",
      "Epoch 703: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 703: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 703: val_loss improved from 0.84795 to 0.83034, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 703: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6308 - accuracy: 0.5952 - val_loss: 0.8303 - val_accuracy: 0.4762\n",
      "Epoch 704/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.6429 \n",
      "Epoch 704: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 704: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 704: val_loss did not improve from 0.83034\n",
      "\n",
      "Epoch 704: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6148 - accuracy: 0.6429 - val_loss: 0.8807 - val_accuracy: 0.4286\n",
      "Epoch 705/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.6429 \n",
      "Epoch 705: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 705: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 705: val_loss improved from 0.83034 to 0.82165, saving model to best_model_val_loss.h5\n",
      "\n",
      "Epoch 705: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6296 - accuracy: 0.6429 - val_loss: 0.8216 - val_accuracy: 0.4762\n",
      "Epoch 706/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6491 - accuracy: 0.6786 \n",
      "Epoch 706: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 706: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 706: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 706: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6491 - accuracy: 0.6786 - val_loss: 0.8701 - val_accuracy: 0.4286\n",
      "Epoch 707/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.6786 \n",
      "Epoch 707: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 707: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 707: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 707: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6420 - accuracy: 0.6786 - val_loss: 1.0555 - val_accuracy: 0.4286\n",
      "Epoch 708/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.6429 \n",
      "Epoch 708: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 708: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 708: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 708: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7138 - accuracy: 0.6429 - val_loss: 0.9439 - val_accuracy: 0.4286\n",
      "Epoch 709/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.6905 \n",
      "Epoch 709: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 709: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 709: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 709: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5896 - accuracy: 0.6905 - val_loss: 0.8897 - val_accuracy: 0.4762\n",
      "Epoch 710/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.6667 \n",
      "Epoch 710: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 710: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 710: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 710: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6247 - accuracy: 0.6667 - val_loss: 0.8482 - val_accuracy: 0.5238\n",
      "Epoch 711/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.6667 \n",
      "Epoch 711: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 711: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 711: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 711: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6557 - accuracy: 0.6667 - val_loss: 0.8410 - val_accuracy: 0.5238\n",
      "Epoch 712/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.6190 \n",
      "Epoch 712: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 712: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 712: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 712: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7209 - accuracy: 0.6190 - val_loss: 0.9136 - val_accuracy: 0.4762\n",
      "Epoch 713/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.6905 \n",
      "Epoch 713: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 713: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 713: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 713: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6420 - accuracy: 0.6905 - val_loss: 0.9717 - val_accuracy: 0.4286\n",
      "Epoch 714/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6486 - accuracy: 0.6548 \n",
      "Epoch 714: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 714: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 714: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 714: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6486 - accuracy: 0.6548 - val_loss: 0.9509 - val_accuracy: 0.4286\n",
      "Epoch 715/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.6905 \n",
      "Epoch 715: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 715: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 715: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 715: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6328 - accuracy: 0.6905 - val_loss: 0.9093 - val_accuracy: 0.4286\n",
      "Epoch 716/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6786 \n",
      "Epoch 716: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 716: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 716: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 716: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6134 - accuracy: 0.6786 - val_loss: 0.9333 - val_accuracy: 0.5238\n",
      "Epoch 717/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.6429 \n",
      "Epoch 717: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 717: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 717: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 717: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6373 - accuracy: 0.6429 - val_loss: 0.9384 - val_accuracy: 0.4286\n",
      "Epoch 718/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.6190 \n",
      "Epoch 718: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 718: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 718: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 718: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7710 - accuracy: 0.6190 - val_loss: 0.8900 - val_accuracy: 0.4286\n",
      "Epoch 719/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6460 - accuracy: 0.6310 \n",
      "Epoch 719: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 719: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 719: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 719: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6460 - accuracy: 0.6310 - val_loss: 1.2454 - val_accuracy: 0.3810\n",
      "Epoch 720/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8328 - accuracy: 0.5714 \n",
      "Epoch 720: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 720: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 720: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 720: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8328 - accuracy: 0.5714 - val_loss: 1.1499 - val_accuracy: 0.3810\n",
      "Epoch 721/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.6071 \n",
      "Epoch 721: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 721: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 721: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 721: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7376 - accuracy: 0.6071 - val_loss: 0.9776 - val_accuracy: 0.3810\n",
      "Epoch 722/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.6548 \n",
      "Epoch 722: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 722: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 722: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 722: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6946 - accuracy: 0.6548 - val_loss: 0.9585 - val_accuracy: 0.5238\n",
      "Epoch 723/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7163 - accuracy: 0.6190 \n",
      "Epoch 723: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 723: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 723: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 723: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7163 - accuracy: 0.6190 - val_loss: 0.9170 - val_accuracy: 0.4762\n",
      "Epoch 724/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.6310 \n",
      "Epoch 724: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 724: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 724: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 724: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6832 - accuracy: 0.6310 - val_loss: 0.9171 - val_accuracy: 0.4762\n",
      "Epoch 725/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.6429 \n",
      "Epoch 725: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 725: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 725: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 725: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6928 - accuracy: 0.6429 - val_loss: 0.9216 - val_accuracy: 0.4762\n",
      "Epoch 726/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.6667 \n",
      "Epoch 726: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 726: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 726: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 726: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6697 - accuracy: 0.6667 - val_loss: 0.9373 - val_accuracy: 0.5238\n",
      "Epoch 727/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.6548 \n",
      "Epoch 727: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 727: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 727: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 727: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6479 - accuracy: 0.6548 - val_loss: 0.9766 - val_accuracy: 0.4286\n",
      "Epoch 728/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6399 - accuracy: 0.6786 \n",
      "Epoch 728: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 728: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 728: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 728: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6399 - accuracy: 0.6786 - val_loss: 0.9442 - val_accuracy: 0.4286\n",
      "Epoch 729/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6667 \n",
      "Epoch 729: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 729: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 729: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 729: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6496 - accuracy: 0.6667 - val_loss: 0.9291 - val_accuracy: 0.4286\n",
      "Epoch 730/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6056 - accuracy: 0.6905 \n",
      "Epoch 730: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 730: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 730: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 730: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6056 - accuracy: 0.6905 - val_loss: 0.9294 - val_accuracy: 0.4286\n",
      "Epoch 731/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.6429 \n",
      "Epoch 731: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 731: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 731: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 731: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6843 - accuracy: 0.6429 - val_loss: 0.9249 - val_accuracy: 0.4286\n",
      "Epoch 732/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6193 - accuracy: 0.6786 \n",
      "Epoch 732: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 732: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 732: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 732: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6193 - accuracy: 0.6786 - val_loss: 0.9178 - val_accuracy: 0.4286\n",
      "Epoch 733/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.6905 \n",
      "Epoch 733: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 733: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 733: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 733: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6283 - accuracy: 0.6905 - val_loss: 0.9066 - val_accuracy: 0.4286\n",
      "Epoch 734/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.7024 \n",
      "Epoch 734: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 734: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 734: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 734: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6387 - accuracy: 0.7024 - val_loss: 0.9163 - val_accuracy: 0.4286\n",
      "Epoch 735/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.6905 \n",
      "Epoch 735: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 735: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 735: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 735: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 18s/step - loss: 0.6158 - accuracy: 0.6905 - val_loss: 0.9153 - val_accuracy: 0.4286\n",
      "Epoch 736/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.6667 \n",
      "Epoch 736: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 736: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 736: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 736: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6315 - accuracy: 0.6667 - val_loss: 0.9216 - val_accuracy: 0.4286\n",
      "Epoch 737/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.6786 \n",
      "Epoch 737: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 737: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 737: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 737: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5956 - accuracy: 0.6786 - val_loss: 0.9341 - val_accuracy: 0.4286\n",
      "Epoch 738/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6151 - accuracy: 0.6548 \n",
      "Epoch 738: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 738: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 738: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 738: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6151 - accuracy: 0.6548 - val_loss: 0.9130 - val_accuracy: 0.4286\n",
      "Epoch 739/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.6429 \n",
      "Epoch 739: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 739: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 739: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 739: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6071 - accuracy: 0.6429 - val_loss: 0.9269 - val_accuracy: 0.4286\n",
      "Epoch 740/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.6548 \n",
      "Epoch 740: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 740: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 740: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 740: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6069 - accuracy: 0.6548 - val_loss: 0.9178 - val_accuracy: 0.4286\n",
      "Epoch 741/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.6667 \n",
      "Epoch 741: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 741: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 741: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 741: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6579 - accuracy: 0.6667 - val_loss: 0.8637 - val_accuracy: 0.4286\n",
      "Epoch 742/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.7381 \n",
      "Epoch 742: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 742: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 742: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 742: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6339 - accuracy: 0.7381 - val_loss: 0.9007 - val_accuracy: 0.4286\n",
      "Epoch 743/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.5833 \n",
      "Epoch 743: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 743: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 743: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 743: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6996 - accuracy: 0.5833 - val_loss: 0.9187 - val_accuracy: 0.4286\n",
      "Epoch 744/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.6548 \n",
      "Epoch 744: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 744: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 744: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 744: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6147 - accuracy: 0.6548 - val_loss: 0.9252 - val_accuracy: 0.4286\n",
      "Epoch 745/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.6786 \n",
      "Epoch 745: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 745: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 745: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 745: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6277 - accuracy: 0.6786 - val_loss: 0.9515 - val_accuracy: 0.4286\n",
      "Epoch 746/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7772 - accuracy: 0.6548 \n",
      "Epoch 746: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 746: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 746: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 746: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7772 - accuracy: 0.6548 - val_loss: 0.9374 - val_accuracy: 0.4286\n",
      "Epoch 747/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.6786 \n",
      "Epoch 747: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 747: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 747: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 747: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6259 - accuracy: 0.6786 - val_loss: 0.9478 - val_accuracy: 0.4286\n",
      "Epoch 748/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.6786 \n",
      "Epoch 748: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 748: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 748: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 748: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6397 - accuracy: 0.6786 - val_loss: 0.9521 - val_accuracy: 0.4286\n",
      "Epoch 749/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.6667 \n",
      "Epoch 749: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 749: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 749: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 749: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6713 - accuracy: 0.6667 - val_loss: 0.9582 - val_accuracy: 0.4286\n",
      "Epoch 750/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.6905 \n",
      "Epoch 750: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 750: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 750: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 750: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6456 - accuracy: 0.6905 - val_loss: 0.9629 - val_accuracy: 0.4286\n",
      "Epoch 751/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6635 - accuracy: 0.6786 \n",
      "Epoch 751: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 751: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 751: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 751: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6635 - accuracy: 0.6786 - val_loss: 0.9723 - val_accuracy: 0.4286\n",
      "Epoch 752/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.7024 \n",
      "Epoch 752: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 752: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 752: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 752: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.6061 - accuracy: 0.7024 - val_loss: 0.9742 - val_accuracy: 0.4286\n",
      "Epoch 753/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.6786 \n",
      "Epoch 753: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 753: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 753: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 753: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6373 - accuracy: 0.6786 - val_loss: 0.9706 - val_accuracy: 0.4286\n",
      "Epoch 754/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7024 \n",
      "Epoch 754: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 754: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 754: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 754: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6096 - accuracy: 0.7024 - val_loss: 0.9425 - val_accuracy: 0.4286\n",
      "Epoch 755/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.7381 \n",
      "Epoch 755: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 755: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 755: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 755: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5817 - accuracy: 0.7381 - val_loss: 0.9256 - val_accuracy: 0.4286\n",
      "Epoch 756/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7024 \n",
      "Epoch 756: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 756: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 756: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 756: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6083 - accuracy: 0.7024 - val_loss: 0.9301 - val_accuracy: 0.4286\n",
      "Epoch 757/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.6071 \n",
      "Epoch 757: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 757: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 757: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 757: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6168 - accuracy: 0.6071 - val_loss: 0.9291 - val_accuracy: 0.4286\n",
      "Epoch 758/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.6548 \n",
      "Epoch 758: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 758: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 758: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 758: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6017 - accuracy: 0.6548 - val_loss: 0.9268 - val_accuracy: 0.4286\n",
      "Epoch 759/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.6667 \n",
      "Epoch 759: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 759: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 759: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 759: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6022 - accuracy: 0.6667 - val_loss: 0.9365 - val_accuracy: 0.4286\n",
      "Epoch 760/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.7024 \n",
      "Epoch 760: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 760: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 760: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 760: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5853 - accuracy: 0.7024 - val_loss: 0.9510 - val_accuracy: 0.4286\n",
      "Epoch 761/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.7024 \n",
      "Epoch 761: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 761: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 761: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 761: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5835 - accuracy: 0.7024 - val_loss: 0.9517 - val_accuracy: 0.4286\n",
      "Epoch 762/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.6905 \n",
      "Epoch 762: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 762: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 762: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 762: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5786 - accuracy: 0.6905 - val_loss: 0.9438 - val_accuracy: 0.4286\n",
      "Epoch 763/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.7024 \n",
      "Epoch 763: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 763: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 763: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 763: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5698 - accuracy: 0.7024 - val_loss: 0.9371 - val_accuracy: 0.4286\n",
      "Epoch 764/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5764 - accuracy: 0.7143 \n",
      "Epoch 764: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 764: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 764: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 764: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5764 - accuracy: 0.7143 - val_loss: 0.9400 - val_accuracy: 0.4286\n",
      "Epoch 765/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5839 - accuracy: 0.6786 \n",
      "Epoch 765: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 765: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 765: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 765: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5839 - accuracy: 0.6786 - val_loss: 0.9237 - val_accuracy: 0.4286\n",
      "Epoch 766/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7144 - accuracy: 0.6310 \n",
      "Epoch 766: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 766: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 766: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 766: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7144 - accuracy: 0.6310 - val_loss: 0.9419 - val_accuracy: 0.4286\n",
      "Epoch 767/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6264 - accuracy: 0.7143 \n",
      "Epoch 767: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 767: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 767: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 767: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6264 - accuracy: 0.7143 - val_loss: 1.0872 - val_accuracy: 0.4286\n",
      "Epoch 768/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.6429 \n",
      "Epoch 768: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 768: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 768: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 768: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6857 - accuracy: 0.6429 - val_loss: 1.1426 - val_accuracy: 0.4286\n",
      "Epoch 769/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.6429 \n",
      "Epoch 769: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 769: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 769: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 769: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7017 - accuracy: 0.6429 - val_loss: 0.9144 - val_accuracy: 0.4286\n",
      "Epoch 770/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.6786 \n",
      "Epoch 770: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 770: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 770: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 770: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6709 - accuracy: 0.6786 - val_loss: 0.9607 - val_accuracy: 0.4286\n",
      "Epoch 771/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.6548 \n",
      "Epoch 771: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 771: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 771: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 771: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6265 - accuracy: 0.6548 - val_loss: 0.9817 - val_accuracy: 0.4762\n",
      "Epoch 772/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.6310 \n",
      "Epoch 772: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 772: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 772: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 772: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.7250 - accuracy: 0.6310 - val_loss: 0.9784 - val_accuracy: 0.4762\n",
      "Epoch 773/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7405 - accuracy: 0.6905 \n",
      "Epoch 773: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 773: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 773: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 773: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7405 - accuracy: 0.6905 - val_loss: 0.9972 - val_accuracy: 0.4762\n",
      "Epoch 774/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.6905 \n",
      "Epoch 774: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 774: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 774: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 774: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6380 - accuracy: 0.6905 - val_loss: 0.9632 - val_accuracy: 0.4286\n",
      "Epoch 775/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.6786 \n",
      "Epoch 775: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 775: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 775: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 775: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6139 - accuracy: 0.6786 - val_loss: 0.9705 - val_accuracy: 0.4286\n",
      "Epoch 776/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.6548 \n",
      "Epoch 776: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 776: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 776: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 776: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6019 - accuracy: 0.6548 - val_loss: 0.9628 - val_accuracy: 0.4286\n",
      "Epoch 777/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.6905 \n",
      "Epoch 777: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 777: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 777: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 777: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6221 - accuracy: 0.6905 - val_loss: 0.9498 - val_accuracy: 0.4286\n",
      "Epoch 778/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.6667 \n",
      "Epoch 778: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 778: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 778: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 778: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6035 - accuracy: 0.6667 - val_loss: 0.9386 - val_accuracy: 0.4286\n",
      "Epoch 779/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.6905 \n",
      "Epoch 779: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 779: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 779: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 779: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6065 - accuracy: 0.6905 - val_loss: 0.9372 - val_accuracy: 0.4286\n",
      "Epoch 780/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.6548 \n",
      "Epoch 780: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 780: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 780: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 780: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5870 - accuracy: 0.6548 - val_loss: 0.9307 - val_accuracy: 0.4286\n",
      "Epoch 781/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.6786 \n",
      "Epoch 781: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 781: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 781: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 781: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5999 - accuracy: 0.6786 - val_loss: 0.9227 - val_accuracy: 0.4286\n",
      "Epoch 782/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.6667 \n",
      "Epoch 782: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 782: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 782: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 782: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5719 - accuracy: 0.6667 - val_loss: 0.9152 - val_accuracy: 0.4286\n",
      "Epoch 783/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.6548 \n",
      "Epoch 783: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 783: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 783: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 783: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6092 - accuracy: 0.6548 - val_loss: 0.9142 - val_accuracy: 0.4286\n",
      "Epoch 784/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6260 - accuracy: 0.6667 \n",
      "Epoch 784: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 784: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 784: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 784: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6260 - accuracy: 0.6667 - val_loss: 0.9218 - val_accuracy: 0.4286\n",
      "Epoch 785/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.6667 \n",
      "Epoch 785: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 785: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 785: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 785: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6671 - accuracy: 0.6667 - val_loss: 0.9436 - val_accuracy: 0.4286\n",
      "Epoch 786/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.6667 \n",
      "Epoch 786: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 786: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 786: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 786: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5824 - accuracy: 0.6667 - val_loss: 0.9239 - val_accuracy: 0.4286\n",
      "Epoch 787/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7262 \n",
      "Epoch 787: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 787: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 787: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 787: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5850 - accuracy: 0.7262 - val_loss: 0.9298 - val_accuracy: 0.4286\n",
      "Epoch 788/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6493 - accuracy: 0.6429 \n",
      "Epoch 788: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 788: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 788: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 788: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6493 - accuracy: 0.6429 - val_loss: 0.9248 - val_accuracy: 0.4762\n",
      "Epoch 789/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5749 - accuracy: 0.7262 \n",
      "Epoch 789: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 789: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 789: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 789: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5749 - accuracy: 0.7262 - val_loss: 0.9353 - val_accuracy: 0.4286\n",
      "Epoch 790/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.6667 \n",
      "Epoch 790: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 790: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 790: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 790: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6111 - accuracy: 0.6667 - val_loss: 0.9214 - val_accuracy: 0.4762\n",
      "Epoch 791/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.6548 \n",
      "Epoch 791: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 791: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 791: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 791: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6412 - accuracy: 0.6548 - val_loss: 0.9313 - val_accuracy: 0.4762\n",
      "Epoch 792/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.6786 \n",
      "Epoch 792: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 792: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 792: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 792: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6443 - accuracy: 0.6786 - val_loss: 0.9391 - val_accuracy: 0.4762\n",
      "Epoch 793/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.6548 \n",
      "Epoch 793: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 793: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 793: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 793: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6125 - accuracy: 0.6548 - val_loss: 0.9813 - val_accuracy: 0.4762\n",
      "Epoch 794/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6578 - accuracy: 0.6786 \n",
      "Epoch 794: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 794: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 794: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 794: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6578 - accuracy: 0.6786 - val_loss: 0.9629 - val_accuracy: 0.4762\n",
      "Epoch 795/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.6786 \n",
      "Epoch 795: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 795: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 795: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 795: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6546 - accuracy: 0.6786 - val_loss: 0.9577 - val_accuracy: 0.4762\n",
      "Epoch 796/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.6190 \n",
      "Epoch 796: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 796: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 796: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 796: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6422 - accuracy: 0.6190 - val_loss: 0.9502 - val_accuracy: 0.4762\n",
      "Epoch 797/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7163 - accuracy: 0.6667 \n",
      "Epoch 797: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 797: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 797: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 797: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.7163 - accuracy: 0.6667 - val_loss: 0.9252 - val_accuracy: 0.4286\n",
      "Epoch 798/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.6786 \n",
      "Epoch 798: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 798: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 798: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 798: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6061 - accuracy: 0.6786 - val_loss: 0.9241 - val_accuracy: 0.4286\n",
      "Epoch 799/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.6905 \n",
      "Epoch 799: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 799: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 799: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 799: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.6146 - accuracy: 0.6905 - val_loss: 0.9337 - val_accuracy: 0.4286\n",
      "Epoch 800/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.6667 \n",
      "Epoch 800: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 800: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 800: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 800: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6923 - accuracy: 0.6667 - val_loss: 0.9358 - val_accuracy: 0.4286\n",
      "Epoch 801/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.6905 \n",
      "Epoch 801: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 801: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 801: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 801: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.7210 - accuracy: 0.6905 - val_loss: 0.9912 - val_accuracy: 0.4286\n",
      "Epoch 802/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.6905 \n",
      "Epoch 802: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 802: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 802: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 802: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.6962 - accuracy: 0.6905 - val_loss: 1.0625 - val_accuracy: 0.4286\n",
      "Epoch 803/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.7024 \n",
      "Epoch 803: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 803: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 803: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 803: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6854 - accuracy: 0.7024 - val_loss: 0.9802 - val_accuracy: 0.4286\n",
      "Epoch 804/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6687 - accuracy: 0.6667 \n",
      "Epoch 804: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 804: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 804: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 804: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6687 - accuracy: 0.6667 - val_loss: 0.9277 - val_accuracy: 0.4286\n",
      "Epoch 805/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.6786 \n",
      "Epoch 805: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 805: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 805: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 805: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6295 - accuracy: 0.6786 - val_loss: 0.9084 - val_accuracy: 0.4286\n",
      "Epoch 806/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.6548 \n",
      "Epoch 806: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 806: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 806: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 806: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6803 - accuracy: 0.6548 - val_loss: 0.8946 - val_accuracy: 0.4286\n",
      "Epoch 807/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.6905 \n",
      "Epoch 807: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 807: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 807: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 807: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6573 - accuracy: 0.6905 - val_loss: 0.8688 - val_accuracy: 0.4762\n",
      "Epoch 808/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.7262 \n",
      "Epoch 808: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 808: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 808: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 808: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6442 - accuracy: 0.7262 - val_loss: 0.8817 - val_accuracy: 0.4762\n",
      "Epoch 809/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.6429 \n",
      "Epoch 809: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 809: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 809: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 809: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6505 - accuracy: 0.6429 - val_loss: 0.8996 - val_accuracy: 0.4286\n",
      "Epoch 810/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.6548 \n",
      "Epoch 810: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 810: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 810: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 810: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6291 - accuracy: 0.6548 - val_loss: 0.9148 - val_accuracy: 0.4286\n",
      "Epoch 811/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.7024 \n",
      "Epoch 811: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 811: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 811: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 811: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 52s 19s/step - loss: 0.6041 - accuracy: 0.7024 - val_loss: 0.9272 - val_accuracy: 0.4286\n",
      "Epoch 812/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.6786 \n",
      "Epoch 812: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 812: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 812: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 812: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6415 - accuracy: 0.6786 - val_loss: 0.9363 - val_accuracy: 0.4286\n",
      "Epoch 813/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.6786 \n",
      "Epoch 813: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 813: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 813: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 813: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6012 - accuracy: 0.6786 - val_loss: 0.9411 - val_accuracy: 0.4286\n",
      "Epoch 814/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.6786 \n",
      "Epoch 814: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 814: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 814: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 814: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6040 - accuracy: 0.6786 - val_loss: 0.9322 - val_accuracy: 0.4286\n",
      "Epoch 815/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6170 - accuracy: 0.7024 \n",
      "Epoch 815: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 815: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 815: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 815: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6170 - accuracy: 0.7024 - val_loss: 0.9193 - val_accuracy: 0.4762\n",
      "Epoch 816/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.7024 \n",
      "Epoch 816: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 816: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 816: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 816: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5987 - accuracy: 0.7024 - val_loss: 0.9165 - val_accuracy: 0.4762\n",
      "Epoch 817/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6905 \n",
      "Epoch 817: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 817: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 817: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 817: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6134 - accuracy: 0.6905 - val_loss: 0.9331 - val_accuracy: 0.4762\n",
      "Epoch 818/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.6667 \n",
      "Epoch 818: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 818: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 818: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 818: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6371 - accuracy: 0.6667 - val_loss: 0.9370 - val_accuracy: 0.4762\n",
      "Epoch 819/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.7143 \n",
      "Epoch 819: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 819: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 819: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 819: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6005 - accuracy: 0.7143 - val_loss: 0.9186 - val_accuracy: 0.4286\n",
      "Epoch 820/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.6905 \n",
      "Epoch 820: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 820: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 820: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 820: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6038 - accuracy: 0.6905 - val_loss: 0.9192 - val_accuracy: 0.4286\n",
      "Epoch 821/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.7024 \n",
      "Epoch 821: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 821: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 821: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 821: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5806 - accuracy: 0.7024 - val_loss: 0.9171 - val_accuracy: 0.4286\n",
      "Epoch 822/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5892 - accuracy: 0.7262 \n",
      "Epoch 822: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 822: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 822: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 822: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5892 - accuracy: 0.7262 - val_loss: 0.9067 - val_accuracy: 0.4762\n",
      "Epoch 823/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.6786 \n",
      "Epoch 823: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 823: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 823: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 823: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5927 - accuracy: 0.6786 - val_loss: 0.9110 - val_accuracy: 0.4762\n",
      "Epoch 824/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.7262 \n",
      "Epoch 824: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 824: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 824: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 824: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5734 - accuracy: 0.7262 - val_loss: 0.9095 - val_accuracy: 0.4762\n",
      "Epoch 825/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7143 \n",
      "Epoch 825: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 825: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 825: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 825: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5632 - accuracy: 0.7143 - val_loss: 0.8992 - val_accuracy: 0.4762\n",
      "Epoch 826/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.7024 \n",
      "Epoch 826: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 826: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 826: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 826: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5862 - accuracy: 0.7024 - val_loss: 0.8916 - val_accuracy: 0.4762\n",
      "Epoch 827/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6149 - accuracy: 0.6786 \n",
      "Epoch 827: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 827: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 827: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 827: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6149 - accuracy: 0.6786 - val_loss: 0.9062 - val_accuracy: 0.5238\n",
      "Epoch 828/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.6310 \n",
      "Epoch 828: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 828: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 828: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 828: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7394 - accuracy: 0.6310 - val_loss: 0.9068 - val_accuracy: 0.5238\n",
      "Epoch 829/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.6667 \n",
      "Epoch 829: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 829: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 829: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 829: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6950 - accuracy: 0.6667 - val_loss: 0.8742 - val_accuracy: 0.4762\n",
      "Epoch 830/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.6071 \n",
      "Epoch 830: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 830: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 830: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 830: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6274 - accuracy: 0.6071 - val_loss: 0.8786 - val_accuracy: 0.4286\n",
      "Epoch 831/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5963 - accuracy: 0.7262 \n",
      "Epoch 831: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 831: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 831: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 831: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5963 - accuracy: 0.7262 - val_loss: 0.8863 - val_accuracy: 0.4286\n",
      "Epoch 832/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.7262 \n",
      "Epoch 832: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 832: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 832: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 832: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5742 - accuracy: 0.7262 - val_loss: 0.8882 - val_accuracy: 0.4762\n",
      "Epoch 833/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6071 \n",
      "Epoch 833: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 833: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 833: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 833: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6496 - accuracy: 0.6071 - val_loss: 0.8889 - val_accuracy: 0.4286\n",
      "Epoch 834/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.6905 \n",
      "Epoch 834: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 834: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 834: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 834: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6028 - accuracy: 0.6905 - val_loss: 0.9082 - val_accuracy: 0.4286\n",
      "Epoch 835/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.7024 \n",
      "Epoch 835: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 835: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 835: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 835: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6535 - accuracy: 0.7024 - val_loss: 0.9057 - val_accuracy: 0.4762\n",
      "Epoch 836/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6156 - accuracy: 0.6429 \n",
      "Epoch 836: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 836: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 836: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 836: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6156 - accuracy: 0.6429 - val_loss: 0.8932 - val_accuracy: 0.4762\n",
      "Epoch 837/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6486 - accuracy: 0.6667 \n",
      "Epoch 837: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 837: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 837: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 837: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6486 - accuracy: 0.6667 - val_loss: 0.9052 - val_accuracy: 0.4762\n",
      "Epoch 838/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6016 - accuracy: 0.6786 \n",
      "Epoch 838: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 838: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 838: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 838: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6016 - accuracy: 0.6786 - val_loss: 0.9175 - val_accuracy: 0.5238\n",
      "Epoch 839/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.7143 \n",
      "Epoch 839: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 839: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 839: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 839: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5805 - accuracy: 0.7143 - val_loss: 0.9289 - val_accuracy: 0.5238\n",
      "Epoch 840/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.7500 \n",
      "Epoch 840: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 840: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 840: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 840: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5832 - accuracy: 0.7500 - val_loss: 0.9327 - val_accuracy: 0.5238\n",
      "Epoch 841/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6151 - accuracy: 0.6786 \n",
      "Epoch 841: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 841: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 841: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 841: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6151 - accuracy: 0.6786 - val_loss: 0.9354 - val_accuracy: 0.4762\n",
      "Epoch 842/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.6786 \n",
      "Epoch 842: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 842: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 842: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 842: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5938 - accuracy: 0.6786 - val_loss: 0.9471 - val_accuracy: 0.4286\n",
      "Epoch 843/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.6548 \n",
      "Epoch 843: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 843: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 843: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 843: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5941 - accuracy: 0.6548 - val_loss: 0.9494 - val_accuracy: 0.4286\n",
      "Epoch 844/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.6667 \n",
      "Epoch 844: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 844: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 844: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 844: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5941 - accuracy: 0.6667 - val_loss: 0.9431 - val_accuracy: 0.4286\n",
      "Epoch 845/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.7143 \n",
      "Epoch 845: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 845: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 845: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 845: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.5782 - accuracy: 0.7143 - val_loss: 0.9692 - val_accuracy: 0.4286\n",
      "Epoch 846/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.6786 \n",
      "Epoch 846: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 846: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 846: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 846: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6246 - accuracy: 0.6786 - val_loss: 1.0513 - val_accuracy: 0.3810\n",
      "Epoch 847/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.6905 \n",
      "Epoch 847: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 847: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 847: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 847: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6191 - accuracy: 0.6905 - val_loss: 1.0641 - val_accuracy: 0.3810\n",
      "Epoch 848/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.6429 \n",
      "Epoch 848: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 848: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 848: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 848: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6821 - accuracy: 0.6429 - val_loss: 1.0193 - val_accuracy: 0.3810\n",
      "Epoch 849/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.6548 \n",
      "Epoch 849: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 849: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 849: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 849: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6364 - accuracy: 0.6548 - val_loss: 0.9176 - val_accuracy: 0.4762\n",
      "Epoch 850/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.6667 \n",
      "Epoch 850: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 850: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 850: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 850: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.6336 - accuracy: 0.6667 - val_loss: 0.9120 - val_accuracy: 0.4762\n",
      "Epoch 851/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.6667 \n",
      "Epoch 851: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 851: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 851: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 851: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5826 - accuracy: 0.6667 - val_loss: 0.9584 - val_accuracy: 0.4286\n",
      "Epoch 852/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.6429 \n",
      "Epoch 852: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 852: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 852: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 852: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6255 - accuracy: 0.6429 - val_loss: 0.9509 - val_accuracy: 0.4286\n",
      "Epoch 853/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.6667 \n",
      "Epoch 853: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 853: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 853: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 853: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6227 - accuracy: 0.6667 - val_loss: 0.9289 - val_accuracy: 0.4286\n",
      "Epoch 854/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.6548 \n",
      "Epoch 854: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 854: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 854: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 854: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6301 - accuracy: 0.6548 - val_loss: 0.9029 - val_accuracy: 0.4762\n",
      "Epoch 855/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5904 - accuracy: 0.7024 \n",
      "Epoch 855: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 855: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 855: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 855: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5904 - accuracy: 0.7024 - val_loss: 0.8691 - val_accuracy: 0.5238\n",
      "Epoch 856/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.6905 \n",
      "Epoch 856: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 856: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 856: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 856: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5934 - accuracy: 0.6905 - val_loss: 0.8834 - val_accuracy: 0.5238\n",
      "Epoch 857/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.5833 \n",
      "Epoch 857: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 857: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 857: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 857: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6369 - accuracy: 0.5833 - val_loss: 0.8895 - val_accuracy: 0.5238\n",
      "Epoch 858/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.7262 \n",
      "Epoch 858: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 858: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 858: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 858: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5853 - accuracy: 0.7262 - val_loss: 0.8847 - val_accuracy: 0.5238\n",
      "Epoch 859/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6027 - accuracy: 0.6548 \n",
      "Epoch 859: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 859: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 859: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 859: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6027 - accuracy: 0.6548 - val_loss: 0.8827 - val_accuracy: 0.5238\n",
      "Epoch 860/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.6786 \n",
      "Epoch 860: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 860: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 860: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 860: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5979 - accuracy: 0.6786 - val_loss: 0.8862 - val_accuracy: 0.4762\n",
      "Epoch 861/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.6905 \n",
      "Epoch 861: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 861: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 861: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 861: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5938 - accuracy: 0.6905 - val_loss: 0.9008 - val_accuracy: 0.4762\n",
      "Epoch 862/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5834 - accuracy: 0.7024 \n",
      "Epoch 862: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 862: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 862: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 862: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5834 - accuracy: 0.7024 - val_loss: 0.9152 - val_accuracy: 0.4762\n",
      "Epoch 863/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.6429 \n",
      "Epoch 863: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 863: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 863: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 863: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5937 - accuracy: 0.6429 - val_loss: 0.9188 - val_accuracy: 0.4762\n",
      "Epoch 864/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.7024 \n",
      "Epoch 864: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 864: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 864: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 864: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5614 - accuracy: 0.7024 - val_loss: 0.9173 - val_accuracy: 0.4762\n",
      "Epoch 865/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.7262 \n",
      "Epoch 865: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 865: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 865: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 865: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5553 - accuracy: 0.7262 - val_loss: 0.9211 - val_accuracy: 0.4762\n",
      "Epoch 866/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.6667 \n",
      "Epoch 866: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 866: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 866: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 866: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5659 - accuracy: 0.6667 - val_loss: 0.9209 - val_accuracy: 0.4762\n",
      "Epoch 867/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7143 \n",
      "Epoch 867: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 867: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 867: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 867: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5610 - accuracy: 0.7143 - val_loss: 0.9185 - val_accuracy: 0.4762\n",
      "Epoch 868/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7024 \n",
      "Epoch 868: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 868: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 868: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 868: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5477 - accuracy: 0.7024 - val_loss: 0.9106 - val_accuracy: 0.4762\n",
      "Epoch 869/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.6310 \n",
      "Epoch 869: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 869: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 869: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 869: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5730 - accuracy: 0.6310 - val_loss: 0.9104 - val_accuracy: 0.4762\n",
      "Epoch 870/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.7262 \n",
      "Epoch 870: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 870: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 870: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 870: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 47s 17s/step - loss: 0.5647 - accuracy: 0.7262 - val_loss: 0.9098 - val_accuracy: 0.4762\n",
      "Epoch 871/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.6905 \n",
      "Epoch 871: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 871: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 871: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 871: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5669 - accuracy: 0.6905 - val_loss: 0.9026 - val_accuracy: 0.4762\n",
      "Epoch 872/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.7024 \n",
      "Epoch 872: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 872: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 872: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 872: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5725 - accuracy: 0.7024 - val_loss: 0.8883 - val_accuracy: 0.4762\n",
      "Epoch 873/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5718 - accuracy: 0.6905 \n",
      "Epoch 873: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 873: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 873: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 873: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5718 - accuracy: 0.6905 - val_loss: 0.8800 - val_accuracy: 0.5238\n",
      "Epoch 874/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.6667 \n",
      "Epoch 874: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 874: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 874: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 874: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5731 - accuracy: 0.6667 - val_loss: 0.8880 - val_accuracy: 0.4762\n",
      "Epoch 875/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.7024 \n",
      "Epoch 875: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 875: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 875: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 875: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5640 - accuracy: 0.7024 - val_loss: 0.8788 - val_accuracy: 0.5238\n",
      "Epoch 876/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.6905 \n",
      "Epoch 876: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 876: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 876: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 876: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5905 - accuracy: 0.6905 - val_loss: 1.1920 - val_accuracy: 0.3810\n",
      "Epoch 877/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7640 - accuracy: 0.6310 \n",
      "Epoch 877: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 877: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 877: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 877: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7640 - accuracy: 0.6310 - val_loss: 1.2365 - val_accuracy: 0.4286\n",
      "Epoch 878/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.6190 \n",
      "Epoch 878: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 878: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 878: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 878: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7579 - accuracy: 0.6190 - val_loss: 1.0520 - val_accuracy: 0.4286\n",
      "Epoch 879/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.6667 \n",
      "Epoch 879: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 879: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 879: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 879: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7123 - accuracy: 0.6667 - val_loss: 0.9250 - val_accuracy: 0.4286\n",
      "Epoch 880/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.6548 \n",
      "Epoch 880: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 880: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 880: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 880: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6246 - accuracy: 0.6548 - val_loss: 0.9087 - val_accuracy: 0.4762\n",
      "Epoch 881/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.6071 \n",
      "Epoch 881: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 881: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 881: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 881: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6298 - accuracy: 0.6071 - val_loss: 0.8660 - val_accuracy: 0.5238\n",
      "Epoch 882/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.6667 \n",
      "Epoch 882: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 882: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 882: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 882: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 51s 19s/step - loss: 0.6361 - accuracy: 0.6667 - val_loss: 0.8607 - val_accuracy: 0.5238\n",
      "Epoch 883/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.5952 \n",
      "Epoch 883: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 883: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 883: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 883: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7013 - accuracy: 0.5952 - val_loss: 0.8786 - val_accuracy: 0.5238\n",
      "Epoch 884/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.6548 \n",
      "Epoch 884: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 884: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 884: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 884: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6956 - accuracy: 0.6548 - val_loss: 0.9039 - val_accuracy: 0.5238\n",
      "Epoch 885/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.7024 \n",
      "Epoch 885: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 885: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 885: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 885: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5956 - accuracy: 0.7024 - val_loss: 0.9475 - val_accuracy: 0.5238\n",
      "Epoch 886/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.6786 \n",
      "Epoch 886: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 886: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 886: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 886: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6039 - accuracy: 0.6786 - val_loss: 0.9554 - val_accuracy: 0.4762\n",
      "Epoch 887/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6050 - accuracy: 0.6667 \n",
      "Epoch 887: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 887: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 887: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 887: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6050 - accuracy: 0.6667 - val_loss: 0.9555 - val_accuracy: 0.4762\n",
      "Epoch 888/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.6667 \n",
      "Epoch 888: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 888: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 888: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 888: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6093 - accuracy: 0.6667 - val_loss: 0.9538 - val_accuracy: 0.4762\n",
      "Epoch 889/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.6667 \n",
      "Epoch 889: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 889: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 889: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 889: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6084 - accuracy: 0.6667 - val_loss: 0.9299 - val_accuracy: 0.5238\n",
      "Epoch 890/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.7143 \n",
      "Epoch 890: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 890: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 890: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 890: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5660 - accuracy: 0.7143 - val_loss: 0.9159 - val_accuracy: 0.5238\n",
      "Epoch 891/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6032 - accuracy: 0.6667 \n",
      "Epoch 891: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 891: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 891: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 891: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6032 - accuracy: 0.6667 - val_loss: 0.9092 - val_accuracy: 0.5238\n",
      "Epoch 892/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.6310 \n",
      "Epoch 892: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 892: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 892: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 892: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6461 - accuracy: 0.6310 - val_loss: 0.9175 - val_accuracy: 0.5238\n",
      "Epoch 893/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.6429 \n",
      "Epoch 893: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 893: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 893: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 893: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6375 - accuracy: 0.6429 - val_loss: 0.9167 - val_accuracy: 0.5238\n",
      "Epoch 894/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.6905 \n",
      "Epoch 894: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 894: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 894: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 894: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6082 - accuracy: 0.6905 - val_loss: 0.9051 - val_accuracy: 0.5238\n",
      "Epoch 895/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.6429 \n",
      "Epoch 895: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 895: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 895: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 895: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6112 - accuracy: 0.6429 - val_loss: 0.8947 - val_accuracy: 0.5238\n",
      "Epoch 896/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5743 - accuracy: 0.6905 \n",
      "Epoch 896: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 896: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 896: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 896: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5743 - accuracy: 0.6905 - val_loss: 0.8952 - val_accuracy: 0.5238\n",
      "Epoch 897/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.6905 \n",
      "Epoch 897: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 897: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 897: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 897: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6093 - accuracy: 0.6905 - val_loss: 0.8942 - val_accuracy: 0.5238\n",
      "Epoch 898/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.6905 \n",
      "Epoch 898: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 898: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 898: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 898: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5887 - accuracy: 0.6905 - val_loss: 0.8941 - val_accuracy: 0.5238\n",
      "Epoch 899/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5996 - accuracy: 0.6667 \n",
      "Epoch 899: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 899: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 899: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 899: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5996 - accuracy: 0.6667 - val_loss: 0.9244 - val_accuracy: 0.5238\n",
      "Epoch 900/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.6310 \n",
      "Epoch 900: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 900: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 900: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 900: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6077 - accuracy: 0.6310 - val_loss: 0.9139 - val_accuracy: 0.5238\n",
      "Epoch 901/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.7381 \n",
      "Epoch 901: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 901: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 901: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 901: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5791 - accuracy: 0.7381 - val_loss: 0.9226 - val_accuracy: 0.5238\n",
      "Epoch 902/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5969 - accuracy: 0.6548 \n",
      "Epoch 902: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 902: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 902: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 902: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5969 - accuracy: 0.6548 - val_loss: 0.9241 - val_accuracy: 0.5238\n",
      "Epoch 903/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5819 - accuracy: 0.7024 \n",
      "Epoch 903: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 903: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 903: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 903: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5819 - accuracy: 0.7024 - val_loss: 0.9314 - val_accuracy: 0.4762\n",
      "Epoch 904/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.7143 \n",
      "Epoch 904: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 904: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 904: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 904: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5845 - accuracy: 0.7143 - val_loss: 0.9191 - val_accuracy: 0.4762\n",
      "Epoch 905/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.6548 \n",
      "Epoch 905: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 905: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 905: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 905: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5721 - accuracy: 0.6548 - val_loss: 0.9144 - val_accuracy: 0.4762\n",
      "Epoch 906/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.7262 \n",
      "Epoch 906: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 906: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 906: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 906: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5456 - accuracy: 0.7262 - val_loss: 0.9146 - val_accuracy: 0.4762\n",
      "Epoch 907/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.6905 \n",
      "Epoch 907: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 907: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 907: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 907: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5694 - accuracy: 0.6905 - val_loss: 0.9152 - val_accuracy: 0.4762\n",
      "Epoch 908/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.6786 \n",
      "Epoch 908: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 908: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 908: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 908: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5623 - accuracy: 0.6786 - val_loss: 0.9459 - val_accuracy: 0.4762\n",
      "Epoch 909/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7262 \n",
      "Epoch 909: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 909: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 909: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 909: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5674 - accuracy: 0.7262 - val_loss: 0.9123 - val_accuracy: 0.5238\n",
      "Epoch 910/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.6667 \n",
      "Epoch 910: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 910: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 910: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 910: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5980 - accuracy: 0.6667 - val_loss: 0.9334 - val_accuracy: 0.4762\n",
      "Epoch 911/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7143 \n",
      "Epoch 911: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 911: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 911: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 911: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5593 - accuracy: 0.7143 - val_loss: 0.9342 - val_accuracy: 0.4762\n",
      "Epoch 912/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7381 \n",
      "Epoch 912: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 912: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 912: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 912: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5545 - accuracy: 0.7381 - val_loss: 0.9369 - val_accuracy: 0.4762\n",
      "Epoch 913/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.7024 \n",
      "Epoch 913: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 913: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 913: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 913: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5641 - accuracy: 0.7024 - val_loss: 0.9491 - val_accuracy: 0.4762\n",
      "Epoch 914/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.6667 \n",
      "Epoch 914: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 914: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 914: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 914: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5730 - accuracy: 0.6667 - val_loss: 0.9832 - val_accuracy: 0.4762\n",
      "Epoch 915/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5857 - accuracy: 0.6667 \n",
      "Epoch 915: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 915: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 915: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 915: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5857 - accuracy: 0.6667 - val_loss: 0.9380 - val_accuracy: 0.4762\n",
      "Epoch 916/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.6905 \n",
      "Epoch 916: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 916: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 916: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 916: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5510 - accuracy: 0.6905 - val_loss: 0.9391 - val_accuracy: 0.4762\n",
      "Epoch 917/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.6905 \n",
      "Epoch 917: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 917: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 917: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 917: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6060 - accuracy: 0.6905 - val_loss: 0.9647 - val_accuracy: 0.4762\n",
      "Epoch 918/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.6548 \n",
      "Epoch 918: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 918: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 918: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 918: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6584 - accuracy: 0.6548 - val_loss: 0.9592 - val_accuracy: 0.5238\n",
      "Epoch 919/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.6548 \n",
      "Epoch 919: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 919: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 919: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 919: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6391 - accuracy: 0.6548 - val_loss: 0.9943 - val_accuracy: 0.4762\n",
      "Epoch 920/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7143 \n",
      "Epoch 920: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 920: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 920: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 920: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6025 - accuracy: 0.7143 - val_loss: 1.0149 - val_accuracy: 0.3810\n",
      "Epoch 921/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7135 - accuracy: 0.6429 \n",
      "Epoch 921: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 921: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 921: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 921: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.7135 - accuracy: 0.6429 - val_loss: 1.0533 - val_accuracy: 0.4286\n",
      "Epoch 922/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7334 - accuracy: 0.6190 \n",
      "Epoch 922: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 922: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 922: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 922: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.7334 - accuracy: 0.6190 - val_loss: 1.0334 - val_accuracy: 0.4762\n",
      "Epoch 923/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.7024 \n",
      "Epoch 923: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 923: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 923: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 923: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6476 - accuracy: 0.7024 - val_loss: 1.0606 - val_accuracy: 0.4762\n",
      "Epoch 924/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.6905 \n",
      "Epoch 924: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 924: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 924: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 924: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5667 - accuracy: 0.6905 - val_loss: 1.0326 - val_accuracy: 0.5238\n",
      "Epoch 925/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.6786 \n",
      "Epoch 925: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 925: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 925: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 925: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6192 - accuracy: 0.6786 - val_loss: 1.0263 - val_accuracy: 0.5238\n",
      "Epoch 926/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.6905 \n",
      "Epoch 926: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 926: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 926: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 926: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5900 - accuracy: 0.6905 - val_loss: 1.0403 - val_accuracy: 0.4762\n",
      "Epoch 927/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.7143 \n",
      "Epoch 927: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 927: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 927: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 927: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5855 - accuracy: 0.7143 - val_loss: 1.0390 - val_accuracy: 0.4286\n",
      "Epoch 928/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6489 - accuracy: 0.6667 \n",
      "Epoch 928: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 928: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 928: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 928: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6489 - accuracy: 0.6667 - val_loss: 1.0218 - val_accuracy: 0.4286\n",
      "Epoch 929/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6067 - accuracy: 0.7143 \n",
      "Epoch 929: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 929: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 929: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 929: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6067 - accuracy: 0.7143 - val_loss: 0.9696 - val_accuracy: 0.4762\n",
      "Epoch 930/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7143 \n",
      "Epoch 930: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 930: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 930: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 930: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 19s/step - loss: 0.5810 - accuracy: 0.7143 - val_loss: 0.9474 - val_accuracy: 0.5238\n",
      "Epoch 931/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.7738 \n",
      "Epoch 931: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 931: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 931: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 931: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5296 - accuracy: 0.7738 - val_loss: 0.9584 - val_accuracy: 0.4762\n",
      "Epoch 932/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.7024 \n",
      "Epoch 932: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 932: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 932: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 932: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5488 - accuracy: 0.7024 - val_loss: 0.9674 - val_accuracy: 0.4762\n",
      "Epoch 933/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7024 \n",
      "Epoch 933: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 933: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 933: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 933: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5664 - accuracy: 0.7024 - val_loss: 0.9636 - val_accuracy: 0.4762\n",
      "Epoch 934/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.7619 \n",
      "Epoch 934: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 934: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 934: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 934: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5283 - accuracy: 0.7619 - val_loss: 0.9715 - val_accuracy: 0.4762\n",
      "Epoch 935/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.7500 \n",
      "Epoch 935: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 935: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 935: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 935: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5364 - accuracy: 0.7500 - val_loss: 0.9867 - val_accuracy: 0.4762\n",
      "Epoch 936/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.7738 \n",
      "Epoch 936: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 936: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 936: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 936: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5132 - accuracy: 0.7738 - val_loss: 1.0173 - val_accuracy: 0.4762\n",
      "Epoch 937/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.7500 \n",
      "Epoch 937: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 937: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 937: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 937: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5158 - accuracy: 0.7500 - val_loss: 1.0399 - val_accuracy: 0.4762\n",
      "Epoch 938/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7500 \n",
      "Epoch 938: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 938: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 938: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 938: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5251 - accuracy: 0.7500 - val_loss: 1.0647 - val_accuracy: 0.4762\n",
      "Epoch 939/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.7500 \n",
      "Epoch 939: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 939: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 939: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 939: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5312 - accuracy: 0.7500 - val_loss: 1.0529 - val_accuracy: 0.4762\n",
      "Epoch 940/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.7738 \n",
      "Epoch 940: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 940: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 940: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 940: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5170 - accuracy: 0.7738 - val_loss: 1.0571 - val_accuracy: 0.4762\n",
      "Epoch 941/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.7381 \n",
      "Epoch 941: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 941: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 941: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 941: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5493 - accuracy: 0.7381 - val_loss: 1.0636 - val_accuracy: 0.4762\n",
      "Epoch 942/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7857 \n",
      "Epoch 942: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 942: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 942: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 942: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4902 - accuracy: 0.7857 - val_loss: 1.0840 - val_accuracy: 0.4762\n",
      "Epoch 943/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6389 - accuracy: 0.7143 \n",
      "Epoch 943: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 943: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 943: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 943: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.6389 - accuracy: 0.7143 - val_loss: 1.0681 - val_accuracy: 0.4762\n",
      "Epoch 944/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5764 - accuracy: 0.7143 \n",
      "Epoch 944: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 944: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 944: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 944: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5764 - accuracy: 0.7143 - val_loss: 1.0448 - val_accuracy: 0.5238\n",
      "Epoch 945/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.7738 \n",
      "Epoch 945: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 945: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 945: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 945: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5012 - accuracy: 0.7738 - val_loss: 1.0986 - val_accuracy: 0.4762\n",
      "Epoch 946/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.7500 \n",
      "Epoch 946: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 946: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 946: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 946: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5874 - accuracy: 0.7500 - val_loss: 1.1324 - val_accuracy: 0.4762\n",
      "Epoch 947/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.7500 \n",
      "Epoch 947: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 947: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 947: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 947: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5629 - accuracy: 0.7500 - val_loss: 1.1297 - val_accuracy: 0.4286\n",
      "Epoch 948/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.7857 \n",
      "Epoch 948: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 948: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 948: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 948: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5175 - accuracy: 0.7857 - val_loss: 1.0987 - val_accuracy: 0.4286\n",
      "Epoch 949/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.7262 \n",
      "Epoch 949: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 949: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 949: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 949: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5306 - accuracy: 0.7262 - val_loss: 1.0648 - val_accuracy: 0.4762\n",
      "Epoch 950/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.7500 \n",
      "Epoch 950: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 950: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 950: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 950: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5120 - accuracy: 0.7500 - val_loss: 1.0349 - val_accuracy: 0.4286\n",
      "Epoch 951/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.7619 \n",
      "Epoch 951: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 951: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 951: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 951: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5005 - accuracy: 0.7619 - val_loss: 0.9837 - val_accuracy: 0.4762\n",
      "Epoch 952/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.7857 \n",
      "Epoch 952: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 952: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 952: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 952: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4912 - accuracy: 0.7857 - val_loss: 0.9595 - val_accuracy: 0.4762\n",
      "Epoch 953/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7500 \n",
      "Epoch 953: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 953: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 953: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 953: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4954 - accuracy: 0.7500 - val_loss: 0.9742 - val_accuracy: 0.4762\n",
      "Epoch 954/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7381 \n",
      "Epoch 954: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 954: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 954: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 954: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4954 - accuracy: 0.7381 - val_loss: 1.0010 - val_accuracy: 0.4762\n",
      "Epoch 955/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.7738 \n",
      "Epoch 955: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 955: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 955: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 955: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.4844 - accuracy: 0.7738 - val_loss: 1.0378 - val_accuracy: 0.4762\n",
      "Epoch 956/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.7262 \n",
      "Epoch 956: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 956: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 956: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 956: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5236 - accuracy: 0.7262 - val_loss: 1.1625 - val_accuracy: 0.4762\n",
      "Epoch 957/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5204 - accuracy: 0.7738 \n",
      "Epoch 957: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 957: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 957: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 957: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5204 - accuracy: 0.7738 - val_loss: 1.1534 - val_accuracy: 0.4286\n",
      "Epoch 958/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8095 \n",
      "Epoch 958: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 958: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 958: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 958: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.4752 - accuracy: 0.8095 - val_loss: 1.1482 - val_accuracy: 0.4762\n",
      "Epoch 959/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.7024 \n",
      "Epoch 959: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 959: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 959: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 959: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6257 - accuracy: 0.7024 - val_loss: 1.1637 - val_accuracy: 0.4286\n",
      "Epoch 960/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8518 - accuracy: 0.6548 \n",
      "Epoch 960: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 960: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 960: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 960: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8518 - accuracy: 0.6548 - val_loss: 1.1387 - val_accuracy: 0.4286\n",
      "Epoch 961/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.7024 \n",
      "Epoch 961: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 961: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 961: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 961: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.8142 - accuracy: 0.7024 - val_loss: 1.0862 - val_accuracy: 0.4762\n",
      "Epoch 962/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5978 - accuracy: 0.7024 \n",
      "Epoch 962: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 962: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 962: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 962: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5978 - accuracy: 0.7024 - val_loss: 1.1822 - val_accuracy: 0.4286\n",
      "Epoch 963/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.7857 \n",
      "Epoch 963: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 963: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 963: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 963: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5304 - accuracy: 0.7857 - val_loss: 1.1966 - val_accuracy: 0.3810\n",
      "Epoch 964/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5867 - accuracy: 0.7262 \n",
      "Epoch 964: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 964: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 964: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 964: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5867 - accuracy: 0.7262 - val_loss: 1.1850 - val_accuracy: 0.3810\n",
      "Epoch 965/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5014 - accuracy: 0.7262 \n",
      "Epoch 965: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 965: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 965: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 965: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5014 - accuracy: 0.7262 - val_loss: 1.0591 - val_accuracy: 0.4286\n",
      "Epoch 966/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.7500 \n",
      "Epoch 966: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 966: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 966: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 966: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5047 - accuracy: 0.7500 - val_loss: 1.0089 - val_accuracy: 0.4762\n",
      "Epoch 967/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.7857 \n",
      "Epoch 967: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 967: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 967: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 967: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4900 - accuracy: 0.7857 - val_loss: 1.0074 - val_accuracy: 0.4762\n",
      "Epoch 968/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.7381 \n",
      "Epoch 968: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 968: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 968: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 968: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5487 - accuracy: 0.7381 - val_loss: 1.0317 - val_accuracy: 0.4762\n",
      "Epoch 969/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.7381 \n",
      "Epoch 969: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 969: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 969: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 969: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5234 - accuracy: 0.7381 - val_loss: 1.0419 - val_accuracy: 0.4762\n",
      "Epoch 970/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.7381 \n",
      "Epoch 970: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 970: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 970: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 970: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5189 - accuracy: 0.7381 - val_loss: 1.0328 - val_accuracy: 0.4762\n",
      "Epoch 971/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.7381 \n",
      "Epoch 971: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 971: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 971: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 971: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 17s/step - loss: 0.5558 - accuracy: 0.7381 - val_loss: 1.0170 - val_accuracy: 0.4762\n",
      "Epoch 972/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.7857 \n",
      "Epoch 972: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 972: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 972: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 972: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4847 - accuracy: 0.7857 - val_loss: 1.0459 - val_accuracy: 0.4762\n",
      "Epoch 973/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.7857 \n",
      "Epoch 973: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 973: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 973: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 973: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5346 - accuracy: 0.7857 - val_loss: 1.0252 - val_accuracy: 0.4762\n",
      "Epoch 974/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.7857 \n",
      "Epoch 974: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 974: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 974: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 974: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4937 - accuracy: 0.7857 - val_loss: 1.1413 - val_accuracy: 0.4286\n",
      "Epoch 975/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6558 - accuracy: 0.7738 \n",
      "Epoch 975: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 975: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 975: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 975: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6558 - accuracy: 0.7738 - val_loss: 1.1773 - val_accuracy: 0.3810\n",
      "Epoch 976/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.7857 \n",
      "Epoch 976: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 976: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 976: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 976: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6250 - accuracy: 0.7857 - val_loss: 1.0899 - val_accuracy: 0.4286\n",
      "Epoch 977/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7381 \n",
      "Epoch 977: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 977: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 977: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 977: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5632 - accuracy: 0.7381 - val_loss: 1.0971 - val_accuracy: 0.4286\n",
      "Epoch 978/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.7619 \n",
      "Epoch 978: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 978: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 978: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 978: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5310 - accuracy: 0.7619 - val_loss: 1.0140 - val_accuracy: 0.5238\n",
      "Epoch 979/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.7619 \n",
      "Epoch 979: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 979: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 979: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 979: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5411 - accuracy: 0.7619 - val_loss: 1.0029 - val_accuracy: 0.5238\n",
      "Epoch 980/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.7857 \n",
      "Epoch 980: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 980: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 980: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 980: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5223 - accuracy: 0.7857 - val_loss: 0.9928 - val_accuracy: 0.5238\n",
      "Epoch 981/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.7857 \n",
      "Epoch 981: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 981: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 981: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 981: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5078 - accuracy: 0.7857 - val_loss: 0.9935 - val_accuracy: 0.5238\n",
      "Epoch 982/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.7619 \n",
      "Epoch 982: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 982: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 982: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 982: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.4913 - accuracy: 0.7619 - val_loss: 0.9939 - val_accuracy: 0.5238\n",
      "Epoch 983/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.7619 \n",
      "Epoch 983: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 983: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 983: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 983: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5052 - accuracy: 0.7619 - val_loss: 1.0294 - val_accuracy: 0.4762\n",
      "Epoch 984/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.7738 \n",
      "Epoch 984: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 984: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 984: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 984: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5688 - accuracy: 0.7738 - val_loss: 1.0168 - val_accuracy: 0.4762\n",
      "Epoch 985/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.7738 \n",
      "Epoch 985: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 985: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 985: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 985: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5160 - accuracy: 0.7738 - val_loss: 1.0254 - val_accuracy: 0.4762\n",
      "Epoch 986/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5349 - accuracy: 0.7262 \n",
      "Epoch 986: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 986: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 986: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 986: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.5349 - accuracy: 0.7262 - val_loss: 1.0446 - val_accuracy: 0.4762\n",
      "Epoch 987/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.7381 \n",
      "Epoch 987: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 987: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 987: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 987: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.5054 - accuracy: 0.7381 - val_loss: 1.0724 - val_accuracy: 0.4286\n",
      "Epoch 988/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.7262 \n",
      "Epoch 988: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 988: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 988: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 988: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5419 - accuracy: 0.7262 - val_loss: 1.1343 - val_accuracy: 0.4286\n",
      "Epoch 989/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7500 \n",
      "Epoch 989: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 989: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 989: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 989: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 0.5850 - accuracy: 0.7500 - val_loss: 1.1703 - val_accuracy: 0.4286\n",
      "Epoch 990/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.6429 \n",
      "Epoch 990: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 990: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 990: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 990: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.7735 - accuracy: 0.6429 - val_loss: 1.2725 - val_accuracy: 0.3810\n",
      "Epoch 991/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0698 - accuracy: 0.5119 \n",
      "Epoch 991: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 991: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 991: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 991: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 48s 18s/step - loss: 1.0698 - accuracy: 0.5119 - val_loss: 1.5145 - val_accuracy: 0.2857\n",
      "Epoch 992/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9814 - accuracy: 0.5595 \n",
      "Epoch 992: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 992: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 992: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 992: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.9814 - accuracy: 0.5595 - val_loss: 1.4108 - val_accuracy: 0.2857\n",
      "Epoch 993/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8419 - accuracy: 0.5833 \n",
      "Epoch 993: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 993: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 993: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 993: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.8419 - accuracy: 0.5833 - val_loss: 1.2589 - val_accuracy: 0.3333\n",
      "Epoch 994/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.6429 \n",
      "Epoch 994: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 994: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 994: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 994: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.7277 - accuracy: 0.6429 - val_loss: 1.0300 - val_accuracy: 0.4762\n",
      "Epoch 995/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.6905 \n",
      "Epoch 995: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 995: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 995: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 995: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6657 - accuracy: 0.6905 - val_loss: 0.9858 - val_accuracy: 0.5238\n",
      "Epoch 996/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6993 - accuracy: 0.6310 \n",
      "Epoch 996: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 996: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 996: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 996: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6993 - accuracy: 0.6310 - val_loss: 0.9708 - val_accuracy: 0.5238\n",
      "Epoch 997/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6691 - accuracy: 0.6548 \n",
      "Epoch 997: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 997: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 997: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 997: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6691 - accuracy: 0.6548 - val_loss: 0.9673 - val_accuracy: 0.5238\n",
      "Epoch 998/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.6786 \n",
      "Epoch 998: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 998: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 998: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 998: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6645 - accuracy: 0.6786 - val_loss: 0.9623 - val_accuracy: 0.5238\n",
      "Epoch 999/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6633 - accuracy: 0.6905 \n",
      "Epoch 999: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 999: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 999: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 999: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 49s 18s/step - loss: 0.6633 - accuracy: 0.6905 - val_loss: 1.0166 - val_accuracy: 0.4286\n",
      "Epoch 1000/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.6548 \n",
      "Epoch 1000: loss did not improve from 0.39835\n",
      "\n",
      "Epoch 1000: accuracy did not improve from 0.80952\n",
      "\n",
      "Epoch 1000: val_loss did not improve from 0.82165\n",
      "\n",
      "Epoch 1000: val_accuracy did not improve from 0.52381\n",
      "3/3 [==============================] - 50s 18s/step - loss: 0.6745 - accuracy: 0.6548 - val_loss: 0.9820 - val_accuracy: 0.4762\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 모델 로드\n",
    "loaded_model = load_model('walk_classification_model.h5')\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터를 numpy 배열로 변환\n",
    "X_train = [x.to_numpy() for x in X_train]\n",
    "X_val = [x.to_numpy() for x in X_val]\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 패딩 처리\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', dtype='float32')\n",
    "X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding='post', dtype='float32')\n",
    "\n",
    "# 손실을 기준으로 최적의 모델 저장\n",
    "checkpoint_loss = ModelCheckpoint('best_model_loss.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# 정확도를 기준으로 최적의 모델 저장\n",
    "checkpoint_acc = ModelCheckpoint('best_model_acc.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 체크포인트 설정 (예: 검증 손실을 기준으로)\n",
    "checkpoint_val_loss = ModelCheckpoint('best_model_val_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# 검증 정확도를 기준으로 최적의 모델 저장\n",
    "checkpoint_val_acc = ModelCheckpoint('best_model_val_acc.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 추가 학습\n",
    "history = loaded_model.fit(X_train_padded, y_train, validation_data=(X_val_padded, y_val), epochs=1000, batch_size=32, callbacks=[checkpoint_loss, checkpoint_acc, checkpoint_val_loss, checkpoint_val_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4355d2-b51e-4811-be19-4e661aaa11c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d00beef4-d554-4844-80c8-de5fb44804bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGGCAYAAACqvTJ0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd3gUVdvG7+2bHgIhCRASeu+9dxEQBUSKIl19VVQELLw2wIIFEXv5pFhAmsiLiiBVmnSC9I60UAKk192d74/N7J6ZndmW7Xl+1xXYnZ1ydnZ2ds4993MfBcdxHAiCIAiCIAiCIAiCIAjChyj93QCCIAiCIAiCIAiCIAii/EGiFEEQBEEQBEEQBEEQBOFzSJQiCIIgCIIgCIIgCIIgfA6JUgRBEARBEARBEARBEITPIVGKIAiCIAiCIAiCIAiC8DkkShEEQRAEQRAEQRAEQRA+h0QpgiAIgiAIgiAIgiAIwueQKEUQBEEQBEEQBEEQBEH4HBKlCIIgCIIgCIIgCIIgCJ9DohRBEEQZuXjxIhQKBRYtWuTR9c6YMQMKhcKj6yQIgiAIgghVUlNTMXbsWI+uc+vWrVAoFNi6datH10sQhBkSpQiCcMiiRYugUCiwf/9+fzfFLryII/d3/fp1fzfRhvz8fMyYMYMudAiCIAgiwPjiiy+gUCjQrl07fzclqOBFHLm/pUuX+ruJknzxxRcev8FIEIRj1P5uAEEQhKf58ssvERkZaTM9NjbW941xQH5+PmbOnAkA6N69u+C1V199FS+//LIfWkUQBEEQxOLFi5Gamoq9e/fi7NmzqF27tr+bFFQ8++yzaNOmjc30Dh06+KE1jvniiy9QqVIlG6dV165dUVBQAK1W65+GEUSIQ6IUQRAhx9ChQ1GpUiV/N6PMqNVqqNV0miYIgiAIX3PhwgXs2rULq1atwhNPPIHFixfjjTfe8HezJMnLy0NERIS/m2FDly5dMHToUH83o8wolUro9Xp/N4MgQhYq3yMIwmMcOnQI/fr1Q3R0NCIjI9GrVy/s3r1bME9JSQlmzpyJOnXqQK/Xo2LFiujcuTM2bNhgmef69esYN24cqlWrBp1Oh6SkJDzwwAO4ePFimdt448YNqNVqizuJ5dSpU1AoFPjss88s086fP4+HHnoIcXFxCA8PR/v27fH777873E737t1tnE8AMHbsWKSmpgIwZ1HFx8cDAGbOnGmxtc+YMQOAdKaUwWDAm2++iVq1akGn0yE1NRX//e9/UVRUJJgvNTUV9913H3bs2IG2bdtCr9ejZs2a+P777x22nSAIgiDKO4sXL0aFChUwYMAADB06FIsXL5acLzMzE88//zxSU1Oh0+lQrVo1jB49GhkZGZZ5CgsLMWPGDNStWxd6vR5JSUkYMmQIzp07B0A+s0gqs3Ls2LGIjIzEuXPn0L9/f0RFReGRRx4BAGzfvh0PPfQQqlevDp1Oh+TkZDz//PMoKCiwaffJkycxbNgwxMfHIywsDPXq1cMrr7wCANiyZQsUCgV++eUXm+WWLFkChUKBv//+26X9KUXjxo3Ro0cPm+kmkwlVq1YVCFp5eXmYOnUqkpOTodPpUK9ePcyZMwccx9ndhlw+Jx9NwV9bpqam4tixY/jrr78s12P8dZzc57NixQq0atUKYWFhqFSpEkaNGoWrV68K5uE/r6tXr2LQoEGIjIxEfHw8pk2bBqPR6MReIojQh27BEwThEY4dO4YuXbogOjoaL774IjQaDb7++mt0794df/31lyWPYcaMGZg9ezYmTpyItm3bIjs7G/v378fBgwfRp08fAMCDDz6IY8eO4ZlnnkFqaipu3ryJDRs24NKlSxZBxx537tyxmaZWqxEbG4uEhAR069YNy5cvt7njuWzZMqhUKjz00EMAzAJWx44dkZ+fj2effRYVK1bEd999h/vvvx8rV67E4MGDy7TP4uPj8eWXX+LJJ5/E4MGDMWTIEABA06ZNZZeZOHEivvvuOwwdOhRTp07Fnj17MHv2bJw4ccLm4vHs2bMYOnQoJkyYgDFjxmDBggUYO3YsWrVqhUaNGpWp7QRBEAQRyixevBhDhgyBVqvFyJEj8eWXX2Lfvn2CcrTc3Fx06dIFJ06cwPjx49GyZUtkZGRgzZo1uHLlCipVqgSj0Yj77rsPmzZtwogRI/Dcc88hJycHGzZswNGjR1GrVi2X22YwGNC3b1907twZc+bMQXh4OACzSJKfn48nn3wSFStWxN69e/Hpp5/iypUrWLFihWX5f/75B126dIFGo8Hjjz+O1NRUnDt3Dr/++ivefvttdO/eHcnJyVi8eLHNtc7ixYtRq1Ytp0rwcnJyBOIcT8WKFaFQKDB8+HDMmDED169fR2JiouX1HTt24Nq1axgxYgQAgOM43H///diyZQsmTJiA5s2bY/369XjhhRdw9epVfPTRRy7vQzHz5s3DM888g8jISIs4l5CQIDv/okWLMG7cOLRp0wazZ8/GjRs38PHHH2Pnzp04dOiQIDLCaDSib9++aNeuHebMmYONGzfiww8/RK1atfDkk0+Wue0EEfRwBEEQDli4cCEHgNu3b5/sPIMGDeK0Wi137tw5y7Rr165xUVFRXNeuXS3TmjVrxg0YMEB2PXfv3uUAcB988IHL7XzjjTc4AJJ/9erVs8z39ddfcwC4I0eOCJZv2LAh17NnT8vzyZMncwC47du3W6bl5ORwNWrU4FJTUzmj0chxHMdduHCBA8AtXLjQMl+3bt24bt262bRxzJgxXEpKiuX5rVu3OADcG2+8Ift+eNLS0jgA3MSJEwXzTZs2jQPAbd682TItJSWFA8Bt27bNMu3mzZucTqfjpk6darMtgiAIgiDM7N+/nwPAbdiwgeM4jjOZTFy1atW45557TjDf66+/zgHgVq1aZbMOk8nEcRzHLViwgAPAzZ07V3aeLVu2cAC4LVu2CF6Xur4YM2YMB4B7+eWXbdaXn59vM2327NmcQqHg/v33X8u0rl27clFRUYJpbHs4juOmT5/O6XQ6LjMz0zLt5s2bnFqtlrxmYeHfj9xfeno6x3Ecd+rUKQ4A9+mnnwqWf+qpp7jIyEjL+1m9ejUHgHvrrbcE8w0dOpRTKBTc2bNnLdNSUlK4MWPGWJ6Lr6V4+GvbCxcuWKY1atRI8tpN/PkUFxdzlStX5ho3bswVFBRY5vvtt984ANzrr79umcZ/XrNmzRKss0WLFlyrVq0k9h5BlD+ofI8giDJjNBrx559/YtCgQahZs6ZlelJSEh5++GHs2LED2dnZAMxh48eOHcOZM2ck1xUWFgatVoutW7fi7t27brXn559/xoYNGwR/CxcutLw+ZMgQqNVqLFu2zDLt6NGjOH78OIYPH26ZtnbtWrRt2xadO3e2TIuMjMTjjz+Oixcv4vjx4261z13Wrl0LAJgyZYpg+tSpUwHApqywYcOG6NKli+V5fHw86tWrh/Pnz3u5pQRBEAQRvCxevBgJCQmW0jLe1bN06VJBydXPP/+MZs2aSTqn+ZKxn3/+GZUqVcIzzzwjO487SDlswsLCLI/z8vKQkZGBjh07guM4HDp0CABw69YtbNu2DePHj0f16tVl2zN69GgUFRVh5cqVlmnLli2DwWDAqFGjnGrj66+/bnM9tmHDBsTFxQEA6tati+bNmwuux4xGI1auXImBAwda3s/atWuhUqnw7LPPCtY/depUcByHP/74w6n2eIr9+/fj5s2beOqppwRZUwMGDED9+vUlYx7+85//CJ536dKFrscIohQSpQiCKDO3bt1Cfn4+6tWrZ/NagwYNYDKZcPnyZQDArFmzkJmZibp166JJkyZ44YUX8M8//1jm1+l0eO+99/DHH38gISEBXbt2xfvvv4/r16873Z6uXbuid+/egj/WZl6pUiX06tULy5cvt0xbtmwZ1Gq1pYQOAP7991/Z98S/7kv+/fdfKJVKm9F/EhMTERsba9Me8cUmAFSoUMFtsY8gCIIgQh2j0YilS5eiR48euHDhAs6ePYuzZ8+iXbt2uHHjBjZt2mSZ99y5c2jcuLHd9Z07dw716tXz6MAlarUa1apVs5l+6dIljB07FnFxcZbsom7dugEAsrKyAMAihDhqd/369dGmTRtBltbixYvRvn17p0chbNKkic31WO/evQWj2A0fPhw7d+60ZDFt3boVN2/eFNwk/Pfff1GlShVERUUJ1u/P6zEAkteI9evXt2mPXq+3ZIjy0PUYQVghUYogCJ/StWtXnDt3DgsWLEDjxo3x7bffomXLlvj2228t80yePBmnT5/G7Nmzodfr8dprr6FBgwaWu3yeYMSIETh9+jTS0tIAAMuXL0evXr08Nmqf3N1PT4RaOntnVaVSSU7nHISCEgRBEER5ZfPmzUhPT8fSpUtRp04dy9+wYcMAQDbwvCy4es2g0+mgVCpt5u3Tpw9+//13vPTSS1i9ejU2bNhgCUk3mUwut2v06NH466+/cOXKFZw7dw67d+922iXlLMOHDwfHcZbMq+XLlyMmJgb33nuvR9bvzesxZ5G7HiMIwgyJUgRBlJn4+HiEh4fj1KlTNq+dPHkSSqUSycnJlmlxcXEYN24cfvrpJ1y+fBlNmza1jDjHU6tWLUydOhV//vknjh49iuLiYnz44Ycea/OgQYOg1WqxbNkypKWl4fTp05ZATZ6UlBTZ98S/LkeFChWQmZlpM11898wV635KSgpMJpNN6eONGzeQmZlptz0EQRAEQThm8eLFqFy5MlasWGHzN3LkSPzyyy+W0exq1aqFo0eP2l1frVq1cOrUKZSUlMjOU6FCBQCwuW5wxQF05MgRnD59Gh9++CFeeuklPPDAA+jduzeqVKkimI+PWXDUbsB8A0+lUuGnn37C4sWLodFoBA4mT1CjRg20bdvWUhq4atUqDBo0CDqdzjJPSkoKrl27hpycHMGyzl6PAc7tW2evyfjtSV0jnjp1iq7HCMJFSJQiCKLMqFQq3HPPPfjf//5nGVoXMIslS5YsQefOnREdHQ0AuH37tmDZyMhI1K5dG0VFRQCA/Px8FBYWCuapVasWoqKiLPN4gtjYWPTt2xfLly/H0qVLodVqMWjQIME8/fv3x969ewXDHufl5eGbb75BamoqGjZsKLv+WrVq4eTJk7h165Zl2uHDh7Fz507BfPyIOVIClpj+/fsDMI8QwzJ37lwA5iwDgiAIgiDco6CgAKtWrcJ9992HoUOH2vxNmjQJOTk5WLNmDQDzaMGHDx+2Gf0WsLqSH3zwQWRkZOCzzz6TnSclJQUqlQrbtm0TvP7FF1843XbejcO6oTmOw8cffyyYLz4+Hl27dsWCBQtw6dIlyfbwVKpUCf369cOPP/6IxYsX49577/WYo5xl+PDh2L17NxYsWICMjAwb4at///4wGo02+/Cjjz6CQqFAv379ZNfNj27I7tu8vDx89913NvNGREQ4dT3WunVrVK5cGV999ZXg2vSPP/7AiRMn6HqMIFzEc8XNBEGEPAsWLMC6detspj/33HN46623sGHDBnTu3BlPPfUU1Go1vv76axQVFeH999+3zNuwYUN0794drVq1QlxcHPbv34+VK1di0qRJAIDTp0+jV69eGDZsGBo2bAi1Wo1ffvkFN27csHEyybFy5UpERkbaTO/Tp49geN/hw4dj1KhR+OKLL9C3b1/B8L0A8PLLL+Onn35Cv3798OyzzyIuLg7fffcdLly4gJ9//tnGOs8yfvx4zJ07F3379sWECRNw8+ZNfPXVV2jUqJEl9B0wh5I2bNgQy5YtQ926dREXF4fGjRtLZj00a9YMY8aMwTfffIPMzEx069YNe/fuxXfffYdBgwZZAlkJgiAIgnCdNWvWICcnB/fff7/k6+3bt0d8fDwWL16M4cOH44UXXsDKlSvx0EMPYfz48WjVqhXu3LmDNWvW4KuvvkKzZs0wevRofP/995gyZQr27t2LLl26IC8vDxs3bsRTTz2FBx54ADExMXjooYfw6aefQqFQoFatWvjtt99w8+ZNp9tev3591KpVC9OmTcPVq1cRHR2Nn3/+WTK36JNPPkHnzp3RsmVLPP7446hRowYuXryI33//3RJrwDN69GgMHToUAPDmm286vzMBbN++3eZGIwA0bdoUTZs2tTwfNmwYpk2bhmnTpiEuLg69e/cWzD9w4ED06NEDr7zyCi5evIhmzZrhzz//xP/+9z9MnjzZIjxJcc8996B69eqYMGECXnjhBahUKixYsADx8fE2olyrVq3w5Zdf4q233kLt2rVRuXJl9OzZ02adGo0G7733HsaNG4du3bph5MiRuHHjBj7++GOkpqbi+eefd2k/EUS5x2/j/hEEETTww+bK/V2+fJnjOI47ePAg17dvXy4yMpILDw/nevTowe3atUuwrrfeeotr27YtFxsby4WFhXH169fn3n77ba64uJjjOI7LyMjgnn76aa5+/fpcREQEFxMTw7Vr145bvny5w3byw/7K/YmHWs7OzubCwsI4ANyPP/4ouc5z585xQ4cO5WJjYzm9Xs+1bduW++233wTzSA3ZzHEc9+OPP3I1a9bktFot17x5c279+vXcmDFjuJSUFMF8u3bt4lq1asVptVoOgGWoZalhjEtKSriZM2dyNWrU4DQaDZecnMxNnz6dKywsFMyXkpLCDRgwwOb9dOvWTXK4Y4IgCIIo7wwcOJDT6/VcXl6e7Dxjx47lNBoNl5GRwXEcx92+fZubNGkSV7VqVU6r1XLVqlXjxowZY3md4zguPz+fe+WVVyy/3YmJidzQoUO5c+fOWea5desW9+CDD3Lh4eFchQoVuCeeeII7evSozfXFmDFjuIiICMm2HT9+nOvduzcXGRnJVapUiXvssce4w4cPS16jHD16lBs8eLDl+qZevXrca6+9ZrPOoqIirkKFClxMTAxXUFDgzG7ktmzZYvd6jL/OYenUqRMHgJs4caLkOnNycrjnn3+eq1KlCqfRaLg6depwH3zwAWcymQTzpaSkcGPGjBFMO3DgANeuXTtOq9Vy1atX5+bOnWu5tr1w4YJlvuvXr3MDBgzgoqKiOACW6yX+/YivI5ctW8a1aNGC0+l0XFxcHPfII49wV65cEcwj93lJXeMRRHlFwXGUeEsQBEEQBEEQBEEIMRgMqFKlCgYOHIj58+f7uzkEQYQglClFEARBEARBEARB2LB69WrcunULo0eP9ndTCIIIUcgpRRAEQRAEQRAEQVjYs2cP/vnnH7z55puoVKkSDh486O8mEQQRopBTiiAIgiAIgiAIgrDw5Zdf4sknn0TlypXx/fff+7s5BEGEMOSUIgiCIAiCIAiCIAiCIHwOOaUIgiAIgiAIgiAIgiAIn0OiFEEQBEEQBEEQBEEQBOFz1P5ugK8xmUy4du0aoqKioFAo/N0cgiAIgiACHI7jkJOTgypVqkCpLL/38+gaiiAIgiAIZ3H2+qnciVLXrl1DcnKyv5tBEARBEESQcfnyZVSrVs3fzfAbdA1FEARBEISrOLp+KneiVFRUFADzjomOjvZzawiCIAiCCHSys7ORnJxsuYYor9A1FEEQBEEQzuLs9VO5E6V4u3l0dDRdUBEEQRAE4TTlvWSNrqEIgiAIgnAVR9dP5TcYgSAIgiAIgiAIgiAIgvAbJEoRBEEQBEEQBEEQBEEQPodEKYIgCIIgCIIgCIIgCMLnlLtMKYIgCIIgCIIgQhuTyYTi4mJ/N4MgvIJWq4VSSf4SIjQgUYogCIIgCIIgiJChuLgYFy5cgMlk8ndTCMIrKJVK1KhRA1qt1t9NIYgyQ6IUQRAEQRAEQRAhAcdxSE9Ph0qlQnJyMrlJiJDDZDLh2rVrSE9PR/Xq1cv9yLBE8EOiFEEQBEEQBEEQIYHBYEB+fj6qVKmC8PBwfzeHILxCfHw8rl27BoPBAI1G4+/mEESZoFsHBEEQBEEQBEGEBEajEQCorIkIafjjmz/eCSKYIVGKIAiCIAiCIIiQgkqaiFCGjm8ilCBRiiAIgiAIgiAIgiAIgvA5JEoRvmfTm8CyUQCNiEIQBEEQhJ848O8dDPliJ/65kunvptiwbN8lDPv6b2TmF0u+bjRxeOz7/Zj75ym/toMIbFJTUzFv3jyn59+6dSsUCgUyMzO91iaCINyD4zhMXX4Yr//vqL+b4nFIlCJ8z/Y5wIlfgX93+LslBEEQBBG0fP7550hNTYVer0e7du2wd+9eu/PPmzcP9erVQ1hYGJKTk/H888+jsLDQR60NPB788m8cvJSJYV//7e+m2PDSz0ew98IdzNt4RvL1HWczsOH4DXyy+axP2vHRhtNe3U55R6FQ2P2bMWOGW+vdt28fHn/8cafn79ixI9LT0xETE+PW9tyhfv360Ol0uH79us+2SRDByJW7Bfj54BV8//e/KDGGlrmDRCnCfxiK/N0CgiAIgghKli1bhilTpuCNN97AwYMH0axZM/Tt2xc3b96UnH/JkiV4+eWX8cYbb+DEiROYP38+li1bhv/+978+bnngUVgSuBf3t3Klr5UKiq3hxhzHeb0dVzPLr3jpC9LT0y1/8+bNQ3R0tGDatGnTLPNyHAeDweDUeuPj410agVCr1SIxMdFneUU7duxAQUEBhg4diu+++84n27RHSUmJv5tAELIUlljP+yYfnPd9CYlShP8IsS8TQRAEQfiKuXPn4rHHHsO4cePQsGFDfPXVVwgPD8eCBQsk59+1axc6deqEhx9+GKmpqbjnnnswcuRIh+4qwr8UOSGYlRi9fz2VXUCddW+SmJho+YuJiYFCobA8P3nyJKKiovDHH3+gVatW0Ol02LFjB86dO4cHHngACQkJiIyMRJs2bbBx40bBesXlewqFAt9++y0GDx6M8PBw1KlTB2vWrLG8Li7fW7RoEWJjY7F+/Xo0aNAAkZGRuPfee5Genm5ZxmAw4Nlnn0VsbCwqVqyIl156CWPGjMGgQYMcvu/58+fj4YcfxqOPPip57rpy5QpGjhyJuLg4REREoHXr1tizZ4/l9V9//RVt2rSBXq9HpUqVMHjwYMF7Xb16tWB9sbGxWLRoEQDg4sWLUCgUWLZsGbp16wa9Xo/Fixfj9u3bGDlyJKpWrYrw8HA0adIEP/30k2A9JpMJ77//PmrXrg2dTofq1avj7bffBgD07NkTkyZNEsx/69YtaLVabNq0yeE+IQg52HN9qHWjSZQi/AcXuHcmCYIgCCJQKS4uxoEDB9C7d2/LNKVSid69e+Pvv6VL0Tp27IgDBw5YRKjz589j7dq16N+/v0/aTLhHkcHxcO+sa8pbZBcGryjFcRzyiw1++fOki+3ll1/Gu+++ixMnTqBp06bIzc1F//79sWnTJhw6dAj33nsvBg4ciEuXLtldz8yZMzFs2DD8888/6N+/Px555BHcuXNHdv78/HzMmTMHP/zwA7Zt24ZLly4JnFvvvfceFi9ejIULF2Lnzp3Izs62EYOkyMnJwYoVKzBq1Cj06dMHWVlZ2L59u+X13NxcdOvWDVevXsWaNWtw+PBhvPjiizCVZtL+/vvvGDx4MPr3749Dhw5h06ZNaNu2rcPtinn55Zfx3HPP4cSJE+jbty8KCwvRqlUr/P777zh69Cgef/xxPProowIBf/r06Xj33Xfx2muv4fjx41iyZAkSEhIAABMnTsSSJUtQVGR1Of7444+oWrUqevbs6XL7CILHwOQxG02hpUqp/d0AohxDohRBEARBuExGRgaMRqOlE8STkJCAkydPSi7z8MMPIyMjA507d7aU//znP/+xW75XVFQk6FhlZ2d75g14kNwiA9RKBfQaldPLmEwcDHYu6DmOQ5HB5NI6XYHjOBQbTdAolTCYOGjV8veIC0uMKDIYoVEqUWIyQaVQwGDiBGJVfokBuhKl2+0tLDFCr1FZ/ue5zZQO3si2lu8ZTRxMHAeNyv69bX4/FxQbEa5TIbfQgOgwDVRK3w5lX1BiRMPX1/t0mzzHZ/VFuNYz3a1Zs2ahT58+ludxcXFo1qyZ5fmbb76JX375BWvWrLFx6rCMHTsWI0eOBAC88847+OSTT7B3717ce++9kvOXlJTgq6++Qq1atQAAkyZNwqxZsyyvf/rpp5g+fbrFpfTZZ59h7dq1Dt/P0qVLUadOHTRq1AgAMGLECMyfPx9dunQBYC45vnXrFvbt24e4uDgAQO3atS3Lv/322xgxYgRmzpxpmcbuD2eZPHkyhgwZIpjGim7PPPMM1q9fj+XLl6Nt27bIycnBxx9/jM8++wxjxowBANSqVQudO3cGAAwZMgSTJk3C//73PwwbNgyA2XE2duxYn5VFEqEJ+7sVauV7JEoRfiS0vkwEQRAEEahs3boV77zzDr744gu0a9cOZ8+exXPPPYc333wTr732muQys2fPFnT4Ao0T6dl44POdCNeqsHVad8SGa51abtT8PTh5PUf29f/+chQ/H7iCTVO7ITnO+TweZ5n00yH8deoWKkfpUFBixJZp3WUFpX0X76L5zA1IitEjs6AE4VoVrtwtEMzz4sp/sPNsBv58vhtqV450qS07zmTg0QV70CYlDgcu3cV7DzbF0FbV8MmmM5jLhJvfzS9BelYBkmLC8MDnO3ArpwjbX+xpV1Abs3Avtp/JEExrk1oBK/7T0aU2EmZat24teJ6bm4sZM2bg999/R3p6OgwGAwoKChw6pZo2bWp5HBERgejoaNksOgAIDw+3CFIAkJSUZJk/KysLN27cEDiUVCoVWrVqZXE0ybFgwQKMGjXK8nzUqFHo1q0bPv30U0RFRSEtLQ0tWrSwCFJi0tLS8Nhjj9ndhjOI96vRaMQ777yD5cuX4+rVqyguLkZRUZElm+vEiRMoKipCr169JNen1+st5YjDhg3DwYMHcfToUUGZJEG4g8HIilJ+bIgXIFGK8B8hpvASBEEQhC+oVKkSVCoVbty4IZh+48YNJCYmSi7z2muv4dFHH8XEiRMBAE2aNEFeXh4ef/xxvPLKK1AqbcWF6dOnY8qUKZbn2dnZSE5O9uA7KRtHr2ah2GBCscGEc7dy0SpFuvMqZte523Zf/2mvuVP/zbbzeHNQ4zK3U8zv/5jzeHKLzGHVR65moU2qfNsLSow4n5EHALiTZ/s6L/x8seUs5g5v7lJbnlt6CBwH7L1oLt+atuIwhraqhh93/2sz7z9XspAQpcfRq2bH3OkbOWhcVX6UNrEgBZhFNl8TplHh+Ky+Pt8uv21PERERIXg+bdo0bNiwAXPmzEHt2rURFhaGoUOHori42O56NBqN4LlCobArIEnNX9ayxOPHj2P37t3Yu3cvXnrpJct0o9GIpUuX4rHHHkNYWJjddTh6XaqdUkHm4v36wQcf4OOPP8a8efPQpEkTREREYPLkyZb96mi7gLmEr3nz5rhy5QoWLlyInj17IiUlxeFyBGEPtnzPFGKqFGVKEf6DyvcIgiAIwmW0Wi1atWolCM01mUzYtGkTOnToILlMfn6+jfCkUpk7zHIdTJ1Oh+joaMFfIFHMDImd74VcJV/lKOnsuI1cwZ2yuJwi6VHcpIYbv5NXLNjn7maa+DoLRaFQIFyr9sufN8u1du7cibFjx2Lw4MFo0qQJEhMTcfHiRa9tT4qYmBgkJCRg3759lmlGoxEHDx60u9z8+fPRtWtXHD58GGlpaZa/KVOmYP78+QDMjq60tDTZvKumTZvaDQ6Pj48XBLKfOXMG+fn5Dt/Tzp078cADD2DUqFFo1qwZatasidOnra7BOnXqICwszO62mzRpgtatW+P//u//sGTJEowfP97hdgnCEUKnVGiJUuSUIvwHiVIEQRAE4RZTpkzBmDFj0Lp1a7Rt2xbz5s1DXl4exo0bBwAYPXo0qlatitmzZwMABg4ciLlz56JFixaW8r3XXnsNAwcOtIhTwUaxwcuilI9GnCsyeOZ6yB1Rqlhm21LdnTt5xYK22svlskeJ0QSVMjiPuUCiTp06WLVqFQYOHAiFQoHXXnvNYcmcN3jmmWcwe/Zs1K5dG/Xr18enn36Ku3fvygpyJSUl+OGHHzBr1iw0bix0Ik6cOBFz587FsWPHMHLkSLzzzjsYNGgQZs+ejaSkJBw6dAhVqlRBhw4d8MYbb6BXr16oVasWRowYAYPBgLVr11qcVz179sRnn32GDh06wGg04qWXXrJxfUlRp04drFy5Ert27UKFChUwd+5c3LhxAw0bNgRgLs976aWX8OKLL0Kr1aJTp064desWjh07hgkTJgjey6RJkxARESEYFZAg3IU9X4eYUYqcUoQ/CbFvE0EQBEH4iOHDh2POnDl4/fXX0bx5c6SlpWHdunWW8PNLly4JXAKvvvoqpk6dildffRUNGzbEhAkT0LdvX3z99df+egtlhhVICkucE6VcKTvKLpR2EXkaT42ep/RQgLjRxEkKchm5RYKAdYOEm4pdhxzuilmEkLlz56JChQro2LEjBg4ciL59+6Jly5Y+b8dLL72EkSNHYvTo0ejQoQMiIyPRt29f6PV6yfnXrFmD27dvSwo1DRo0QIMGDTB//nxotVr8+eefqFy5Mvr3748mTZrg3XfftYjo3bt3x4oVK7BmzRo0b94cPXv2FIyQ9+GHHyI5ORldunTBww8/jGnTpllyoezx6quvomXLlujbty+6d++OxMREDBo0SDDPa6+9hqlTp+L1119HgwYNMHz4cJtcrpEjR0KtVmPkyJGy+4IgXIF1qoaaU0rBeXKsUhfZtm0bPvjgAxw4cADp6en45ZdfbL70YoqKijBr1iz8+OOPuH79OpKSkvD66687bYvMzs5GTEwMsrKyAs6GXm6YUZo/MHQh0HiI/XkJIpgoygXObwFq9QK0ng/HJQjCP9C1gxlf7Yeb2YW4mVNkySs6dysXl+7ko2X1CjiZno02qXFQKhWCMO7ZQ5pgZNvqDtdtMJpQ+5U/bKb//mxnXMssRGrFcPT5aBsAoE7lSHw3vi2yCkqg16hw5GoW1EoF1KXij0KhgALmnI92NSpaMpYidGoUGYw4dCkTkTo17uYXo2OtSlApzRk3NaYLRyZ7oW89PN3DOqpYdmEJms740+X9NrpDCmYMbISd5zKQX2xE+5oVkXY5E0UlRhhMHGpXjkRmfglap1TAoct3Ea3XWN4ryycjW+DZnw7ZTH+geRW80LceOr+3BQAwpGVVDGxWBV1qV4JaNBLf5Tv56PL+Fsl2pr3ex+lQencoLCzEhQsXUKNGDRID/IDJZEKDBg0wbNgwvPnmm/5ujt+4ePEiatWqhX379nlFLKTjPDTJLizB6es5aJVSARm5xUjPKkClSB2yCkpw8no2nl92GACwaWo33MkrRp3KkTh3Kxctkitg94XbuJtXgjCtEq1T47D/4h3UrBSJ1EoRDrbqxffj5HWDX8v38vLy0KxZM4wfP95mKE45hg0bhhs3bmD+/PmoXbs20tPT/WJVJdyE1UCpfI8INVY9Dpz6HWg6HBjyjb9bQxAEEZS0fcec1fLn811RPS4c987bhhImS+PNBxrh0Q6pbpXvybl0Bnyyw2ZaTqEBHd/d7ErT0al2RSye2B7TVx3BqoNXLdO/eKQl+jdJEtzp5vlg/Sk82iEF0XpzadGgz3e6tE0epUKB9ceu48nF9vN87m2UiHXHrsu+LiVIAaWZUsw+X3XwKlYdvIp3hzTBCJEgKCdIAZDcB0Tw8u+//+LPP/9Et27dUFRUhM8++wwXLlzAww8/7O+m+YWSkhLcvn0br776Ktq3b+8X9xoRvAz6fCfO38rDvOHNMXlZmuC153vXtTwe9e0epGcVWp4/2j4FP0gMUBGuVeGfN+6xuXEQaPhVlOrXrx/69evn9Pzr1q3DX3/9hfPnz1uGB01NTfVS6wivEGJWQ4IQcOp38///LCNRiiAIoozsPJsBXf3KAkEKAFYcuGIWpYyul++5UvJQaHC9rG7nWfPIfqwgBZidQ4B8htPt3GKLKHX+lsQQe06gVipw9mauw/nsCVL2yCsySApKl+/aD4/u3aAyAGDjCXN5k8FI14KhhFKpxKJFizBt2jRwHIfGjRtj48aNaNCggb+b5hd27tyJHj16oG7duli5cqW/m0MEGfz5f83hazavHbuWZXnMClIAJAUpwHzDJq/YiJgwEqU8xpo1a9C6dWu8//77+OGHHxAREYH7778fb775plPDcxKBADmlCIIgCIJwTG6hAbfzbIe3j9SZL1+FTinn8p9cyTPKzPdc0Dnv5GJzsBpVicaxa9kAAKObrv8wjQoFpYKcSqmQ3F+eIr/YKCmqOXKpfTumDQCg4evrkF9sJFEqxEhOTsbOne65+0KR7t27u5RdRxBSSEUEupsjVVBsREyY45B/fxJUotT58+exY8cO6PV6/PLLL8jIyMBTTz2F27dvY+HChZLLFBUVoaioyPI8OzvbV80lpBCU79EJmyAIgiAIaXKLDLiTayuyROnNl69FbpTvmfwUss07uXhRR6NSCNxdfPtd7cxWqxCGM6XuqBIjhzteFKUKS6RFKWddahqVEoCRyvcIgiAcIDV6pb0BJOxR4OQ52p8Eto9LhMlkgkKhwOLFi9G2bVv0798fc+fOxXfffYeCggLJZWbPno2YmBjLX3Jyso9bTQgQuKNIlCIIgiAIwgoryuQUGSRFlkid+Y5vsRuj7/lr5DdedOLbrFUpUVhibT8/Al9ukWPHFy/KAWZRiqfYaPSqKOWuU4pHozJ3sgyUBUsQBGEXqbFU2d8MV3DWSexPgkqUSkpKQtWqVRETE2OZ1qBBA3AchytXrkguM336dGRlZVn+Ll++7KvmEpJQ+R5BEARBENKwLpqcQgN+2nfJZp6fD17BZ5vPCC60nRFGCoqNltH63CEx2rkRriZ+t99m2g+7/8UrvxzB19vOAQA0aqWg/TN/PY6M3CKnRKVKkTrL4+Q460ivP+6+hB1nM5xqozsUFBvx5/EbNtP/l3YN2YWOSx3VSnO3g8r3CIIggA3Hb2D1oau4mlmAzzafwYF/71pekzrX/n3+tlvbeW31Ufx1+pbb7fQFQVW+16lTJ6xYsQK5ubmIjIwEAJw+fRpKpRLVqlWTXEan00Gn00m+RvgBGn2PIAiCIAgZWCdOdkEJDl3KlJxvzp+nBZkbBU6IUmuPpGPJHluRy1maVIvB9eOFDufbeMK2MwEAi5ltZ+aXYESbZCzdZ75Zejw9G4t3X0KXupUct6NqDC5kmMNwu9WNx/d/SwfcepqCEiMW7boo+dpbvx3H+0Ob2V1eozZ/YFS+RxBEeYfjODz2vfkGhl7DO2fdv2lij4OXMjFmwV5cfHeAV9bvCfzqlMrNzUVaWhrS0tIAABcuXEBaWhouXTL/aE+fPh2jR4+2zP/www+jYsWKGDduHI4fP45t27bhhRdewPjx4ynoPGgIgUypkkLg4g7A6LkAVIIgCIIgxDlR9ksO2Eo8ZwJg07Okox5aVI91qm1vPtAY3zzaCmM7ptq81qWOYzFJzKv3NRQ8zykswW2JDC0AeLpHLTzetSZe7lcfbwxsiG8ebYXlT3RAz/qV0Tw5VjCvTq3E8ic64NledTC2YyrGdkzFvOHNMbSV9A1cnkZVovFgS+s8WrWwmyAufZw9pInlMXsXXi63S0NOKYIgCADCfCh3y/JCCb86pfbv348ePXpYnk+ZMgUAMGbMGCxatAjp6ekWgQoAIiMjsWHDBjzzzDNo3bo1KlasiGHDhuGtt97yedsJNwmFTKlfngCOrwY6TAL6vu3v1hAEQRBEyMA6pXIKnc/BcCYqSm5kuhfuqYeHv93jcPnEGD0SYxJxT6NExEfp8MH6U5bXXrq3Praf2eF0ewHzKIKJ0Xpczza7r4qNJtzJK5Kc94W+9QXP72mUaHk8vE0y0i5nWp4/2j4FbWvEoW2NOMEyPepVxsoDwriL7vXisfWUWVB6oHkV1KkchZ8PmudJjNbj0p182fZXiZW+IVwikxml5jOlyClFEEQ5x1v5hjXjI3D+Vp5X1u1N/OqU4ofMFP8tWrQIALBo0SJs3bpVsEz9+vWxYcMG5Ofn4/Lly/jwww/JJRVMhEL53vHV5v///syvzSAIgiCIUMNdUcqZUYnk8ppUUmNvu4hO7d4lNesGKzaYZIUze4jDx+WG/o4Os70XHaW3zhup0yBMq7I8T4i2xl+oJfZRODOvgonllfsszKPvUfmeN+nevTsmT55seZ6amop58+bZXUahUGD16tVl3ran1kMQ5QFviVLseVmMq6O7+pKgCjonQoEQEKUIgiAIgvAKrGCRXeB8mbwz5XveFKXEpW7Owg7VXWww4Y5M+Z49xCP2SYwkXjrd9oUwjbXdkXq1QFyrzAS7h0l0dPRq6zSOub6T62ypVVS+J8fAgQNx7733Sr62fft2KBQK/PPPPy6vd9++fXj88cfL2jwBM2bMQPPmzW2mp6eno1+/fh7dlhwFBQWIi4tDpUqVUFQk7S4kiEDG6KXzYJhGXpQqCeBzL4lShG/hQiBTiiAIgiAIj7L7/G30/3g77vlom2VaTpEr5XuOrykyZAQff4pSbCdh1aGr+HbHBZfXIRalXIF3LwFAlF4tEJQSohhRSqKjY2T2+Y3sInxXGoIu19nSlO5ng0x5X3lmwoQJ2LBhg+Ro4gsXLkTr1q3RtGlTl9cbHx+P8PBwxzN6gMTERJ8NLvXzzz+jUaNGqF+/vt/dWRzHwWBw/ztIlE+8dR4M08qnM5UEsEuVRCnCx3AyjwmCIAiCKK8UGUw4np7t9vLOlO/JOa9USgV6N6hsd1lxSHjvBgmWx7UrR0Krcu2SulPtigCAaffUdWk5Ke5l8qUAYd6UmPY1hTlTaqXC4qxqWjUGteMjLa8NblEVAJBSMdymJGRU++qoUTFCMO2NNccAyGdKWcv36PpPzH333Yf4+HhLhAlPbm4uVqxYgQkTJuD27dsYOXIkqlativDwcDRp0gQ//fST3fWKy/fOnDmDrl27Qq/Xo2HDhtiwYYPNMi+99BLq1q2L8PBw1KxZE6+99hpKSszfnUWLFmHmzJk4fPgwFAoFFAqFpc3i8r0jR46gZ8+eCAsLQ8WKFfH4448jNzfX8vrYsWMxaNAgzJkzB0lJSahYsSKefvppy7bsMX/+fIwaNQqjRo3C/PnzbV4/duwY7rvvPkRHRyMqKgpdunTBuXPnLK8vWLAAjRo1gk6nQ1JSEiZNmgQAuHjxIhQKhWUQLgDIzMyEQqGwRMps3boVCoUCf/zxB1q1agWdTocdO3bg3LlzeOCBB5CQkIDIyEi0adMGGzduFLSrqKgIL730EpKTk6HT6VC7dm3Mnz8fHMehdu3amDNnjmD+tLQ0KBQKnD171uE+IYILZ36z3CHcjlMqkF2qfg06J8ohbMkeOaWIUEOhpLJUgiAIN7BXcqBUADtf7onNJ2/ilV+OSs7jzE1nuSwjlVKBLx5phUt38vDfVUex9+Idy2vd68Xjv/0boGYloQBTLzEKf73QHTmFBtSKj5S96/3R8GZonRKHrIISPLX4oCU4fMHYNgCAp7rXRk6hAV9vO+/4DcjQLDkWW6Z1R6ROjbwiA1JFbWVZNK4trtwtQO+5f5W+dyUOvtoHBSVGVIw0u1x2vtwTEVoVYsO12DqtOypH6/Dgl38L1jPz/sZQKRVY9VRHDPlil+A1uc6W34LOOQ4okQ9s9yqacPl6Sga1Wo3Ro0dj0aJFeOWVVyyllitWrIDRaMTIkSORm5uLVq1a4aWXXkJ0dDR+//13PProo6hVqxbatm3rcBsmkwlDhgxBQkIC9uzZg6ysLEH+FE9UVBQWLVqEKlWq4MiRI3jssccQFRWFF198EcOHD8fRo0exbt06i+ASExNjs468vDz07dsXHTp0wL59+3Dz5k1MnDgRkyZNEghvW7ZsQVJSErZs2YKzZ89i+PDhaN68OR577DHZ93Hu3Dn8/fffWLVqFTiOw/PPP49///0XKSkpAICrV6+ia9eu6N69OzZv3ozo6Gjs3LnT4mb68ssvMWXKFLz77rvo168fsrKysHPnTof7T8zLL7+MOXPmoGbNmqhQoQIuX76M/v374+2334ZOp8P333+PgQMH4tSpU6hevToAYPTo0fj777/xySefoFmzZrhw4QIyMjKgUCgwfvx4LFy4ENOmTbNsY+HChejatStq167tcvuIwMZbmVJsqfUDzavg0fYpGPqV+fwtd8MgECBRivAtVL5HhDIKFYlSBEEQbmAvnNXEAUkxYYgN09q8plCYLyecKd8TB4LzqJQKaNVK1K4chRqVIgSiVJReg7oJUZLLpTBOIbkKutYpcUiOC0cygPgonUWU0pXmMSmVCjSqatupd5UapUJUfJT98im9RoXala1uKJUSqBChRQVmnqrMqHq8wCUOcudLHmtVioQYubvxGn9lSpXkA+9U8e02ef57DdDKi4Qs48ePxwcffIC//voL3bt3B2AWJR588EHExMQgJiZGIFg888wzWL9+PZYvX+6UKLVx40acPHkS69evR5Uq5v3xzjvv2ORAvfrqq5bHqampmDZtGpYuXYoXX3wRYWFhiIyMhFqtRmKivCNvyZIlKCwsxPfff4+ICPP7/+yzzzBw4EC89957SEgwOw0rVKiAzz77DCqVCvXr18eAAQOwadMmu6LUggUL0K9fP1SoYD5q+/bti4ULF2LGjBkAgM8//xwxMTFYunQpNBpzkH/dulZH4ltvvYWpU6fiueees0xr06aNw/0nZtasWejTp4/leVxcHJo1a2Z5/uabb+KXX37BmjVrMGnSJJw+fRrLly/Hhg0b0Lt3bwBAzZo1LfOPHTsWr7/+Ovbu3Yu2bduipKQES5YssXFPEaGBt86DrChVo1IEWqfGQaNSoMTIUfkeQUhCnXci1FDKd6oIgiAIeaSCtMVIuZEideb7q0YnRKkig1FyOjuynFJ0ZRypc+68Lle+x2ZNyQlvrpb+eRKlk3lacrPptcK2lxhNdkbfM6+ERt+Tpn79+ujYsSMWLFgAADh79iy2b9+OCRMmAACMRiPefPNNNGnSBHFxcYiMjMT69etx6dIlp9Z/4sQJJCcnWwQpAOjQoYPNfMuWLUOnTp2QmJiIyMhIvPrqq05vg91Ws2bNLIIUAHTq1AkmkwmnTp2yTGvUqBFUKuv3IikpCTdv3pRdr9FoxHfffYdRo0ZZpo0aNQqLFi2CqfT8kJaWhi5dulgEKZabN2/i2rVr6NWrl0vvR4rWrVsLnufm5mLatGlo0KABYmNjERkZiRMnTlj2XVpaGlQqFbp16ya5vipVqmDAgAGWz//XX39FUVERHnrooTK3lQg8vJUpxZbv8Q5ktTLwB5kgpxThWzjKlCJCGAWJUgRBEO5gr3yPR0rsiNSpkVNogMlBKYTJxMk6pZRMeZV4hDpe9HKE1Mh2gFBwknuPYheSL1E5UVpmD7Ggll9slL0bbx19z8eilCbc7FjyBxrXQsYnTJiAZ555Bp9//jkWLlyIWrVqWUSMDz74AB9//DHmzZuHJk2aICIiApMnT0ZxsesjNsrx999/45FHHsHMmTPRt29fi+Poww8/9Ng2WMTCkUKhsIhLUqxfvx5Xr17F8OHDBdONRiM2bdqEPn36ICwsTGZp2H0NAJSlnXeO6a/IZVyxghsATJs2DRs2bMCcOXNQu3ZthIWFYejQoZbPx9G2AWDixIl49NFH8dFHH2HhwoUYPny4z4LqQwWTiXNabPcn3sqUYm/w8DdCNCoFCkrM2Y2BCjmlCN9CmVJEKKOgUypBEIQ72Cvf45G6bOBLwuxd33+w/iSaz/pTdh41Y49SizozMWG2bgtX0KjZ0e2k1+XuyH2eQFNGl5ZYjGs280/0YUZQFGyrdN/6fFhyhcJcQuePPxdFv2HDhkGpVGLJkiX4/vvvMX78eMs+3rlzJx544AGMGjUKzZo1Q82aNXH69Gmn192gQQNcvnwZ6enplmm7d+8WzLNr1y6kpKTglVdeQevWrVGnTh38+++/gnm0Wi2MRmnXIbutw4cPIy8vzzJt586dUCqVqFevntNtFjN//nyMGDECaWlpgr8RI0ZYAs+bNm2K7du3S4pJUVFRSE1NxaZNmyTXHx8fDwCCfcSGnttj586dGDt2LAYPHowmTZogMTERFy9etLzepEkTmEwm/PXXX7Lr6N+/PyIiIvDll19i3bp1GD9+vFPbJsysPZKOZjP/xJZT8m67QMEXmVL8SHz8eb733L/w31+OeGW7ZYV6UISPYTOl7P+gEUTQIa77IAiCIJzCXvnepyNbAADubZyI5Lgwy6hwgDXbyN5d58+3nEN2ofyQ7SqVVTh4qnttVCoN/E6K0WNU+xTn3gCAgc3MZVG8e6htahwimPf1fJ86qBihxbM9haHF9kSpcK0KL/R1vxMvx/hONZAQrcOYjqlOzc/u3bcHN3Z6O28Oss5rcUp5qTMWCkRGRmL48OGYPn060tPTMXbsWMtrderUwYYNG7Br1y6cOHECTzzxBG7cuOH0unv37o26detizJgxOHz4MLZv345XXnlFME+dOnVw6dIlLF26FOfOncMnn3yCX375RTBPamoqLly4gLS0NGRkZKCoqMhmW4888gj0ej3GjBmDo0ePYsuWLXjmmWfw6KOPWvKkXOXWrVv49ddfMWbMGDRu3FjwN3r0aKxevRp37tzBpEmTkJ2djREjRmD//v04c+YMfvjhB0vZ4IwZM/Dhhx/ik08+wZkzZ3Dw4EF8+umnAMxupvbt2+Pdd9/FiRMn8NdffwkytuxRp04drFq1CmlpaTh8+DAefvhhgesrNTUVY8aMwfjx47F69WpcuHABW7duxfLlyy3zqFQqjB07FtOnT0edOnUkyysJeZ5afBA5RQaMW7jP301xiLdK6djyPb3GfM5VM79xS/a4VorrK6gHRfgWQdB54FoICcItqHyPIAjCLeRyld4Y2NAi9kTo1Nj2Qg98NLy55XXeiOJM0LkcbAlbYowe+17phYvvDsCul3siNtw2XF2OT0e2wIXZ/XHqrXtxYXZ/LHuivcBJVK1COPa/2htT7hGKTPbK9w6+1gdP9/D8yFuvD2yI3dN7IS7C+ffH80g754S6qrFheJQR9fhMKWMAjwAVCEyYMAF3795F3759BflPr776Klq2bIm+ffuie/fuSExMxKBBg5xer1KpxC+//IKCggK0bdsWEydOxNtvvy2Y5/7778fzzz+PSZMmoXnz5ti1axdee+01wTwPPvgg7r33XvTo0QPx8fH46aefbLYVHh6O9evX486dO2jTpg2GDh2KXr164bPPPnNtZzDwoelSeVC9evVCWFgYfvzxR1SsWBGbN29Gbm4uunXrhlatWuH//u//LKWCY8aMwbx58/DFF1+gUaNGuO+++3DmzBnLuhYsWACDwYBWrVph8uTJeOutt5xq39y5c1GhQgV07NgRAwcORN++fdGyZUvBPF9++SWGDh2Kp556CvXr18djjz0mcJMB5s+/uLgY48aNc3UXEUGEOFPqzUGNMbVPXZm5nYe9wcM7pMrqiPUFlClF+BjmotFETikixKCgc4IgCLeQy2TiR6mTm4/PgyqTKCUq2eO3IdcmezhaRup1e04pcds8iSvvz51WsHfnAet7IaeUfTp06CDINOKJi4vD6tWr7S67detWwXO2fAwwj0K3fft2wTTxtt5//328//77gmmTJ0+2PNbpdFi5cqXNtsXradKkCTZv3izb1kWLFtlMmzdvnuz8U6dOxdSpUyVf02q1uHv3ruV506ZNsX79etl1PfHEE3jiiSckX2vQoAF27dolmMa+t+7du0t+PqmpqTbv9+mnnxY81+v1mDt3LubOnSvbtqtXr0Kj0WD06NGy8xDBj9jdq1UpnBrwwxF8yR5gvREQDKJU4LeQCC04Kt8jQhhyShGE98m6AqQtAYzS4bNEaOEob4l3OZUlNNabwo8z2Bt9T1nGIHJ/It6vwTACFEH4i6KiIly5cgUzZszAQw895HaZI2GbDRiIiMV5tVLpkfM9W77Hn3ODYX+QKEX4FrZkj+zbRKhBTimC8D5fdARWPwnsnOfvlhA+QKOyfzHNj7JUFvNNWUegKyv2mh4EfQlZxB0hckoRhDw//fQTUlJSkJmZaeNU8yUmE4cT6dmSo2Seu5WLvCL5fL5AwZc3GjiOw+kbOSgsEZotrmUWICPXNm+NRyzOq1UKV8dFkETPilLklCIIOShTighhyClFEN6nKMv8/5mN/m0H4XH4jgTrjnLUuagcZQ4lL0v5nr/R2Bkkw50SQm9QtYLrw9JfyywUPFdTphRByDJ27FgYjUYcOHAAVatWdbyAl/hsy1n0+3i7zShtaZcz0evDv3CPzOiagYQvRal1R6/jno+24eH/s45kmV1Ygo7vbkbrt+SvU8SZUiqlAhVcyDCUQ6ex/p5El474eiO7UG72gIFEKcK3UPkeEcoESOeBIMoF9BsScnz4UDO0rB6L9x5sYpmmkEkz+nhEc9zbKBGPd60JwL3yvSe718ITXWsiJlzjXoM9RPWK4Rjr5Ch4/uK1+xqgd4MELBrXxua178e3lVwmV+SoUJNTiiACno83mUPfl++/Ipj+x9F0AMDVzAKft8lVfClKLdlrHs3u4KVMy7QLt6zh9XK/TeLpaqUC9zVNkt3OQ62q2W1H46rR6N8kEc2qxeKFvvUwqn11NKoSDQC4nVdsd9lAgILOCR9DQedECEPlewThO0yBX0JAuMagFlUxqEVVlBhNeH7ZYQDy5WsPNK+KB5pXxeHLmQDMJSdSSAUS87x0b/0ytdeTzLi/ERbtuujvZshSOUqPb8e0lnyta9149G+SiLVHrttdh4oypQgi4JEV+IPoa+tLUUrKzWpkfneKDSbJAHOxOK9SKqFWKfHukCZ4edURm/mn3FMXKw5csZkOAE90q4np/RpYnntjxFZvQ04pwrewJXtUvkeEGlS+RxC+g25shCxsFpGj8jWVg0wpcuX4hjCN9T53uMwIUr52StkTJAki2KHjWx5/5wSywl6xQbq/a+OUKi1vlhPU1HbKvMM1we8zCv53QAQXVL5HhDJK5pRqMgF2fkAIgigjdGMjZGGFKEd9C360IqNMB02uQ0B4FlaIignTIL/Y9hrPV5lSGo0GCoUCt27dQnx8fMDkchGEp+A4Drdu3YJCoYBG49/y40DEp04piWklTEh8kdEIwPYzKhEFyfOivVpmcA97g36EaYO/v0GiFOFj2PI9ulAkQgyN3vq4JB/QRfqvLQQR6pBTqlzgaIhsXvsXl++tPZKOSpE62Qt8wrOIRan0LNtgXYtTysvleyqVCtWqVcOVK1dw8eJFr26LILyF0cShoMSICK1KUlhVKBSoVq0aVKrgcOnvu3gHa9KuoUudSrinUaLsfIcvZ+JCRh4GtbANe/9f2lXsv3gXw9sko3HVGPxzJRNbTt7CkJZVkRxnHYzBnii17+Id3MopQv8mSbiZU4h1R69jUIuq2Hj8BmpXjkTTarFuv8c952/jQkYe1h2zljI765Ti2yz3m2fvPYVpg1/SCf53QAQXHI2+R4QwKp31cXEuiVIE4U3IbVsuqBobZvd1vkyDHX3v39t5eGrxQbvLRevpEtiTsB257vUq4+T1HEEZJsBkSvmgfC8yMhJ16tRBSUmJ17dFEN5g6Fe7cDevGINbVMWknnVsXtdoNL4VpMqo7z/01d8AgB92/4vd03shMUYvOd8Dn+8EACTHhaFVSpxl+oF/7+K5pWmWdVx8dwAenb8XWQUlSLt8FwvHWQdcsCfg8O3YOKUbnl58EKdu5ODLrecsQvrFdwe49L5YDWn4N7ttXpcTpcTnQb48jy3TqxCuwd188zmMHZVWTJyDUfsqReqQkVtkdx5/Q7/IhP+gDgURcjA/MMV58rMRBFF2yCkV0iye2A5XMwvQsHT0IDmUSr4kzHr+tTfS0Ov3NUSx0YT+jeVHOSJcp3+TRMwY2BBNk2PRMCkaFcI16NUgQTCPWuKz8iYqlSpoXCQEIebodfMod78dy8C0/k0czO1bTCbOcu51h1s5RbKiFM+FjHyBKHX6Ro7NPFkFZsHmRLrwNWfK965mFuBU6TqlnJ3O4mhLxUZXM6Ws057rVQfFRhN61k+ATq3C14+2whM/HBAs92T3WujVoLLdNvz8ZAe8/PMR/H3+toPW+g8SpQjfwrqjqENBhBqsE7A413/tIIjyAN3YCGk61a7k1HxKi1PKOk2rkr+j3LF2RdRPtC90Ea6jUCgwtlMNy/MnutWymUdlCTonpzxBBDPFRhP0ZRhxmnNiKD+x01KcwVRkkL8GcEaUMvkoKF7WKSWTKaVinFIROjXGtk62PO/bKBF9GyVg/bEblmnP9KwNvcb+Z5FSMQJzhjVDp3c323Vc+ZPAbBURulD5HhHKsMc0OaUIwrtQx5aAdPme3J1pAAhzcPFOeA8+qNfbmVIEQXgXe+dYKcQjBTqjB4mFpRLReeNunnxprlOj73noNORoIAVny/dUSlunlEbiBos4c8pR7qJ1PvP/gTpqI4lShI+h0feIEIZEKYLwHfQbQsAadO7MENwAEKYlUcpf+DJTiiAIzyIoBnBxVFNxqZrcGYAVTMROKbGz6HaeMCOJXbYsTilXRRuH5XtOBp3zWVKsU0rqfYjLJp0dadAyUm2Ann9JlCJ8C/tFp7vcRKghEKWofI8gvAqVgBNgy/fM1xcn0rNtMjdYyCnlP3ydKUW4z528Yjy39BB2ns1wa/kT6dmYtOQgLmR49wbdjjMZeG7pIWTmS+fIHb+Wja7vb8HAT3dgxf7LmLz0EO7IZM7N23gan2w6Y3d7V+7mY9KSg/jnSqbNa4UlRkxdfhjrjl63XRBmsWPGmmP4zw8HcDPH9Qyj3/65hhdWHLZbtuZNWHGFf7xw5wWMX7QPk5YcxKnrtplPPGIhWk4QKmK2wYothy9nYvYfJwXz3s4Vfo6se0ulVOC11UexdO8l2TbJnYbsieZplzPxzE+HcDWzQHYeMUUGE4oMRryw4jB+/ycdAPDT3kt46/cTgvksmVKM80kszAG2ziinXGGQLnUPJChTivAtbKed7nITIQdzpi8iUYogvAr9hhCwdlz4C+1H5++xhN9KER7AQ2d3qxuPv07fgkalsClVCQUoUyp4eO+Pk/hf2jX8L+2ay6ORAcCgz3eiyGDC8WvZ2Dytu+cbWMqo+XsAAHq1Cu8NbWrz+uAvdlqEjhdW/gPA7DSZO6y5YL6cwhLM22gWpB5tn4IKEdKjmf3nxwM4ejUb645ex9l3+gtem7/jAn4+eAU/H7wiuc8u3cnHol0XAQCd61TCqPYpzr9RAJOWHAIANKkWg9EdUl1a1hOwok+RwQSO4zDz1+OWaX+duoUjM/tKLmvjlJI5vbHb4EUawDoiH4tYXGRFs2PXsnHsWjYAYETb6pLbkhPGig0mybI5wHxcA8CNrEIs/08HAMLR96QoMpiwePclrDhwBSsOXMGApgMwfdURm/l4ASop1hoAnyQx+qxYp3I2cJ6djeM4h2WHviZwf5mJEIUypYgQhsr3CMJ3kFOKgG1JQkau/Mh7gPOlDv7g60db4di1LPx5/Aa+/uu8v5vjcShTKni4luW8E0QKXgg672WnFM+lO/l228Fy5Y7te2MFjbv5xbKi1NGrZqFDyk2T7mCfFZYIRR13kXN6eRuxUyq70CB4PafIIF7Egu3+kheEeBxlJYlHWXWmpJAtzZMVxgwmROjsr+d8hvM3nouNJtxwwhnH/zbVio/EmkmdUGLk0KxajO18zH5x5feM3Z8mDlAF2E8hle8RvkVQvkcdCiLEoNH3CMJ30G8IAeHdX1Og1iU4iV6jQquUOKfLMYINypQKHqTKhgIZZ0Zzszcv69C5K1MK6AhHhzU7epx4JDlX8NdnIxalbucW2ZlbiNNOKWYbjqKdxNt3JnxdMN6WnFPKxc/G1aBzufJlNZMl1bRaLFqlVJBcNzvNld8KoSgVeOdgEqUIH0NB50QIwzqlqMNMEN6F3Lb4/PPPkZqaCr1ej3bt2mHv3r2y83bv3h0KhcLmb8AA10tzAgn2TnEgXmi7Q4hqUpQpFUSoZcqXXMVXzsSyHlKscCDOKnIWR6cfVogSh3a7AhuE7UsEopTR6JJjS/x+5YRpdhuOzhPi7ReVSO9TgTuKmS63eldD3F0NOi8ske4fqJ20LrFfKVcOBQUzbyD+VpIoRfgWgURNHQoixOBIdCUIn1HOv2PLli3DlClT8MYbb+DgwYNo1qwZ+vbti5s3b0rOv2rVKqSnp1v+jh49CpVKhYceesjHLfcsbJ6GkeOg9VBnmvA81kypwOsQEUI0Hqrt8ZXrz5UR06RmZYUDZ8SWcIlRPB21gT3uy5IZ5zenlChTSlw+Zw/xd15OcGK3YXSwP23K92SEPnY6K8bIudXKUlopuX2DUaCG5RfLiFJOfq6s0OuuUyoANSkSpQgfQ04SIpSh45sgfIdJPr+iPDB37lw89thjGDduHBo2bIivvvoK4eHhWLBggeT8cXFxSExMtPxt2LAB4eHhwS9KMRfa205nuFx6EYgoHN57D07UlkypwPqMrmUWoMPsTej+wRa7IfnO8POBK+gwexOOXMmyO9+6o+lo/84m7L1wx+E6DUYTHvxyF1Jf/l3w99GG04L5nvhhP0Z887eljDWnsAS9PtyKN387LrVaWeZuOI21R6wjyPHb+/PYdRQbTLjv0+14YcVhAMDHG8+g6/tbcCvHWkr12Pf7LY+LjSZMWZ4muZ3nl6Xhgc924JFvd+PR+XtcEpbEcBxwISMPnd7djO9Kw8S3nb4lPa/EtHGL9lkef7vjAjrM3oSjV+U/w2i9xvI4v9iAez76C0v3XbZM+79t5ky4p5ccxJAvdsJo4lDCiB2ulu99ttk6KiD/PXryxwMY9tXfZS5bvpZZgM7vbcaXW8/JzvPD7n+x+aT1hsfr/ztmd5RTMWIRSixS3copQtf3t+CD9acs05744QBeWHEYTd5YL7lOtnzvenYhBnyyXXI+XnA8ezMXHWZvtkyfvCxNcv7ec/9Ctw+2WNbPcRwmLNqHR77dLTm/I12o2GgSHHPtZ2+SnM9ZVyFbvudsyDkgKnUPQFWKRCnCx5BTighhBKJU+e4wE4TXKcfCb3FxMQ4cOIDevXtbpimVSvTu3Rt///23U+uYP38+RowYgYiICNl5ioqKkJ2dLfgLNNg7xWxnmKVeQhR0aiVGd3BttCt/Map9CjQqBQY1r+LvpngUPjMl0Mr3dp+/jfSsQly8nY9jdoQIZ5i64jDSswrxwsrDduf7z48HcT27EP/50XHH/p+rWTjw712b6R9vsgoVJUYT1h+7gd3n7+DCbXO4+LJ9l3HuVh7m77jg0nv4hFkvy+M/HMDOsxk4ejUbKw5cAQB8tPE0Lt3Jt4gmxQYTNhy/IVhu1cGrNusymjj8cugqDl/Jws6zt7H9TAayC9y/bjJxHGb+egxXMwvwxppjAIDRC6TLmcXil8FowpW71pDyszdzkZ5ViElLDgq3wRy3seFWUer3f9Jx+oYwR/TttSdgMnH4/Z90HLyUiWPXslDCLO+qW3DOn1YBUq1UgOM4/HH0OvZevIPTN3NcWpeYTzadwZW7BXhv3UnZeV5bfVTw/OxN13JTbZ1Swj7gZ5vP4NKdfJtjZ8WBK7IB6jmioHU59xkvSs1eewIZTuZg/Xs73yLSFRlM2HTyJnaevS05r6ObCMWlIxXyiM9/KqUC9ROjEOHkyLCsGVi8D+zB3sAJtHMwQKPvEb6GyveIUIY9pst5aRFBeJ1y/B3LyMiA0WhEQkKCYHpCQgJOnpTvWPDs3bsXR48exfz58+3ON3v2bMycObNMbfU2zlQv1K4ciV+f6QytOjjuxSbG6HFs5r0eK6EKFAK1fI/NEJIrrXEVZzuLmU6Eauc6sS6pHBx3A7vtwX52bEf7Tn5J6evOXdtLvW9nl5XCxAH5Re59dnLuStb9BUDgomOdUnIUGqztMXFCh2BZgs5VSqVAVCjDbgMA5HnomLeH2B0pFpD448cVnP2u8p+vK64ic5vMx2hZz1eOMqoOvd4HEVq10+3TMKrUjIENnW6HePS9QMOvv87btm3DwIEDUaVKFSgUCqxevdrpZXfu3Am1Wo3mzZt7rX2EN6DR94hQhj2+SXQlCCIwmT9/Ppo0aYK2bdvanW/69OnIysqy/F2+fNnu/P7AmZIHrVoZNIIUj1atdDiqU7DBZ6YEWvkem01TIBNC7E/yix2LUkUSo5Y5I2aJcSXkmd1mbqFZVHA2K0kqt6ksWT4cnBe1xC2UC8gWizXsccJ+NTUyOXYFzPIcx7k9+p5YQFArFQ7zltylLCWU9nCUKcUfP67g7HeV/3ydERKFbTJ/fxydrxydposMJrsZTtF6jUsDArDHm8aF3zV2E976nMuCX3+h8/Ly0KxZM3z++ecuLZeZmYnRo0ejV69eXmqZjykpANb9F7i4w98t8T4UBE2EMuSUIgjCB1SqVAkqlQo3bghLHW7cuIHExES7y+bl5WHp0qWYMGGCw+3odDpER0cL/gINZ4JeKfw8MLBkSgXYbfo7eVZHTIEPXCMszgiPd51wkbBiEi/OyJU92d+WfXcV25llnSq5RfY78OJOsFRItqujnonX72x2uLg/7mwOHSukscew3Khp7P4pLDEJBDuDC0Hn2SLBRqlUCEQdTjIlyz285ZpylCnlShkajzNiLWD9fKPDXCsQ49tU1vNVkcHkwU9IGIiucWH4vUB3Svm1fK9fv37o16+fy8v95z//wcMPPwyVSuWSuypg2fkJsPtz89+MstWyBzxUvkeEMpQpRRCED9BqtWjVqhU2bdqEQYMGAQBMJhM2bdqESZMm2V12xYoVKCoqwqhRo3zQUu/D9um1aqVkxzbYXFKhCp8pVWw04VpmAarEhnl0/Rm5RYjQqhEmMTIaD8dxuHK3ANUqhMFo4nA8PRtplzMtr9/OK8axa1moViEcZ2/mIkKnQrhGjezCEkTp1VBAgYQYHXRq8zauZhYgSq/G3bxigfulsMQoyIHSqBSoEK5FYYlR0lmTnlVgESrUKgWSYsKQW2RAscFkdzS461mFKCgx4jDzHooNJtzOLcLNbKvYxnGcXQEsI7cIeo3Kbrg3AJy7lWd5zIoCOYUGXMjIw83sQsnlcosMiCp1qlzNLMDNHNtsH2fFocz8YmQXGBAXqbVMu3q3AEmxestzcekdy5W7+UjPKkBSTBiu3M13Ol+HFS95oeJ2bpGsmMZ+/gUlBsG+FTvKrmYWIClaj2tZBagaGyb4rDJFomRBsQH7LlrXzXFmwUynViJCZ+7aX88qRMVILW7lFMFo4lA1NkyyPOxmdqGglPJOru2x5kqQuqn0O5VSMRx380qgUZuPZXuZUjezC3H0muv930IZh5uYnFJRL8pVp5RFaLV9/0YTh8t38nEnrxjXsqSPeZ6rmQUedSYJnVLOO6zYr7+J43AtswAJ0XqkZxWgSoz08eFLgi5TauHChTh//jx+/PFHvPXWW/5ujme4LR0oGJpQ+R4RwrC/OXR8EwThRaZMmYIxY8agdevWaNu2LebNm4e8vDyMGzcOADB69GhUrVoVs2fPFiw3f/58DBo0CBUrVvRHsz2OQqGAUmG+8yvnmiJRKjDgS1Qy80vQ8d3NmD+mNXo1SHCwlHPcyilCm7c3IiZMg8Nv3CM739wNp/Hp5rOYMbAhzt3Kww+7/xW8/t66k3hvnf1tNasWg/9N6oyfD1zB1BXSgea384rx4Je7nGr75Tv56PbBFot7QakAtr3YA70+/AtFBhOGt06WXVZqJK9bOUW4/7OdgmkGEyebUcbvO2dgw7ALmfKpk9dz0GPOVtnlOs7ejCMz+2LHmQyMmr9Hch5nnFKFJUY0n7UBgNABeTuvWOC+svd+MnKL0WH2ZjzVvRa+2HoOLavHOtwuvw0eo8mErIIStHpLfjvs6G6fbDorED9ZAfOPI+l4crE1VH1C5xr4b/8Gsut97X/HBM+zC0pw36c7oFUrcfqtfjhyJQsDPxNW3zzYsho+HNZMMO3crVz0+vAv0Xu0FfNe+99Rm2lyfLTR/P1iWfVURxthixflrtzNR+f3tji9fnd48Mu/cfHdAS6PacqLUlKllnfzS9DlfefaLQ5vLyvs91jtglNKoVBAoTCLmH+duiU4d43pkIKZDzT2aDtdJah+pc+cOYOXX34ZP/74I9Rq5/S0YBg5plw5hsgpRYQyVL5HEISPGD58OObMmYPXX38dzZs3R1paGtatW2cJP7906RLS09MFy5w6dQo7duxwqnQvmODFDt4lAABVYqyuCRKlAgNxboqro8LZY//FOwCEYdRS8B3mGb8ed8udAQCHr5iXm/PnKdl5UiqGW/4qMY4eMUYThxPp2YJyGhMHHL6cZclYOnDJduQ9e0iN1GdP8NlXuu9cxZVQeL6U8PMtZ2XncSZTinV/OeuskuOL0tHVDl7KdGp+NqPLYOTwzxXnlgMgEKQAYf6VWMSZv+OCS5lT/HHMf8Y/H7xiM4/UtN8Op9tMy5Uo+Vy855LTbdl+JsNm2vJ9l2Uzpf485lnBxh6uBumbSvusgTZSnZp1Srk4GAZfwvet6Nz73d//Ss3uU4LGKWU0GvHwww9j5syZqFu3rtPLBcPIMeVLnCFRighhBOV7dHwTBOFdJk2aJFuut3XrVptp9erVC8iA07JiLnXhkFfaoVr6eHs0rRaDhq+vBxB4wdrlFWfyv9xetxulJ/bK4spCuFaFv17oYXm+7uh1/OfHA7LzX8sssJl2+kaO5bG4fMsRUgJUscGECJ1Lq3GIlIBRlmWccUr5M/ufDdY2mLgyjdTIlu9JnZFdyTESlwKqnfwuSIk0rmaqjWxbHT/ttYpWMWG2JXLiDCzzts3PfTmwgLMh/Dy8gFOWUSG9gaB8z8W8RJVCASO4gLwOCJpbRzk5Odi/fz8mTZoEtVoNtVqNWbNm4fDhw1Cr1di8ebPkcsEwcky5EmcEnXZykhAhBjmlCIIgfA4vdvAdnDCNCuFa631XdzrPhOcRV5p4UmBgRSlnO1y3JfJznCW/2GB3RC2WcDsZVwBw5a6tKHX2Zq7lcUaufD6SFFIOorK6iqTIduBKE2MwmuyLUk600RUHkSdghTJWsDGaOKeDtqVg34fU8Vri5iiIJhMncNLYb4PtdlmRyNH3U6NSQCdyoUo5d9RKha1TqvT9+3JgAVePHf6U4qqY5W0E5XsuilL8Zxpo7i8giJxS0dHROHLkiGDaF198gc2bN2PlypWoUaOG5HI6nQ46nYdvDXiaciVKhdjoe2c2AAcWAQM/BiIq+bs1hN9h/ffUCSIIgvAFYpeMWATIdmNkJ8LzuONmcmfd9vKTWMoiVt7OLXZ61DN7weuAtCjFOqVcJa/I9vrangvJXdOEo1JJMfklRkvotBTOOKW8Ia7Zo6DYaCn/ZZ1RBpPJZQcbCxuebZISpVxw57D7zcjJH/visHspkYZ9j0oHqpRWpbQRpaQcXiqlQhBszs5XFreZq7gy4iFgPae4upy3YXOkNC6eU/nP1BiATim/ilK5ubk4e9ZaR3vhwgWkpaUhLi4O1atXx/Tp03H16lV8//33UCqVaNxYGMBVuXJl6PV6m+lBRwAeGN4jxILOFw81/6+NAIZ849+2EP6HnIAEQRA+R9x30muEIkAuiVIBgbfK9zLzi/Erk49TbDDZlLUcvpyJvCKDJei3rNzJK3Z6PWEa+6LUX6dv2Uw7wzilXEUqP+jU9Rz8cugqovRq9G+ShPSsQhy5kon7m1dFsdG965U3fzvh0vyfbDyDDDvutGX7LqNT7YoCl6MYZ4QrT3L6Zg6uZZpHRmRD8S/fKbDJgnIFXhAyGE04fcP2s1539LrT62L3yeU7+bIZUDlFBkQzI9CtP2a7jd/+uWZ5bDRx+HST/GBcWrXSJq/vADMqIM+fx26gfU3hwBrzd1xAsdGEPRduy67f07jqlOK/34FXvmc9j2pczEvkNSxXRlT0FX4Vpfbv348ePaw111OmTAEAjBkzBosWLUJ6ejouXXI+XC1oKU+iVKgGnWdfczwPEfpQ+R5BEITPEQckR4uG/q6XGOXL5hAyeGvI8Qnf7ReEexdJ5Cc98PlOeJI7+cVO+qQcl++5kqvTu0ECNp5wPRx64vf7LY93n7+N9aUB01cyC1A9Ltzl9QGuO83E4cpiNp64gZlrjuO9oU1l5/G1KPXQV3/LvlaWTDJeIJELmH5dNMKePVhR8fnlh2XbdTev2HJuPHo1S9Kht/OsUCT6cMNp2e1q1UrBCIiANdCe5WpmgY3Ilp5ViPfXyQ8U4A1cLcPjS9xcyffyBazg7mx+GA/vlBK/JVfX4w38minVvXt3cBxn87do0SIAwKJFiySDOnlmzJiBtLQ0n7TVuwTWwe5VBJ32YBWl/P/FJQIUckoRBEH4HPYufIRWhZhwc8frj+e6YFKP2ni2Vx1/NY1g8JZTSjzanFi4sJefUrNSBJ7rVQdjO6bavJYUo8ebgxqjRfVYm2OoqMTovFNKJEqplQoMbFZFMG1i5xqY0LkGGiZFS65jUo/aeKp7LbxvR7BxlqNXrSORX71b4FSuT8vqsXiwZTW0qB6LLnW8F1exOu2q3dflRKnWKRVs9unM+xvh/QebomX1WMmSttSK7olxnoAXOtYdtR0Bz1XYfXJYNMofS2GJdb6zbjjxKkZoERtuFfy1aqXTTh0pN6Cz9Kpf2fL4hwlt8UzP2qjv4o0Gk4lz2fHEf0a+zjFzhJp1SnkoU8rV9XiDoMmUCmmCVpxxhxAo3/OU95sIPULVCUgQBBHA9GuciG2lnZ6H21W3TG+QFI0GMp18wvd4yyklRixc2HPXaFRKPN+nLracvIlFuy4KXtOqlXi0fQoebZ8CAPiEKWUqMpicDlQP1wi7W18/2gq9GiTg18NWl/2r9zUEABy6dBeDv9hls45pfes5tS1n0GmEHVBnRKlVT3USPH9+WRp+OWRfQHIHR7u0SEIg0KmVWPlkR3AcZ9mnrw5ogDGlQuOwNsl4ceVhLN8vLGv8dkxr9J67TTBt2j11MedPeXeQp+BdO57IVGLFJnuw3wNxibMz/P5sF7yz9gTWlO5jrUrptMPG3RLqj0c0R7Reg00nbwIAmlaLRZc68dhz4Y7sMlF6NXJE2ysxmVwWl3jhxhuh4Fq10m3XX1lG3+PPweJzlzfz/pzF/7IYUb46r6HQaVfQ14aQgT2+KeicIDyPuMcSYFkPhH9gw3bFGSdE4CDu+Ci85DwXZyQ50/mTCiMXlya5uk4evVa4HnsjZlUU1x16ATbAmoM5gNxVHIW3u4ujAOYiCQGG75izId7iY03qvBATprWZFqnzjV+jxIOjzzkrbLmbHcYTplWB3a1atcppUcTdgHq1UilyBikE/0uRXMHWAWcwci6X71lzvzwvSpXFNSocfc8zQeckShFmglWccYtQGH3P/19cIkCh8j2C8C7i71XQ/o4QnoTtcGpV3uksE2XHW+V7YsQZY0VOdMalwsjtCZzFRpPT4RticUu8F9jX4yJthRJPIxZC3BFGHIW3u4vUSHQsUuKGVMdc7OCROi9E6W0FqChRHp23MPCilBuCoJiCEudugrLfC2eXYQnTqARuR51a6fWyL41KIcpQUpZOl99u1QphNtNKjCbLPncWoxfL98oiAglH33M16Fw6UyoANCkq3wsIypMoFQqddh9dVBFBCAWdE4R3Ef9emgyAyjedCCJwYTv14tIkInAQd8R2nM3AllM30aNeZZkl7HP5Tj7mSgQx8y6m9ceuY+aaY2iVGudwXVJh5HZFKRfK9xQOrhv1zDEb4aYDSaVUOF1mlFdsFSR+/ycdHWtVtDO3NI7C293F0S6VcqhJCRTiUlEp4Uon8flGSghV3uDi7Xy8tvoo0rMKy7wucTi5HPy+4zgOb//u2siJgPn7wArLWrXSZaeOq2hUSuFoc6WP1XbEmGoSotSo+XsEWWrOcDuvGFOXH8aWUzddWs4ZyiICCcr31K46pcz/i0ff84YbzFXolzsQKFeiFPs4WN+3xAmAMqYIACGRmUYQgYyNKEXfM0LslKJL20BFyh0wbuE+t9e3bN9lyVwjvvP9xA8HcC2rUJDdBACx4Ro0T44FADzUuhoA58r3EqKtpXXFBpON28BZalWOBABLYDgbsi4lYN3TMEHwvFKkuR3d6sYDAJLjwiSD2uXIE42Qtuucc6IGi5Sg4wukRKnKUbYljy2SKwieSx174n3dt1ECGlXxXQbdD7ulR97zFvy+O3wlCxm57o0cyJZ+6jUqu2V0nkCtUqB6XITlOf+Z2dtu1VhbUcpVQYrn54NXyjTKohwj2pqzD/nzkCsIyvfcdEqJy/ekstp8DTmlAoGgFWfcIRQypcgpRcgQCk5AgghkxA5Eym4jIBKlKFMqYPF0iUiuxPDzgP38mneHNEGL6hWQGK3Hwct30aW2WRhypnxv7bNd8MDnO3HlboGNOFI9LhyX7uTLbnf7iz2w+/xt1Kocaek0f/FIS+y/eBedRaPZLXmsHR7+vz0AgCEtquLNQY0Fr6+f3AXHrmWjU+1K2HE2A02qxiBCp8LpGznYfiYDADC6Qwo61a6ESJ0aj3y7R7C8XLZOq5QKeO2+hvh00xlLsLQcrKCzdVp3qJQKpGcVYtjXf8sus3t6L5y+kYPRC/baXbc9ig3W34D3H2wKDhw61rLuv+0v9sD17EI0FIlLzgRyzx3WHBE6NX55qiOu3C3AMz8dErw+qn11dK9bGRO/32+Z1rVuvGWQhUfaVcfiPZcsr1WO0qFn/cpYuu+yZVrz5Fik2Rkhz5vw3wuxKAkAnWtXwoWMPFzNLJBcdvPUbgCEDrS4cI3Hyve+G98W1ePC0WPOVsF0tVKJ+Cgd1kzqhHCtVbawt11etHWH9jXjMLhFVbz08xGJ9Woxsm11fLr5rMP1rH22C5RKc2nst9sv4PcjwlEWn+lZGx1qVUSrlAoya5CH7Ya6KgryH5/YVck7Px25Or0J/XIHAuXJZcOFgpOERClCBirfIwjvQk4pQgIKOg8OFAqFR4UpuSwkuRDyKjF6jGhbHfUSoxATrkGPepUtoeNsh5dHLGRUjNShdwOza0k8+l7fRkI3k5jkuHA81DoZLatbO6FReg161K9s08Gum2Ad7r5fkyREiMK3K0bq0LVuPFRKBbrVjUdchBY6tQpDW1WzzDO8TTL6NkpEvUTruppVi7HbxuGtk9E8ORbta7pWzlc9LhzJceFoW0O+TDIpRo/EGD26lrq73IUXVga3qIphbZIxvE11JMdZg62T48LRRqJcU+lEZ5vfzy2qV0CvBrYlpfUSotCeKXWsHheO9jWt26oicug0rRZj05Z+jRMdtsNb8N8LKdeYSqlA30bybasZH1k6n3VaXITOZaeOFOrS4zglzjagXFtanta0WixqlzoMAfsB3zHh7pf0cxzQra50OXHTarF4tEOKU+tpWCUa9ROj0aJ6BdzXNMnmda1aiR71KiO6jBlmroqCvOgkLt8D5MVqX0G/3N5i91fA/HuAwizH85YnUYqcUkQoQ04pgvAu4t8NEn8JCEOM/VVWRDiHJ0d5khs1TiwY8dgTLPVOZpHx6yg2mgSX754UQ9m8Jlf2VlyENSSdF9kqhFun6RyEk/PvIaewxIWt2uY3SeGpT50XVlwt05U77uQu6aVEyjCtWiBUKhVAJWa0RLGIGa3X2Ign/jw/8ftOKlxdoXAuJ4wV9ypGaj1SvsevUuo4khO97AV8h5chhJ+D/LGiViqgc2MgDalzQ9kGfSjD6HulTZEqPXZ3hERPQb/c3mLdS8DlPcCuz5yYuRyJUiHhJJE6AZSjz5CQh71CDdrjmyACGLHYS+V7BChTKphwxrHiLAXFMuV7BpPNCHyAfeHI2bIV/vgqNghH3/PkqI96tXvrYkUpvhxRJRotzR78/skqcE2UcgZPlQVZRCkXxR05ocEVASNMoxKsR6lUCPa5WnTuiQ7T2GxXKrvMV/Cig5TDUKlQONU2gSgVobV5z+6gsCNZyoku9sSYMgnEnHxJnFqlcGvdkqJUmcR565nH/dH3bPutcg5TX0GZUt7GKadUkDqG3CEUyvfIKUXIQU4pgvAu4gspEqUIUKZUMOFJp5SU4wMwd66kOt6eODb4dYhFL08ed6xjxJVLTrYUSCojy1GpDy+4ZRe6f15VKqRdGGXh3T9O4sC/d/DjxHaWQGZPiVJhWhXyZMpAxYRrVQI3lEqhQFykVZTSisSMaL3axj2lL4OLp6wUG0x44of9WH/shs1rSoX0MSOG3Y9xEZ5xStmz0cndZLB3LEsJ0q4g75RSuidKSbTVUyKtMy5FFlUAi1L0y+1pLu0BTvxqfe7MBXN5EqUQCk4SEqUICTgONPoeQXgZm0wpEqUIypQKJspWtiIkXy5TymiSLO3TOXAgdSjNUkqK0QMAetS3zZaxlO+JSgR1aiVe6FsPAPDug02daL196pdmQbWxk9MkJilGj7gILRKj9YjUW30HQ1pWBQA83aO23eX59zaxSw0AQIXSbJ7Hu9a0mXdQC/M6O9cWhrR/OKwZAGBy7zoCQYb92LuIgt0d8dVf57Dv4l2sO3rdEtId4aLjSCwO9ahnzrZ6d4j5s5rSp67NMo+2t+YHadVK1E2MEogJBhOH2pUjEa1XQ6kA6iUKw9Ufap1sU37mz/K98xl5koIUYBZJ2GOGZc5DzSyP2d0YpbcfdM66yOyJ0ewrHWsJ88zkRDx762taLQbhWpUgg8pZOHCyJYNqlcIpUf3pHrUEz2vGRzpdHuwMDZLMx1mV0vOUK/CHr5RwXGTwb7+FnFKeZus7wPmt1uf8BbOxxFzOV7U1oBEdROVJlBKUNwVpyRs5pQgpxMdz0IquBBHA2Iy+V45+PwlZWCHKU6NBEd7B1Tv79pALOjeaOGmnlINj48eJ7VBQYoROrcTNnCLJoeUt5XtGk6Bjp1UrMb5zDYxqn4KYsLKFFwPAb890RpHBZBNybg+1SoldL/cEIOy0f/hQM8y4v5HD989/jxpVicHhN+5BtF6N7EKD5PupGhuGIzPuQYQoe2lwi2roWT8BMWEaPNG1Fhq8vg6A8NJ50bi2yC00oM9Hf+FmTpHT76+oxITbucUAhIKHM7BlZ4ffuAdRpfu1d8MEHH7jHsn3OOuBRpjWtx5MJg5KhcImQDun0IBovQa7pvdCQbER8VHWfKmHWlVDclw4zt7KFSwjzvWqHheODVO6ot6r61x6P2IqRmhxO6/Y7jxFJcLfy171K1tGWVQqzOsQ80i76oIAffb7q1UrZMtx103ugjqVo5BXbEC4RoWCEiM6vbvZ4sJLe70Pms/aAEB4bPw4oR1m/XYci3ZdNL+vSOnP2V4XMlyrxr5XekOrVqLOK3/IzyizXnuZUvZ4d0gT9GuSZHMsxUfpsPeV3mg640+X2iJHuFaN47P6uvVbx39e7Oh7dSpH4pvRrW2C+n0N/XJ7GqXox4MXpXZ8BCwaYM6aElMWUYrjguuiPCTKm0iUIqQQlxUF6/FNEAEMOaUICVhRypOZRYTn8UX5nkFOlHLgUlEpFYjUqaFRKSUFKXYdxSJXAT/dE4IUYBaYXBGkePQalY27RKFQIFqvcVqUAszvQ6FQ2H0/UXqNpMjILxMmCGxnyt6UZoHH1U61ieMswkvFSJ2DuYWwGUQxYcJ2y71H/v1XiNBKjuiWW2TO3orUqQWCFACL68imfE/k1qsQ7vhzcQZ7ggJ/ShQHWScwThulQiEp9LFB+fx8PBqVUvYzjCnN0zKHvSsRJRplLpZZL3tsKJUKGJh+rVTgPCBdfsYSUfo9dhUO8uKTo/ysKL1G9liK1ms86pIL17r3/qR+HxskRaNGpQi/39AhUcrTyIlSW942/39gke0yZRGlfhoJfNYaMDh/p8G/hED5Hl3vElLQqGAE4X1IlCIkoHDz4MGToqFc+Z7RZJIUrDyZKWUOOheW7wU6SqXCbgaQN79HUh+7q6N9GTkOd3hRqgxOKU9RWCLffl5oEYuwOlEZl0qpgEJh/3NxBnvB33yYu3hgADZDSqEAKkkIfWLRkS2/VSvlRSm5EjgpxB9NZr7ng/adheM4WTenxoGgzgXBoFdSXwN/i1E8gdGKUEJOlFLYqX12V5TiOOD0H8Cdc+bSwGBAUL4XRA4vARLf6GAtRSQ8B3WWCcL7iB2IJP4SIFEqmHDlo+I4DllMB/VOXjEy882iRFZ+iWz5nsHEIV9iZD5PiFK8+CQu3wuUjp0j7H1XvCmsSXXnS1wUpQqKjbhyNx8ABAHjzuCo9MrT8J1/8XEhHi2NF2/KevzYe3+8Yy2vSPh9CWedbDJOKfF6WcFGo1LIj47nwv4Wz+mMKOXIKeUu9tbqyCllMNpvUyCYeKXE2UDJYaRMKU+jFIlPfMdUpQF4q6+hCFAzarS7XyxDIfMkAI50pwiBIOhAOKsQgYeNKBWsoitBBDAk/hISsB0lT5aHEZ7HlaDz2X+cxDfbzuPb0a1RIz4C93y0DSqFAtP718fMX4/LLmcwSpfv6TwgHFkypQwmwSVtsJSNatVK2dHmvNk5lSpritSpXXLFvPX7CcvjihGule+FuRiMbg+tWulwpDLehSQ+H4kNRPxzsyjlfr/I3nnPIkqJhFq2zDNKpxaIVHLrZZ9qVErZ7aokxKroMI3kyI7ikeicOYd7UpRSKxUwlCrM9lbrSGhzVWT1B1IGtkBxeQZGK0IJsSOKF17YC+f828J53P1iFeU6nifQ4EKhfI++NoQEFHROEN7HRpSi7xlhZnSHFHSvF48WybH+bgphB1eCzr/Zdh4A8PbaEzh1PQdGE4dio8muIAWUZkqJyveSYvQY0DTJ9QaL4N0SBhMnuB8cLFqoXEYP4Hh0Qnf48pGWqJ8YhQ+HNbd57atRrVA/MQp9GyXILm+SGiYM1pEBnaVf4yS0rRFnMzKaO/z0WHvUT4zC4ontbF57dUADNKsWg8dKRywUCxnisjZegJHLInKmrG/JxHY262VHZgvXmD/zXJEgFBOmwduDG6Nx1WhMuacuFAoFJnaugWhmFD6xQCQo33PRKfX1o+bPe8HY1oLp4jnfGNgQDZKi8enIFpLrBqRHjwOARePaCJ6/92ATNEyKxrhOqWiWHCto19LHzZ/j0sfbW6bZd0rZvid+xE0AFmFLDrFLzh9IiedlLR31FOSU8jRS5XtGg1CUKsgEoqtYn7tbxlacY30scE2Vke0fAv/uAkb8BKhds8c6xCZ3hwtC55FUe6l8r9xDnWWC8D70PSNkmPVAY383gXACd51sjpwpLEaTySZv6u/pvdzarhi++SYTJ7gaDBaH3tR76mLK8sOSr3nDKdWvSRL6NZEWAxtXjcG6yV2xfP9lrD92Q3IeuY6+o1IqMVq1Esuf6ODSMnK0SqmAdZO7Sr42sUtNTOxS0/LcRtQRO6VK+0CvDGiAJ344YLM+vVqFEqN9R3DH2pXwzfbzludzhzVDrwYJaDbTPNqbvtQBlV0odKVVjNCiX5MkPNIuxTLt1fsaYnDLqhjwyQ5ze0V9NGH5nlIgckzuXQfzNp4BIJ0p1ahKjOR+E3cDa8ZH4o/nusi/YZhLe8W8/2BTdK9XWTBteJvqGN6muuV5i1l/4m6pO699zYq27bFjFJF6T0/3qI0P1p8CABgcOKV0GiVcGGzSK0iLUv4XywBySnkesShlLAGMoiOwMEu0kAecUsUedE1tmgWc3Qic/M1z65QjGDsUQSeiET6Bgs4JwvtQ+R5BBDWulO+xuCJKGUwcCmVG5isrfKfOJLqn6ooDzJ+EaeTdUP7KlrF3TBgduE8CHXGHX2WTKWV+73LB7Xonyw5ZB5BapRS4X/Sln2t2gfD3Um4EQ1a4sCe2akTbYd+rS5lSbpwTJBMynFiNo23ZO9ocOYpKHGRKBUL2odTHEiiiFDmlPI04U6o413ZkvMJM8/8XtpsdU247pVhRKt+9ddjFCz8EkiVOwXYYBseFB+FrRMc2dZYJwvOQKEUQQY274k2RC3ktRiMnOzJfWeHbL860cVds8zX2spX81Wm2N3JcSZDnc9orfzO/bt7nUiHjgH0RUW47GqVC4OrRla5DPNqh3DbZJtr7bNQqheD9sG3wtkjrbqaUo1bZW61YUIzUCfuvjgRUnZOfpTehoPPyhNgpdeskUFIgnLbuZUCtB34YZH5eqa71NalytoyzwNGVQPsnAX2MdTrrlCrxkChlKLY+1kZ6Zp0CxKJUEP7YSF140Oh7BAWdE4T3Ebtrg9FtSxDlGF85pbwmSpU232jiBD/zwVK+Zy9Tyl/ZMvZC4o0O3CeBjtgxJA4A53VAueB2vcY5wYAVocQOpjCZdci5s9jPQ/zZsMKLRqkUBGe7O8KhO6cEqaPC20ev+PshFqUcCaiBECguKUqRUypEETulCrOA22eF0+5etApSgLAzy5lsw9L/rwdQlA3c/RcY/KV1Opsp5U753o1jgCYMiLPWPqOIWacm3PV1OiIk8kCC48KD8DFiYdLk/IgyBEE4ibgslspkCSKocNdB4ZooZUKhl36CeVGN4wAj87tfIdzDGaxewp7zxp0yKk9gT8yQypRKYkK8Ax1HTqnalc0GgOgw6S756RvW/l2YRmUT4M8LHayjSa1SCD5LuQB7uXB1tsniz0YgSqkUApEjpWKE5Poc4c7IlakVbfuozhy/jmapkyBvyBDvi1apFQTPE6LsH5eB4EiSev9VK4T5viESkCjlacROKQDIOG1/GbEoBdHJoyjb/P+5zaLprCjlolPq5gngy45ARGXghTPMOpm8K2/8OIXCCGVBYtEmfIxYcDUUBmmQP0EEMOLAVyrfI4igwt2b8nKiVLhWhed718Xba09YphlNHIoN1nPDqqc6urdRCRSWTCnO0kEf2zEVDatEe2wb3oQt39OplXi5X33kFxtROUraqeML7AmVBpH7pE7lSHz1aCtvN8lj2GZKCd/rpB51AJiPqw+GNsW8jWcwrlMqjCYOMWEavLzqiGXeyb3r4NSNHJy6noPnetXB9jMZGN0hxWY74m3Kua3k97t8GR4bMK5SmsWv2UOaIKewBH0aJmDaPXWRGOOayOHOVfLELjUx509h/9q59UjPtWZSJ/x84Aqe71NX8nUAUJXu118ndcby/ZcxpXTeBWNbY/f5OxjUoqrdLQeiU6p7vXjc2yjRT60RQqKUp5ESpfLv2F+GLcOzV84mLgNkhShXy/f2fGX+P++msOPMCl1ecTGFQPkejb5HSCFVwmkoAjTBc0ePIAIeY7HwOYlSBBFUuFO+pwBQbJS+JtWolHisa02BKGUwcigsFbFev68hWlavILmsO7DlezzP9qrjsfV7G1aUer5PXYzrVMOPrTFj1yklKt9bP7lr0ITKA1Kj71mft68ZJ/g8HmqdjIdaJwvmZ0UpvUaFucOaW57fw4gJrAvHVpRyLcvIrlOKudblBdqRba2j203q6Zvvgl6jQpOqMThy1WqmKMs94KbVYtG0WqzdeTSl+6JJtRg0qWaN0+lZPwE96yc43IacY82XiAcQfKV/g4D5Pvlfsgs1xOV7AJCfYX+ZvJvWx3ZFKZHwxJYHuVq+l/6P9TF7kV+YzbTFC6KUTYlTEIpS5HwhpJD67hoKbKcRBOE+4rJYEqUIIqhwp0SMg7xTSioHyWDiUFBsPjeEOzl6mbPwogIbGh0sIecAEM4IFLEy5Vu+xl6nWBweHSgdaGexyZQqQ/uzCuRrUtlcIHE4uauiFPsdFbfXhfEG3NqeK4iPBXfKAOWI1tuaTNRlzF4KhPI98T4q63vyJIHTklBB0il12/nl7bmTxBfjRua5eIQ/e3AckMGU7BkKrY8FTilvCEYhUL5HEFKUilJGTgEjV3rSLym0swBBEC4jdkrRIBMEEVS422+UE6XU4lv/EAad2xttzh34DjTr4JFoQsDC7g9P7xt3cTVTKpiw55Rylcx8eVGKLQ3TiA5IV8vG2CaK2+vuqHf2cPecIN6VzqzH2W1FSwi2ZR0IIBDL99wNp/cG/t87oYY4pBywilIJjR0v70o5GytgGVzo/BbcFYaks4IWK0p5xSklzt1xQUwLFKROyNQxInhRCkoUojTwlJxSnmXbB8DPjwWnw5LwDOJMqaAsASeI8oujLlB6VgHe/eMkrmYKfz/Fw9nzaNS2a/z18DXcyTML2PaCvd2B78OVsE6pAOrYOYLtGHt637iLPaeZOFMq2BCLpmVx1WUWFMu+JijfU5fNKcUKF+L2ip1r/sSTzigxUiHwZf2eB4YoJXwudtX5E//vnVDDnlNKGwE8ttn2dRaXRCnWKVV6orp5Alj/CpBnx51VmCl8zgpa7GNvZEqJxZuMU57fhtcJnBMyEUhwpf8yohQ5pTzL5reAI8uBf3f4uyWEv7DJlCK3LUGEEk/8cABf/XUOj367RzC9SK58r7TTX1c0atbJ6+abrOFaz8bn8h1TVpTyZufY07ClUvUSo/zYEivizj4bps060mpUcm90N38idtdoVArERZivEXs5kUOUwowy175GRdn5BOV7IiFMKlNtaKtqDrcNeNcp1TDJPDjA/c2quLW8rVPKidH3nFy3VJukXJmu0LVuvLkNfjxd2DqlAkcKoqBzTyOVKXW9NKROrQNUDka3cEWUEpTvlXZ+v+lhdmfk3gQe/D/p5djcKEDoVmIv+L1yB1p0MvvxQeDVm+Z9E0gcXgoc/gkYuhAIjxO+Rq4oQorS7wsHkFPK27COTkKagkxg6cNAk6FA6/H+bo3nEJexk1OKIIIKRx3Hf66Yg4vPZ+QJpsuW75V2+pc+3gHPLT2E7WeEOa6eLlHjO3UlRuEoZMHE7892xp28YqRUDAyRx1aUsnbcWWfOiv908GWzPIJapcQvT3XE3+dvo33NilCrlPjjuS7Yd/GOU6OerXiiA1anXUVCtB73NZUXb4RB5+adt3lqN5y7lYcOtSri4XbVsWTPJQDm/T3rgUay62KzmsROGpMHnVI/TmyHHWczcE9Dx+KcFO5kUTm7yITONVAhQosXV1ozmMtavje4RVXoNSo0ZULSfY14n5X1PXmSwJHHQgUppxSPSgckNAKaPAQ0HSE9jyuCBxvwygtLfCf40m755QqzhM9Zd5RAlPKBU0qqPYHAL08A57cCf38m8aLUZ0RCVbmntHNsghKFHDmlvAqFWztm+xzg353Ab8/7uyWexSgWpcgpRRDBhLtdIPmgc3NXJi5Ci0HNbYdk93z5nq1TKpiCzgGgUZUYdKkT7+9mWLARpZjHfKZUSsVwVIoMsBvYTtKiegU81b22xbHEC0zOhExXjtbj8a618EDzqnbFT6nR92rGR6JPqeDTIjnW8vqDLavadRCymxE7a4wevDEfF6HF/c2quFxeyGPjlPJAm3jUKiWGtBCeT8oqPisUCvRvkoRqFcIdz+wlxG8hkAR1ckp5GimnFI9aZ5ZoH/zW/PzKXuDOeeE87jqljKJsJjk7HscB398vnMY6pdjHXinfkxqhLIA77lKdX3JKEVJYRCkFisgp5V1IlHJM7k3H8wQjNqIUOaUIojwglymltjPqGOD50ff4y2s2gDvYRoQLNKRKxFTgA+VNkvMQQlhRSup7wE5zJIYpGHlHXN4VQJFSNoKZU0HnLkhX4mNOE0Aj1bmLeJ8F0nsKnJaECvacUmq98Pm4P2znWXAPcOI357bFdsyu7AP2fG19LhW4Dki7kgTle8wFvy/K9wCgJMA67qzoFBYnNYPPmkIEERyfKaVAIUoDEskp5R3EYdeELYF2XvUUlClFEEGNo46jlPhg4jh5p5RSvgMNeEGUEr0BEkvKjr3cIr58TzyiHCGEHUVNal+pmGkaB8eswCklWpUny/fKio0o5YTg5IqpUVzqFkgj1bmL+PMMpPdE33BPw4pS9QYIXxNnE0VK1NDeOQ9snCG/fnYUCnG2xh8vWh8rRB8txwFXD0rfPReU73nbKSVxMivOs53mT1jhLsw2HJBG3yMkYUUpvnwvkF2AwQw5pRwTqqKU+LMv506pzz//HKmpqdDr9WjXrh327t1rd/7MzEw8/fTTSEpKgk6nQ926dbF27VoftZYgpDuObLC1VCfp39v52HVOegAf9k6/lECk97YoFWSle4GIeJ+yl9Rf/nUOAFAS5KPweRtWQNFIjPImEG8duWOYj0Ms9AbS6Hvir563v4ruZFgFGoEsqvtVlNq2bRsGDhyIKlWqQKFQYPXq1XbnX7VqFfr06YP4+HhER0ejQ4cOWL9+vW8a6yxs+V6V5kCde6zPq7YSzit3cOekWx+LxQ5WiLLnFhA7og79CPxfD+CHwbbzygad++gOdKB1nvJuWR+rbIcE9YlTymjwvtBFQppnYcr3rKPvBdixHSqQKOWYUBVExU6pcnweW7ZsGaZMmYI33ngDBw8eRLNmzdC3b1/cvCldullcXIw+ffrg4sWLWLlyJU6dOoX/+7//Q9Wqtjk8BOEtpAwv7NfYmTv3FcKt12aCsiSJZcM9niklek639z0O65Tig+vP3wqwG9gBBis2SH0PWPHBUckWuy7xrCPbVQcAtK0hVUniWx7rUlPwvE2q4za5KsGwXfUQ0KRsjpNAEtr8eirNy8tDs2bN8Pnnnzs1/7Zt29CnTx+sXbsWBw4cQI8ePTBw4EAcOnTIyy11AdYppY0w//EkNXduHexIdOLOF3tBLnZKsYg7w3u/Mf+ffcV2XrbzYmDX70WnVN1+1mmB1nHPY0ZukdoH3u4EFecB85oASx/x3jZybwIfNbbvyiNcQ0qUClVhwN+QKOWYknx/t8A7UNC5hblz5+Kxxx7DuHHj0LBhQ3z11VcIDw/HggULJOdfsGAB7ty5g9WrV6NTp05ITU1Ft27d0KxZMx+3nCjP6NS2IhErQthzcVSO0mH39F74dkxryzS2gy0erl6rUjoVJu0K5JTyPOJdyH+MXDm+6eAqrA4lJTqx4q1WwkklXJdC8jEAtKxeAXv+2wtLJrZzs6Weo2vdeOye3gvHZ/XFgVd7Iz7K80H4B17tY3kcCt909jiRyh7zJ34Vpfr164e33noLgwdLuHckmDdvHl588UW0adMGderUwTvvvIM6derg119/9XJLXYDNctJGQHAIR8sP5SmALUcwiALMWdFIfHEumE/UGdZF284TV8t2GwKnlJtWWY4D/ngZ2P2lxGulHQhtOJDc3vxY3HniOODiTiD/jnvbLytseyQ7PF4efe/CdiDnGnDqd+8JYLs+MQuUOz7yzvrLJdbyvTyuND+uKNuP7QlhSJRyTKjmmVHQOQCz6+nAgQPo3bu3ZZpSqUTv3r3x999/Sy6zZs0adOjQAU8//TQSEhLQuHFjvPPOOzAay6+wR/genUSH2GTHKRUXobU8Tq0YgcQYvaDTzQ5pLg5D12s8380Rl7tQyLnn4cXF3CL6rXcWobtJyill/S5IfQdZ2KXFohRgHj3Q02KvuyTG6BGuVaOikyMzuuoMitBZ+/WB5CpyF/bzDLSctqAefc9kMiEnJwdxcfJ2vaKiIhQVWUWX7GwvdxIFTqlIocChi3JuHexFtrhUQeCUsnOyNpWYXT58OaF42zV7AJpw4M45UaaUB8r3Ms4Ae0oFqdbjRc6v0nUq1YAmzPxY7JQ6+RuwbBQQXx94eo97bSgLRgduMW/fudFFWh8X5wmfe4qiHM+vs7zDOKXuoPT75i9hNRRhv3ckSjkmWJxSBXcBlc58o8IZxA7hchp0npGRAaPRiIQEYTZlQkICTp48KbnM+fPnsXnzZjzyyCNYu3Ytzp49i6eeegolJSV44403JJfx+TUUEfLoJMrphE4pYccvKUaPO3nm6zLe4SEoQWE6xyUiUcresPfuIu6XBlImS6jAi5S3c4vtz0hYcKSXsJlSjkQpe06p8gabqRUKu4IV1lTklPIcc+bMQW5uLoYNGyY7z+zZsxETE2P5S05O9m6j2EwpbaQwxFvqaBYHkgPCzpfYKeWsKAUIxSaxKKWPsYpF7DYMHgg6Z8WsjDPC1/g2K1RmUQyw7TwdXmr+/5b0hbXXcbgPpEQpD36xlUyOVX6G/HxlIVRdFP6kVJTioMRdrpyKUhxn38FZ1nXzkCjlmEAri5aiKBf4pAXwTXfnxX6bTKny6ZRyB5PJhMqVK+Obb75Bq1atMHz4cLzyyiv46quvZJfx+TUUEfLoJTrEwkwp4etJMdaRq3lRSpCPwzwWj9AX5uGQc/G2ASrf8wTi8Hu+bO92HolSzuLIxcMet47K99iuaXk/vNmveyjsCkH5XoA5pQKrNS6wZMkSzJw5E8uXL0flypVl55s+fTqysrIsf5cvX/Zuw1inVFgFx3erpUQpVggx2hGlHHX+WHFFKfph1kdbR5a7e1F6/e5e7LPbvXlc+BovWCmV8k4pf9/5duQWk+w8edA9xToB8qRHmykzweKiCCYknVJe+vwClZXjgDl1zO4Xj8N8x7wlfIUSwZBndu2Q+VjJOAVkX3VuGfEAH+U0U6pSpUpQqVS4ceOGYPqNGzeQmJgouUxSUhLq1q0Llcp6PdCgQQNcv34dxcXSnT+fX0MRIY9OoqSOg7xTKpEVpVS2opS9MqIwD4ecA7bOESrfKzviXchZnFJFtjMTkmgduF4EmVIOSu8cle8FM66+HYWDsshgQ1C+R06psrN06VJMnDgRy5cvF+QpSKHT6RAdHS348yoCUSoWCK9of35NhO00VgwqyhW+5mzQOSDslBSLRq3QxwA1u5kfn90os343L/ZZUYodSRAA+CFdBeV7IoGEfV/+GALWkVPK2+V77GfgLaeUIEeMXCcegbNmSlmdUuVMlDr2i1lkOLba8+tmz4v+Fq6DgWBwEGX+a32c/o9zy5BTCgCg1WrRqlUrbNq0yTLNZDJh06ZN6NChg+QynTp1wtmzZ2FifldPnz6NpKQkaLVayWV8fg1FhDzSQefWx+JMqUpMVoxU+R7r+hjYrIrFWaVVKzGgaZJH2sxiU74XYp12f1ArPhLta1qjWPhyzvxi62/924Mb+7xdwUSP+pXRuGo0HikdHU8MmynlUtB5UCoF8rjzdX2geRW0Sa2ARlViPN8gH8OK6IEmsgVdptRPP/2E8ePHY+nSpRgwYIC/m2MLe7SHVQD6fwCszgc6PiM9f0Ij4NIu4TSBKCXKb/iyI9BpMtBnpmMxgRWlxMKPLsYaNH77rDlAXa31TKYU6+4qFgtObPle6YWG2CnFuiAKM4FwHw87yrbf6aBzT26f+VzzvCVKMfu8JA9QBf+J1u8wTqm7XGkOWHkTpXikHKBlRSBKkZDqkGDoKN0+a31857xzy9hkSpVPUQoApkyZgjFjxqB169Zo27Yt5s2bh7y8PIwbNw4AMHr0aFStWhWzZ88GADz55JP47LPP8Nxzz+GZZ57BmTNn8M477+DZZ5/159sgyhlS4eNsppR45LDYMGukgU6ifI91fYRr1fh7ei+PtVUKsQgVaB27YESpVGDp4x2Q+vLvAKwiJV+O2bN+ZTzSLsVfzQsKdGoVfnumi+zralfK95hDOtScUu7w8YgW/m6Cx3A0SqM/8asolZubi7NnrRelFy5cQFpaGuLi4lC9enVMnz4dV69exffffw/AXLI3ZswYfPzxx2jXrh2uX78OAAgLC0NMTIB0qlkRRh8LRFYGxv0uP3/LR+2LUoUSoaI755lFKf7iXBsJFOfazmewIw7pY8xtU+vN4lX2FSCupnB0P084pcTtspTvqeTL9wozrY/zMnwvShkc5HZJOaWccU/dPmcWKh29H1YYlPpcPQErfBXnm48HomywTqnyHnTuFVGKMqVcIwguJFnRlj3v3T4H/DgE6Pgs0GaCcBmb8r3yK0oNHz4ct27dwuuvv47r16+jefPmWLdunSX8/NKlS1Ayt7mTk5Oxfv16PP/882jatCmqVq2K5557Di+99JK/3gJRDpFySrFfY7HIU4EZfc+SKcV0lB2FNnsa2/I9n24+pFEqzIIUnylVVBpc76jcjHCMK+V7wqBzrzXJL4jzy8obgkEiAuzD9asotX//fvTo0cPyfMqUKQCAMWPGYNGiRUhPT8elS5csr3/zzTcwGAx4+umn8fTTT1um8/MHBKyzSS1thxfQbKS5lG3VY9Zp9pxSLHzHbOgCoCAT+OVx4eu82MNxtsKXPtoshcdWBzJOA5mXzKKURzKl7JQN8m1WqgFtaemi2MXFOqXyMwDUda8dYgoygRVjgCYPAS1Gyc/HOqUk78K74ZS6cwH4tCUQXgl48Zxw/cZiQGPNTPCJKMU6wChfyjPwTilOgTt8+V5Rlvl4VmnsLBiCiDPsPAE5pVwjGO5usud69ibI2hfMWYe/T5EQpcTle+W7lHPSpEmYNGmS5Gtbt261mdahQwfs3r3by60iCHmkRCRhppTw9WjGKcV3olghyJHrw9OIRSkq3/McCoUC4KxHA++U8vVnHIq465QKihtchNMIgs4DTOz1qyjVvXt3ixouhVhokrrACjhcFREUCqDpMOCX/1gvrjmjWUhSKIDCLOnljCXWO8ZKNaCLtJ3HUGQWPdjMKB7eGROTXCpKlYaXOixdcwKBU0osSpWuU6GUz5RiOyqulq9xHLByvHm/PbJSeOWy7QPg/Fbznz1RyuBO0LkD+M9AnBH14xDg6gHg+aPWz4R9/2KHm6dgO/Xiz4hwE6tTKguRMEEBJThzxlKk/GAMIQP7vfCGUwrklHKNILiQFIhSzOMCOw5DXpRSKM1CZTl2ShFEMKKXCB9nM6XE2bts+Z6x9HdGrnzPF4idURR07jmUCsAIczknx3HYc97spiVRquyoXciUYt1EoXZ4l3cNWRHATin6lnua+veZ/6/c0LXlxOIHf+Et55QquGu9iFdpAF2U7Tx/vgrMqQ1sedv2NV1pWCm/HO+qEpSueWD0PbHgwXcg7AadMx1OV4O+s68Bx1YB5zbZjubEhuraw+gg6FzSKeVAqGLFRbb85PwW82d8ZgOzTaZz5i0XE7uPySnlGZhMKROUyFeVfsfKS64Ue0xRppT/CdQrr8xL5hsH71YHjq60Theck+yMHFiUY/5fH1u6XHA5pVJTUzFr1iyBC5wgyhMJ0XqbaWymlLh8L0rPiFImCVGKnFIhA99hNnHA9jMZ+PO4eXRREqXKDvudcVTyyh7S4dqgi5+2S81KEgOMlSME5Xs0+l6IU7EWMPU08PhfZVsPXwLHX4CLyb9tvcus1JhzpcRc3W+eLz3N9jW+dI4vK+KFEDYA222nFFu+J3KOWcr3VIAm3PxYnCnFdjLyXOzQXzskv21nXUcGB24xd5xSbE6WlNDIvmdB+Z6XXEzs9rxVIljesIhS5tNqrrKciVLscettUcroYORRwktuNQ+w+S3g6M+2LmD2MzWIfhNY+OXCKpj/DzKn1OTJk7Fq1SrUrFkTffr0wdKlS1FURMOeE+WHexom2ExjRalovbDcne1AG4ylopTM6Hu+wEaUCjC3QTDD70qTicPGEzcs0ylTquywAoRUrhuLRqXE9H718VyvOkiMsRWRg5n3HmyKQc2r4OcnO/q7KX6BPV3pHRwHvoa+5d4gKsG5PCl78I4lqaBzwNzR5YUkpVraKSWmGxNmyruUlKUKOC8WFWRa5/FI0Llc+Z6doHOTOFPKBVhRSrzvnHUEGR0EnbuTKZV70/qYF6VYx5Rch9sXTilDsfx8hPMwQedAORSl2O89BZ0HAMyVhztCurcokhHB2d8be06pEBCl0tLSsHfvXjRo0ADPPPMMkpKSMGnSJBw8eNDfzSMIr6NUKvDf/vUF0wSnd9H5ihWdpMr3fJ2LIi7fI1HKc7CCHzsymK/D7EMRV92FT3Srhef7eCjTN4CoHK3HvBEt0Cqlgr+b4hfYcuNwLYlShDPwJWRyLpaCTOtFvMpJUSoiHuj1OtDuSaBS6YmGF6WMJeZyPdbF4w2nlGX0PTXjlLJTvpeXAVzeB3zSEjj9p+NtC0Qp0Z14ti32BDeDg/I9dzp4rDjHi2WsG2Dft1bhSuCU8oEoJQ4OJtyjtHPMi1JZinImSgncS14QQQTle8FVsuUX2Lv5gSTcVG4gPd1UPpxSPC1btsQnn3yCa9eu4Y033sC3336LNm3aoHnz5liwYIHdvE2CCHbEh7dQlBK+xrpk+PI9tmPl61wUckp5D35PmjhOOFociVJlRsOoqYGWJUT4Dvb8FUaiFOEUvDAiV75lKBSW70UmAC0eBVqNBer0lV4mPA7oMhXo9661w2JxShmB4hzPdPzslZ9ZyveUQqfUge+A7weZA9fFmVLfPwDcOQcsecjxttMPWx+Ly+SyrzHtKhWoOA745Ulgwb3ArdO27Zfs8Eh0Fhx1IFihju9UsQ6xq/uBpQ+Xbp91SnmrfI8VpagUyiMwmVIAkKUoFYrLjSjlgTw6Z/GGU6qkEDjxm7w7NehgLjoDSsSTOVc6nSlV+vkEuShVUlKC5cuX4/7778fUqVPRunVrfPvtt3jwwQfx3//+F4888oi/m0gQXkN8FmDdUfacUgaTbfmer0UhcYaUWKQi3EfJZEqxIgqV75Udlcp/3xkicGA/+jCJQSf8SWill4USfAdPrnzLUGh9TRNmFpke+Mz8fOV46WXC4mynsZlSYmeRuxf7djOlStepYDKl7lwAfn3W/Pjoz8Kytvw7zgszxhJhuR+b45R1RSgOFOWaw96/6gLcOGKetu9boP/73nFKsevkO1XissUr+8z/+2T0PeZ9mUiU8ggiUSoTvFPqrr9a5Fsclr2WEcH5yAsukg2vAXu/AWp2B0b/z/Pr9zUCp1QAiVJyIjg73SiTsVRSaP194UWpgBLcHHPw4EEsXLgQP/30E5RKJUaPHo2PPvoI9etby5kGDx6MNm3a+LGVBOFdxJdRzopSJomgc1+7PsQaFHXwPQe/b40mE37YbR2ciJxSZYcVU0lILb+wn32gle+RKBWo8BfovCihCRcKVCUFVsFHXLrHiz1iwivaTmMzpTwmStkbfY8t3yt1SrEdkNtnRU4pO0ODixG3vyATyL0FRMYD19KErxXnArk3rIIUAGScKm0P65RydvQ9B7D7pFBGlALMnze7fV9kSlH5nofgM6XMF093UJ6dUt4QpZjvnTfcMfu+Nf9/fqvn1+0P2Fwvfwo3+XfMNwBUjCtXCmfaaHG/KgB9jPlhkDml2rRpgz59+uDLL7/EoEGDoNFobOapUaMGRowY4YfWEYRvaJAkvG4VlO+JvtKs6NQm1XxzVelHp5RCoYBCYW0zjb7nOfiyzN/+SUdWAd0w9SRsRld0mO3vDlE+EDilAmxkxcBqTXlGqRZ25Hj3Cu8S0scKBQq2oyseeU8rM9xluIRTis2UYkPOgTIEnTNOKWOxOUibD34XjL4XZrvs7XMiUcqFDr24/ZvfNP8NX2wVnHiKcs1lgSw3jptdWwKnlETnWtIp5UL5Hv+ZSglO2VeFziWvjb5H5Xsex5IpZeYOV/q9JFHKM3g7UyrIxA3HBIBTKvMy8EkLILExMG4doNHLOzOdcWzyo9FqI60u30BygTnB+fPnkZKSYneeiIgILFy40EctIgjf061uPOYOa4Ypy82RC+xlFR9mPqRlVTzUKhkKhQJbp3XH7vO38WCragDETinfu2hUCgUMHJ9v5fPNhyz8p3rmhrDKwiAOGiNcRqtW4scJ7WAwmRBDolS5RcFmSgVY+R6dSgMFsZAkdkrxpQo8fCi2QkLckXNKSZXvsZlSbLkbYL3Y5zjgtynAr5Ody1sRu5vY8jvB6HsS4tmNo84H3ooRt59n3XRrXhRPUTaQky6clncT+KQ5cHaDbXsF2PlxvH0O+KozsH2ucDrbYedFLymnVE6670ffI6eUZyi9QOXL926bypkoZfClKEWj7zmEvXvvL6cUfz6/dsjqQOM/O7Xod8uZz1Rcsg4EnZh48+ZN7Nmzx2b6nj17sH//fj+0iCB8j0KhwJCW1RCpM1+DsiV7fMj/PQ0T0KGW2eGfWikCI9pWt7g92Lv9/iif86dTK5SRKyszkijlETrXqYTu9Sr7uxmEHwnk8j0SpQIFsUDDCxP8RXhYrPD1vFvm/3WRtgXucqKUVmI6mymVLRJp+I5M9jVg/3zgwEIgbbHsW7Bw87jwOev2ETil9LbLisPJWZQOjH1yohRnAtLThNP4/QcAo9dA4CoQLyt4LvPDyE8/vxW4fgTYNNOcY8UjcEqVilFSglvOdR+Nvse8LxKlPIMlU8p8Wr1lLC1P4AXkUEfglPKGCOLl8r2QIwBG32NLqvP40UVLf9v48jseOccme67iA9A1evONDfHrQcDTTz+Ny5cv20y/evUqnn76aT+0iCD8B3/5yopSvAChsFMWx77mj5HE2KZRPo/nkPvMSZQiCM8gLN8jUYqQQuyUMomcUvpY4et5pYHeWlGeFCAtPsnBZkplXxG+xjulCpigZnGJnJhDi4EMkSuJFaX4zpFSBaglRCkWsbgmvrMuRq5t2VeAWyfNj5OalU67av5foQJqdJVvi7hzLRtyztnOzzrGWBcJL1BJOqVEolRJnnvB6o4IxPI9kwm4vM97Qpy34fhMKfMZ/4wxwTw9+0oIjehmh2Av3ws52JAWPznL2HMy/5hviz5aOC/7mQrysCScs+ow6zxBJlAeP34cLVu2tJneokULHD9+XGIJgghd2NHWePjHzmY1saOK+QrWHUVOKc8htyvF4fcEQbiHUknle4Qj5Mr3+NI3cfmexSklIUrJOaWksGRKGYCsUqGGD0Tn70CzDiRH5XRb3rZtBzsCH9/xUKoBldb+usTB7FIZVCyWdsr8qlVuCMSWZnnw71Ufbb7lJX5f/F18m86cA6cU20Fis6mknFL8/zW7A52eMz/OOC0cfZAzCdfjKQJRlDr4HTC/N7B4qL9b4h6WTCnz8XfLGAFEVTG/dvOEv1rlO3wadE6ilEMCQcRjnVL8zQ3+2NCJRSn2PMScw9nzH+uUUpZeTAXZsaDT6XDjxg2b6enp6VCrKeaTKF/w/SNOonzP2awmjR9CnQTle+SU8hhyrjPKlCIIz0Dle4RjpEQpQ7H1At5e+Z6jdQFAZKL0dgVOqVKhJibZ/D/fqWE7FiWMuCJFlRbWxzHmQErJ8j2FyiwG2XNLiYPZpcr9WPg78RVkQmRTOlk7Qvx7FZeQ8PDvQ9zhkS3fM9nOz44qaJB4XMLc9a/V0/z4xBpbgcwbuVKBmCm1f775/393+rcd7mIp3zOf8IsMJnCVG5hf4516oYxPnVKUKeWQQBDx2BsavChlKd8Ti1LMZ8p2TIwSTilNeNA6pe655x5Mnz4dWVnW39XMzEz897//RZ8+ffzYMoLwPXy5FntlxQed2yvfYxGP5OcLWEePkpxSHkPuI7+3kUwfhiAIl2BPV3pyShGS9HxV+NxUIgwIF4tS/MW+eOQ9QOiUikwAYqoDo/8nvV02U4oXdSJLy44s5XuZ1vmdDR4fMNfaNkH5Hu+UKv0iqHXW18TOKHEwuzoMOP8XcHaTtDjE75NYGVEqtbNVxOPznsR363l4Qc3GYeBAlGLnZ4UogUBVIHxdrQOqdzQ/LsyylmbyeHoEPo4TiWcB4pQK9hthIlGK4wBTbKr5tcx//dQoHyLOlDJ6Wjhi6zu8KLI4cnAGC4HulLLJlGJHn5UR9/mbImq9VZQKslLOOXPm4PLly0hJSUGPHj3Qo0cP1KhRA9evX8eHH37o7+YRhE/hO0hseRZv0nfkQEp7vQ92vtwTFSN1dufzBqwQpVVRV8pTKC0ipfV46N2gMpolx/qpRQQRWrBOKU2AnbsCqzXlmZSOwIsXgBrdzM+NJVYnjUIlLT4B0k4ftsxtxBJg8j9A5frSy7NOKV784EsC+Yt9V5xS/Dq0kVbHVrHE6Hv8dlmnVDzTRoVSotNSDHz/APDjEODICttt8+JZbHXptqV0su5HR04psVuMx6FTSiJA3GQUugD4fciX9Kl1VnEQEO5vwPNOKXEnLlCcUkHmeLCFH33Pelo1xJQei3fLgyjFiJtn/gTeqQIc/N5z62ePD28eK46y7oIFX+0vewgypRyV75VONxkhECDvnLc+tjilwqxB50F23qhatSr++ecfvP/++2jYsCFatWqFjz/+GEeOHEFycrK/m0cQPoV3Q7HjFfAClaMA8dhwLarGOoh18BJs29R+yLQKVaQ+8pSKEtUfBEG4BfsdC7RzFwUYBBLhcda79MYSq2ihCROKFiwRlWynsSJDYlN5PywgzJTis5/4sgpOQpRy5JTiBRRtOOOUksiU4u9ys06p+HrW0i3OZFveUZIPS2dFKqOHd0pVSLVOS+0C3D5rDjiPjLc6pfJvm//nRanGDwJHf7Yux7vFnHVK8fOxDiT+8xNnQvHTjYxTSqEAVDrzNPEIhJ52SolLnwJFlAp2q5RE57g4shp0AHD3oq9b43vY4+pq6dD2a54BWo72zPo5Hzml1L6/6+4VAsEpxZ7L+PMzf5zIDe4hdm6uGAe8cMb8WOCU4sNogsspBQARERF4/PHH/d0MgvA7kk4piyjljxY5h0CU8kOmVagiJUTSyHsE4TmETqnAOsmSKBVosOV0JayTRqakRFzyBpjdVtXamMvV1A5KUSzbY51SpWJQWZxSmnBpp5RN+R7jSuBL5njELqYiRtzihxe3rJdjMqVSzcsWZpmzmsb8au3AiB1nkZXN/9//KVD/PuDXyUDVFsL9It6OFPz7Ym/3/T7NLAqKQ+otYlWpGKQq7QSrS0Up8UhtHndKGew/9xdB5niwgS/f46wXqAWR1REFlI/yPW8LHwJRyovHbEg6pfwk3LDnLnGmlHg/W5xSos+WPdezTilL0HlwdliOHz+OS5cuobhYeFPg/vvv91OLCML3KCQGpuE1iEDOamKbplUHbjuDDf5SvcRoG3xPEETZYc9dgVa+R6JUoMGLIaxTSh1mFS7EhEs4pbThwMSNzm2Pd0oZCq3OHfHIc2zHwmBHlCrIBDIvl7Yh0upKslu+x7wvXbR5uqW8QyRKsRlbbO7SjePAgnuBoixr+59NA46vNjug2Dsv4pIRvkxPGwE0HgLU628WAP9ZZp7Odub+/kJeXODzUNiOYH4G8Mt/gIcWid6HRPkeYBUebZxSpfu/pMC8L6Xcca4QqE6poBel+PI96/FWEFEqtObdMouqUgMThAreFj58JbKESqaUrzK4JDfNATvmAlcPWKfxNw34ttg4pXhhX+SUiqtpfRwCmVLnz5/H4MGDceTIESgUCkuHiy9jMhqD6/0QRFmQzpRyrnzPn5BTyjvw+7XIYP29J0mKIDwHK/YH2rnLrdZcvnwZV65csTzfu3cvJk+ejG+++cZjDSu3KKVEKZ31rrAY8Qh1Lm+vVBxi3VD8OnmxQjAkt0z5HscB76VYhSHZ8j1m9D1AeLc8vKJQZBOX77Hwow8CwIFF1u0CgD7W/B5aj7d1W4mfi/OnNHrzOMT8/r560NyZKsoB1k8H9nwl3R5+X4k7y1f2Al91Fk7j7/bzy6h1wv/FohEvxi1+CJjXFLh1WroNzhKwmVIuXHpwHHBhu9V9EQhIiFKFqijz8QgAmZf80Cgf4nVxwEcii9y5Ntjwp1Pq8l5g0yzhtKJs8+8aLzpVqgvU6mV1+/IOKnFAflSS9XEIZEo999xzqFGjBm7evInw8HAcO3YM27ZtQ+vWrbF161Z/N48gfIolU4o9vQdB+Z6K7dgFWAlMMGNxSjGilImcUgThMQK5fM8tUerhhx/Gli1bAADXr19Hnz59sHfvXrzyyiuYNWuWg6UJu7Dle2ymlNwIaWV1zYhFKTZUnRejWHeUOB+JR9w+bYTr5XvhFYV3z8Wj77HwohTHASd/E74WnWQ7v2WdolI63iklhr8LX5gJrJ0GpP8jv05AGGoupuCO8Lllv5b+rxKJUjx8Z6043/w+L243C1QbZ9hviyNsnFIBMvqeK/fDtrwNfHdf2feFGKPBHM7tTjB5aeeYY0SpIoMRqFA6GmSolvBd2gNseF0oPnsDn2UkBdaPtNv4KoNLitzr0tMLs6znH5UGeHSVeTAOgCnfE52PDA5G3wsyUervv//GrFmzUKlSJSiVSiiVSnTu3BmzZ8/Gs88+6+/mEYRP4W/UCzOl+NcC91zMmrho9D3PwXeYi42sKOWv1hBE6CEcpCGwzl1utebo0aNo27YtAGD58uVo3Lgxdu3ahcWLF2PRokWebF/5w1K+VyzMlBJfqANmASmhsWe2x4tS2khh2Dog7BTIBZ0bRWKVRkaU4jOXLKIUI8SIRakIibwsnpzr5o7WtUPWkfQAc6mHOJuKRSxKRcZLz6dkKluPrACuH5FfJ2DdV850kHi3Gb9f+dwvcYmmRZTKEwUGi0bnc5VQKN/b9oH5/wOLPNuG3V+Yw7m/G+j6spKilAmILRWlgiHs3FjiekbPgnuAnR8D2+d6p008bLs87fxh160IrB9pt/Hn6Hv5d6SnF2Raz5W8K9gyAqxM0Dn721JaSn63RIVtZ0u3EWRB50ajEVFR5hFuK1WqhGvXrgEAUlJScOrUKX82jSB8Dp8pxeYGGYOgfI+cUt6B361FBut5nTKlCMJzCDOlAuvc5VamVElJCXQ6cwd648aNlmDO+vXrIz093XOtK49YyvcMwkwpKeGgRlcPOqUyzf9rI6xCkVHCKSUXdG4QtU/LBp3bKd9j81siKgmDyKXysniMxcDHzYAWo8zPa/YAmo0wl4PYIyxW+FwfKzWXbceULReUgu9QOdNBshl9r9QtJg6l551iJXlALhP2W5zjeBv2ELcxYEQpNy48xBlhZeXwUvP/7ria+KBzRpQqNpiso0G6477yFYZiYP8CYN1L5ly1kT+5vg5xFpqn8aZTil1foHWESgqAWyeBpOautc1fo+9d2g2cXiecxo8sWnDX2hZV6W+PRZTiM6XsODlLj7F5268jk8tBVy2CzinVuHFjHD58GDVq1EC7du3w/vvvQ6vV4ptvvkHNmjUdr4AgQgi+gyS451D6RBVo52IGypTyDrxIWcxmSpEmRRAeQ1i+F1jnLrda06hRI3z11VfYvn07NmzYgHvvvRcAcO3aNVSsaMfdQjhGqnxPrbPN2QCAyISyb4/vEPAX9tpwYdg64KRTSiRsaCKYTCk75XtsRlVYnMgp5UBwy7oMbJ1tfhxfzyxKyTmfeMQilDhjiif7mvWxUuPYnWQsNv9yOtP5s4y+x5fvOXJK5QtFKTbk3R1sOn0BOvqeodj8t2U2cGixzDIevlphM6pMrnZ2+Uwp62lVUL4XiE6pHR8BHzYA3oo3C1IAcGqteeRIX2IscTyypzczktjvhLtOqZIC4NvewMaZnmkTz88TgW+6WwVTZ/GWs4zjgH3zpUuaC+4CC/railIxVa2v8wK+UiRKGWWcUuzvT6kD6zYXDY7/ngVZ0Pmrr74KU+m5ZdasWbhw4QK6dOmCtWvX4pNPPvFz6wjCtyiZTKms/BJcvpNvKdcKYE1KWL6nDqyOXTDD79diypQiCK+jDrASabecUu+99x4GDx6MDz74AGPGjEGzZs0AAGvWrLGU9RFuwpbvsZlSTR4y5+jU7A6cWGOeXql22benFB0C2girOCKVKVWcD0nE5XtKpUz5nkG43RtHra+ptUJRKtwFgTOulnPzid1IcqHGvJAAmDtRJ351uGrOWAKFvR9Ppca8rhKRKCUOOucJLy01LM4Dcm9Yp+dlmDuG7l6xBWrQOZspZTQAn7cpdReVTm88xPxdkFvGEwhGeLwJRCU6v6ylfM9KUYkJiCkN02fLTAMBo0E+k2vf/wED5nhmO84cq1+0N5d2TTlh/o7m3f5/9q47PIqq754t6T2EhBYIvffeEUGkKaKIWBDsBRv6qrz2il0+FUVRxNeKFVGQIghIUWrovYWWQEhvm23fH3dn587Mndme3U3ueZ482Z22d2en3XPPOT/g9D9A61GiokYSdO5nIlWyPS/Pq70/AWe2kr8Rz/mlWQDEzLytnwLdpri/XqCUUvt+AZbOJK+fl5H1NJlPI6ERUHDcQUoJ9wDHvY4eiAG07cUOQr4ACUiBQ4EbZh2WUaNGOV+3atUKBw8eREFBAVJSUpyhzxwcdQXCIW+z29H9pZWS/KCQtu9JlFKh285wgzNTiiKlUuNUqo9zcHB4DCv1zBQRYoS6V60ZNmwY8vPzkZ+fjwULFjin33XXXZg3T6U6GYd7kNj3KNIirh7wn6PA9f8Dxr0LtL4C6Hef758ndAgERCdTmVKM6nuWSnYwtty+B6iQUo7OkWDfG/gw+T/iecc6lH1PTtII6HKDclrDLuxlvUXrUcAE6lhWC+6l8MzP2dqKBKGaoKWSdKTk9j15KXqBlDOXS+2DVpNvodLBzpTa8SXwSkNgxVPS6XTnsvCEQ1lETTu/S7ktf3ZIt38hVcQVuyCRLh2TqnucpBStlLKJ1Swri/zUUDdRng/s/Ep6/tFwVblQraiBp3C1HYsJuHQUqMgHCo6RaZ+NAL67Edj8vrichGTxs2VLopTychtqlUl9AU0mJasUZVBDIJRlpjLgx+nq82lFJw1B9VpVJCozhYEJ4TovnEtaQecVlwAAhfYE0SYbRplSZrMZRqMRe/fulUxPTU3lhBRHnYRYfc+uCLQ2hDDZQwcEh1pYcDhDcELSQef3XebmoDMHB4dL0MrDiBCzHnvVmsrKSphMJqSkECXHqVOnMGfOHBw6dAjp6el+bWCdAz1qLHRynJlDUWRYqddtwE0/SFVF3kKulIpJFtVETvuezFZTxciOkSulAJFgMpWSDsfql8SsHkGh1O9e4KFdwKBHyPumfaXbmPor0PVG6bTmQ8h0AaktgEzZer5Cr/dMlQBgyY6T2ooEIf/IbiP7ViDyBDJKTsIJmVJypRTgm4XPm+p7FhOw7k1S6t1XbP+cBBZv/kBm7aQ60awML2YFRD+SUr/JKl/JqybSOLUZeL8H8P0tVFNUMqWEcH2t7QUC308Ffr1fXQ310QDt9V3lqLkLs4q6UoCJzkhz7LuC4+T/vl/EWYEMOpdkSoXQTZpW12ll7LEgIfH8pCyjfw9ASQrTxwzdXuc5QCmlBAVchOM+ZqkkZKP82uYsImF3klIF9gTRJhtGmVIRERFo2rQprNbwIdI4OAIJgXdijTOEMCclsexFhlhYcDhDyJQyOZRSz47rgMToCK1VODg4PICNYv9DrUiDV0/fV199Nf73v/8BAIqKitC3b1+8/fbbmDBhAj766CO/NrDOQWLfkylpAgE5KSVRSgn2PRnhJISi02AppaJIhSGYSoE1LwF/U3YggZTSG8QgaADodjMw+g3g7r/J+xbDgB5Uxx8gZBxNQvW+0zMrW5KHigM3EQELbFodJGF/AIToozPDAKVSSlBWyTOlAD+TUi6UUpeOAS+nA3+9DHx/q/efCxCVxIUD4vtzO6l2UR01lg3I0SGVLOcvpRSrWpiWkmiTI/vlyEpFW2yS6ntWsUNeXcY+TwKB/UuAUxvJ6x1fKudXVxB7ohbkRKi3cHV80aSUPLNOovapIfteKNnBaLu0p0qsQNj35NdZ+XVIOGaa9geG/EecLiGlZJlSkbHicgeWAF9fJ92mcB+qKnKSkYWglFJhlin11FNP4b///S8KCmqYpObgCEEIdq1Ks/I8DmX1YBRXSgUEYvU9cv8KtepgHBzhDitNSoUY8+/VlXTHjh0YPHgwAODHH39ERkYGTp06hf/97388qNNXSOx7MqVUICAnQmKSKVJKRSnFqrLFUkoJIeJVxcDu72Wfk8Juj14P9L1basfTyXKfIuNIttAti4Ge04Ge09jbUsNVDkvQgAdcL3vj966XcSACFlSaHB3w5kOVC0QlwKkEsVSJnXWBlKIVGsYYUUEgr74HEKuTt5B34hR2mWpSQevkRiBvH/AtZZcspciiNS8D394IvNsZ+O4m9zrzpeekypm8/eJr2jrHyl/K/hq4cDAwdkM626x+e/Jfi5RikYJO+x5NStkc54FjGovQDQRoBRfrwb4oh70eTVKrWbE8havfi7aiyokXyTFFk1J+JiLsfiA6peWjfGuPAHrfeWzZDQCJJx/EeKsVIUAFCKRU455A95uA+u2IzZwmpYT7inCvM8bAeX78wCC9LSayP0sdFuroJFQjAtYwVEoBwAcffID169ejUaNGaNu2LXr06CH54+CoSxCIp3KT8hoVyvY9WikVah27cIZwPJgd9j1O+HFw+BcWipQKNeLfq6DziooKJCQQ1cfKlSsxceJE6PV69OvXD6dOhXDZ83CAYMmrKhaVNREBJKXkwdG0UkqwVtFV4qzVbPseKzfGaVezSlUZg2YCyU3db6M8jDzCMbLe8jLy5ylaXgY8dtR1dT+AKLXchFFnRWWVCXEAkDUIOLFOuoAhghCMlkrS+XbuVwcplX9YXPbRA4QUAsjvYJMFCtOkSHU5UTA16Ow6YNliAvb8IJ0mt++tfBrY8jF7fZ2BEAJntgHr3xSnF+cASx4AukwGmg9W/3w50SNY2syVUpUMK8+p+DTwYV/gSZpQ0ej8F58BFowGukwCLn9WfTkAKDhB/rcaSbJ7Lh5gq6cECKotQAzyZiqlbOT4jU4ihFRlIRAfYIszrUQDCAl46RhQj8plkFcC1Okd+1UHzOlMfpfiM/5pD8seKii1UrJIThE9nYZEKVVD1fe83jYd1F+tnonnCbwlpeSkmDsWXXcgsVo68PvDQIeryOsyh30vPp3cv+7/l7zfQZTVElJKsO/p9eSablbJPoOd/D7CNSGxCVCEsMyUAoAJEyYEuwkcHCEDI6y42bAKGceOAGgFOtQvlLkeiX0vGGHBdjuw4wuiKu88CUhrTaZXFgJ7fgQ6TiRZtMFEeT5pY2kuaU+z/i5XEX5z4RYWaiXrJTi6GsjdDfS4VcwOtdnE79x5kliQqqKATG/YFWg5XLmtczuB/CNAmyuB3YuADleLz4qFp0j13dhUMghPZwGfWE+Kq/SYpvy98/aTQimdriW/Rel5oOME4PAK0p+rKiJFSOSxKeGCCweJurrjNdLjf/tCADqyr2KSpetYTMC2z0nUQEQ00H0qkCCrZJ9/hFQYTm9HftvTW0RXRGZf8lxGuzxopLUBuk4Wz8NO14rHxq7vyEBdRkfSxsh4oP04Mr39Vex2nNoIdL9FvSiXFqqKyedUlZDnMcf+EDKljLAA/35CBhPT2wOdr9PcXE3AK1KqVatWWLx4Ma655hqsWLECjzxC8oAuXLiAxMREvzawzkGwshWeFDs1nlSh8xRyFRatlLJbyYkpKKXiMwgxIFdKFRwnF0Y5IuMIiUF3Gu5YAzTp6Vkb5Rkv/sjSiq/v3nIedCwjYUFpVTXSAKIouP1PYOMcsXqWIZJcBC2VZJ8KpJ9ADNJEQUyKaGuppoLO0zsCF/ZJlVJ7fwKOriJ/53cBt/xM7ugX9pOqhDSpuXwWsO0z8rrndJLvJFeysAipntPIxc1uBf54XMy7orHzS6Jmek5DYSQP+xZIKjlZpVWpjiaEtBQz694gZNnfb7smpYSss+Sm4k1MTSmVf0QM5AbI+RCdBIGUUCilAPJ7VhVpE13+wNrXgbWvKqd/dgXwONVmOTF57aciCd71BuCfD5XElbdgESIfDybh5nf/LVNKyYgJNVIqkEHn3ipvaCLIYvIPKUWT/WqB9cy2yL6Dv5RSVcXKaVZq24JSKl72cCWoZisLxd84inpWiNQipUCUjF9fS14nNQGAsMyUAoDnnvNjZUYOjjBHF+tevBzxObAX6Kh7FfvsWc55oVx9L5K27wUjLPjMNuC3h8jrs9uBm38ir3+8HTi2muT/TV9W8+2i8ffb5FkCALZ8oqzWyoD8N0+I9qqbGnhYzcBXE8lriwkY9iR5ffpfMlADEOLiJofb4p8PxYHcZy5RVYUd+GQY+Z/QkJBH278A7t1Apq15SXxmS24GtLmCvLbbgS/Gk9cVBcCoV6TbXHQT6Z8dXgGc3ebY/krgm+uly7nxu4QkfrgVuHiQVEe/xxH5sm0BsPpFxwJ2Ma9YwKFlwPInxPfll4DRr0mX+aCX+DoqAVj5jGfVsxv3IH2to6uAfYuB6UuBY38Bv9xN5k9ZBCx7jLze0p0cJ1s/A+7/h90Oux3opVFgRg07vwJWUX0fnQ4Y+BCEGgJD9buAP94W52f29bygjp/h1ZX02WefxWOPPYasrCz06dMH/fsT9nvlypXo3r27XxtY55Dagvy/sI+w8IBnZek9hUIplSQGnQOk9Ldg74pzEDl0x6TgOPBed2DDO+K0po4AZZ1OzEUCgMx+nhNSgJIh9gcpFQBEwIKSCtKJNNt1QGZv6YiIIVIkAc2VokpIYNHHvg1AB0ycT94LxE/BcZGUEkY0jq8VtysoqgDyMLL7B1Ih66MBwPInxXkWk0hIAUCfu8h/VyqKhEbAACoEfOunwPo32MvabWwlnQC5fU2NlFKzlwHSKnw2s1IVYqkmHWUhLBsAXm/usP6pdM6Fz0tpJu53tWDyjwZK3wskmTPonK6+5yDNhE566Xn2Nv2BPT9KCalh/6XaSJGYlUXA3h+l63a6Vnyd0pz89xspxbDvXTpK/h9YIlXfaCqlApkpZWW/9gR0W/1lMaVt0SZPlFJyUspf1fcY57apWPzdBMtnnIz0F6zIpVROGU1KCepXNdBZdkmNAQBmYUytpnLaODg4/I5EiGR0gk56/deHsFRKYt8LRu4R/RxOvz7m6Dec2oigwwu1tZyHHNbWzQHkmgYdNUAPNtLPjfRzLZ3RyYo7ESA8I+btEaep/db0fZ2VASo8AwuElHy74Y6LB8n/XKoIUoXK/mfNV1uGxvnd7MFkvRHoc7f0T3imqSwkhBQAnHIQixeoqBL6GBEUVxdlDgcaZ7Zqt1EN8kFEx3e1OgZ1U3SyZ0rW810NwytS6rrrrkNOTg62bduGFStWOKdffvnlePfdd/3WuDoJOoTb5DigEhoG7vPkSqmoRGXOlICUZuQ/nQN04DflcpOpYGVhhBzw3rbEypSqSXR1rwpfBCw4foGc1N9uc7Dq9L4U7HsAuSgJHVeBBOl6A/DUeaCLYxRD2F+CUi2hEZDegbw+vlYkDfIPSRvy8x1ilaztn4tETP4RcZlHDwFRjuqIWh3o9uOBh/cQ61dmP+X8K18HmvSWTvtmsvr25DcA4WIvv+hrkVLnsqXv6fZXFQPvdgBeqgec/Jv63AJi/Vv9AnubuY5MqZTmzk6v09JHw2ZTPlBUOL6TM1NKRLWglBLOHUGR5S0uHQPObFcScRcPAz/dLr4f8jgw7Ang3k3iNKuFnLuvN5Ou21M2ApPqZ1JKnllGQ2fwTikVUPuel8ob+jhk2Zm9AU24eGTfk5NSfrbvdb1RasEWzlfBpi1XSgmqT+GB2xgjHfxgXdObDYLTykOfN46Bm3K741rqiYIsBKDX62EwGFT/ODjqEug8Jp3Mjh/CnJSs+l4w7Ht+GEgJQdAZN12aJCHKGKLXRPqZgR44pwd51e67ntrpJcV96N+d2o68n6QGeS5kbQP9u7AGoeUDmq6ejfQG9jIRccCYN6R/wnMP6/eln9n9FafgCvLrguNzBaWUEfJ84eBfR7y+kjZo0ADdu3fHuXPncOYMYcP79OmDdu3aub2N9evXY/z48WjUqBF0Oh0WL17scp21a9eiR48eiIqKQqtWrbBw4UIvv0GIwhipvLjIH/D9+nkyUioiVsw4kkNQUxxaKk6jiQ4AaHm5NKuJHg2Xj567C7VMqZrCuHeB4U8TLzgDwoj99YZ16KIjIxMnLzmIJJqU0keIyrSS8+I0ukNGK9fk+yutFfEWC53B/b+S/8JoiEr7nCMjQkW7Bp2J+k4eaM9Cw26izHjyl45QYgrp7ZT++JxNQP5R9vYE+x49ogBI87QA7UBwuZebJgDy9knL0sux6T3g1GbyuvAksP4tory6eIDcrJsPJhZJgIxsyG1iRSeV26yUklJSpZRjfeE30yLbXKHsIjBvMPDpcGDXt9J5wugoADTpA1zmUEnVay1Ory4VraTCvHs2AGOoqpiAeP7m7QUW3+d51Tc5tI4vnd59pVQgg879kSlFH4daI6GewOqtfU9GWvrNvucYSWvQmZDVqY6cssoC8gAoZN3J71nCNVt4uKMVtPR8Gjf/xK5M2owoFcshkFKlRK4/f7jy+AlB/PLLL/j555+df4sWLcKTTz6Jhg0b4pNPPgl28zg4ahRacUyGULbvBVsppUZUhDloIjI6IkQJKUA6CKVGONCvfSEl1AhIug3uZg65S16FK+h9whpwl09zpWrXGdjPT3L7JSBmfbnaZiCKNbEgvy44jjshUyoCFu3lgwCvSCmbzYYXX3wRSUlJaNasGZo1a4bk5GS89NJLsHmQ9VFeXo6uXbti7ty5bi1/4sQJjB07Fpdddhmys7Px8MMP44477pCotWolAqmU0uulD/yRceyLW2Q80MhhzSw+Sy6w53eTHCEa8hwVOmTOL0opHWlLTSIihpQ3r88mXCvsZP9NNa5CSz0hm5yVoehAQtq+t9bhYY5NZVdHE9als5vqtSb5UH3vIe9z/iUKI0Gi2Yiq3HTla0CWI3A8VyClhKDgxmJ7AHIhEm509I0zKhHoMVV8H5+u9GcnNyP75vZVUnJq++eE0Nn7k9SyI5BNghpHIHS2fc7eBwBw2dPArb+JJebPZ0vn0xd4Wi6e2BgYxchXWjiWWPn+ryvx6n88hEzPGkSyn1JbkN/JXEFksyufFit/CYoqGtWlhAzb+RUAWaaUWSClHOqkQh+UUof/EJVEB5dK5wnti4wHJn8lHlPGSJFkNpWSvAMBV88l5IL85kqrG7O/Bv5i7ENPQP8+Z3cAq6hMHZ1eWo3RrEFKyZVS/qpwB/hJKUWTUn4aCZM84HrwICP/Dv5qj3D+CqSSoIAyVzjmOX6T2FTpenIlVJSMlKIfhiJigX73k2udcD+h13eQ7xKl1N9vk1yV3Ys8/UY1jquvvlryd9111+GVV17BG2+8gSVLlrjeAAdHLQJNPMmVUqFWGYpGlKT6XjCUUir3xjAHnSkVGxnCBIqEcKLzLlVIKZuKwskdqP3WtBIohM+VGoUrpZr8WUgt0sMJledMlqNI6O+xSCz69/F3/IQaVBTzIill1V4+CPBKx/fUU0/hs88+w2uvvYaBA8mo5YYNG/D888+jqqoKr7zyiostEIwePRqjR492+3PnzZuH5s2b4+23STBX+/btsWHDBrz77rsYNWqU518kHHDj9+KDf6BgjBE7PJFx7ItbVKKo3LGZCZnwJyOwVX6iprYUQ9DdqXbHAk2SxdVnM9Q1AZpgolCOaCRB2pl2VoZSs+8VOxQzQql0NcSni/7jtDbkv0AOnt8lEk6JTYBG3YDtjvX63UsImpN/k6p49VqJ1pnERsrvYzWT/Ux7ih89pDz2BswgyyQ3JWSpQC5l9gEmzANW/JfkFR34Dajflnx2egfgvs2kwsQGh703tSVpf1URIdXyHGRP/XaiTxwgYfGZDntgdDIJipT7pGmFikBK1W8H3PILaeMKKlvJGEOC5umsLQHtHYGRBiMpOHDxILDAESh5fB0JUhS8+d1vJqqQw8sJIfTDdAg3L5udDjp3XPQ9VUoVnCBqNkE5Z7cD/1IB9DmyQEQhy2fwo8oKHlEJQIWJqFwuOhRpt61Qr7gSlSR9v38xcMVL5PX53cQCefmz6so8OYRri90OzJdVy9TppGSLJillV87z16gfPZjibYg6fRz6zb7nJdEVqEwpgfwUcg6FrKjqCtGCG53kWt0qV0rR33PWWTJYAhAi/uw2kbyessi57XI4zg36Ac8Ti2OIoV+/frjrrruC3QwOjhpFhJ4qTy7rANr8OfDgZ0ir7wWBEJCoZ4LfmWTCi04u3f0IaVKKJjzo30KNFFEjq9z6LJXoAkkb3N1Y6J5TfoG79j2hL+CKIKSPYWEdgLhc5BCmhbh9z2Kjqu9Jlg/+dcQrev+LL77Ap59+invvvRddunRBly5dcN9992H+/PkBtdNt3rwZI0aMkEwbNWoUNm/eHLDPDDra1ADZRldnU7PGRSeSUWtBRVF2ATjtCF9LorJF5Eqp+m3F197aEOnqewKhEgyoZG1V2pV2R6eFS0JKRSrLk7Kq2NFoTAXDp7cn/xt0BqADSs44Sp+ClNrtfgsw9ElgmqPiSocJYqf9y4lEzQOQbCpAelEViAGB8DFGs8nQyDhS4aPv3WIZeAEJGcDo18nrohwSiA4QG9yeH8XKEwDQxFFVwlTmOI7sRE0kBP0LoI+f+m3ZJISQuQWIpFS7ceRY0enIawEjnif/j/+l3E6PaeJrOi8HIEGKRTnApvfJ+wZdxGp12d+AvtHboXNK0JWZUjmkvOzen9VvANnfAu91AxbfS96bK4HfHxGJO4AEl9Md+TIZWUBDaOev94tkqFbxBDlhUE5VO/zfVcDRP4GF4+A2hAeDi4eU8xSklMwqSN/IA0W0AP5RStHHYSCCzj0ZXQ1U9T1BbSlcQwTS1FwhEkesa5r8WhItIz6veIko+lqNEAkpgFSxoUGR+E77Ho2aetjzMyorK/Hee++hcePGwW4KB0eNQqqUkiJkK68BiKJypIKSexQO9j359diNTi+tlApt+56KCkpiH1NROnu6X9yx77l7DISAGiagcNe+Jzy7uHpWo/cXHa8SFvY9R9uFfpPjuLusLXEtxRvlA73Bv454RUoVFBQws6PatWuHgoLAlTzPzc1FRoaU2MjIyEBJSQkqK9m5JyaTCSUlJZI/Dhnojr5gk2h9hXQZwW4R57Dg5e4mtiWdQcyvAZTEDR2C3Xyod+2jR91DkJSqgJKUYtv3IpRqMVffZ9SrQO87gZEvEXsZQEiGzD7k9b6fyf/WV5D9dNksIMtRHS6zt1gm1VIp5gkJChe5UgoQSSl5p9FdxKUBsfUA2KVV8ugQboCUHgVIp1uwgLYcLr3o128vJUiMUaJajIZwga8sAgod4eQ0qXTZU6SjPPIlpYoIIMfwrb9JbzJyUgogxJ7dRtQb3W4UbaQC2eeADTrnw5QzUyqxMSFXrSbgs5GkOuK/H0m3b7eTh5MlM8j7fb8Qy93ie4kdEgC63Sweh5vnig8oglKKZZEVSCna9qhFEBsiRAUMaZj4UiAeBEWdOyMrwu/DqjxoqVbPYAC0LQr+vIHKSan1bwL/epjvQ5NSgQg6t1a7b1kMRNC5uVL8/RMdlnLhflFdTpFSDPWn5HiC0r7XagTw+DGihKLRSEZKUdclG/SotMuuyTX1sOcDUlJSkJqa6vxLSUlBQkICFixYgDfffDPYzePgqFHQGeG0Umpij8aIjQxdUiqCanhqnEpxoEAiHOx78vuOG+3Uhb19z8JehlbtyPeLJ2odiX1PpQ3ubss5rRapp9y17wnPLq7se/S+oUUbvtj3apqUEhw6jv3Rp3kqlswYiDsHZrKXDyK8IqW6du2KDz74QDH9gw8+QJcuXXxulD8xe/ZsJCUlOf8yMzNdrxRs1HR1OfpAFD77ellWlEAOCJ1ewZKX2hxIpvapnGRp0guY8h3wYLZSgeEuaNIskPlarqBStaLMrlQUqdr35J0xuXJKjphkYOxbwMAHpeScUKEPIMqpzpPY62d0BNLaSqcJhJbeQDHoMqWUt6QUoJq95URqS4faywHhWOpwlZSUEtRUNJoPVk4zlQFbPyNV5Y6tcbSB+s4ZHYDHj5N9GC9TCF3+HPCfo0DzIdLpLFLq0hFycZ+0kBA9AtkjI0ds0CHGSUo55hkipJU1AaUFb/ULwIsp0hva7CZiJUUAGPCA+FutfoHYJAuOiyoq+fcDlMccIN3PLAjfDdB+YHFHgSO0lw40F2Cp1B5BVAs6d/ez3QW9rbJcYM3LwB//cSNvgEKgg84BD9RhAdhXdOW86GTyWnhIM1eINmMWKWWMlF4/Y+spl4lKUI4+ypVSsutSmVwtFQak1Lvvviv5e++99/D777/j1KlTuOqqq1xvQIa5c+ciKysL0dHR6Nu3L7Zs2aK67MKFC6HT6SR/0dEMxRkHRw0hQoWUap2ewFg6dFBtFe9NQSelQqBqFhOK+7nrduol9r3QJSX9at9zpfCVqOJUCDB3jwEW8RCqx483UCMF5fPlxVfUIPy2eqN0IN8X+548piJQEH5XwcFEtatLk2TEGALoPvASXp3xb7zxBsaOHYs///wT/fv3B0CsdadPn8ayZcv82kAaDRo0QF5enmRaXl4eEhMTERPD7mTNmjULM2fOdL4vKSkJfWLqhq+B76cqq2IFCvQFVTjpjFHkBBNOWKFjm9wUOLUROL6WvE9tIe0IC4HONNq6nxvGhCRTystcKn+AJph0eufFvQDK4HVV+x59wdTpxdByT9HrduDSMeCfj0iOkFbIYVprIN9hnWo7VkqEGSKlvmp/kFJpbcgxwsK9mwmRaYwS/dlCZza5mXQkgmUx63QtsEWmYDEVA0tnSqfJFVXC/pErpeq3U4YyA2IFPhqJjQlZ26ATec8iewDYoXcqpZz2PaHtG94R3wvKkrz9pDMt5G2xEJ0M3LiIVDukkbcX+OFW8T1rn3lDBkvy03wlpRzHFivvx1ylLbfWypQKlH2PhqmEfXywIFFK+YkckW/HWu1epp58X3lCrqlBsHHG1RfPJ2fQOaWiUttfEXHkXBW24Q7qtSbnXclZIKW5Yr1yewzq6yj1s7kKoY5p06b5bVuLFi3CzJkzMW/ePPTt2xdz5szBqFGjcOjQIaSnswuLJCYm4tAh0UobymHSHLUfUqWUiIhgVLTzAKVV4jU1KDazcLDvye+rbtyzw9O+R6uXvLDvubSQeWHfUxtMZE23W+ElHRB60NrP9Hy37XuO/aWPkDlffLDv1VSVYOGYcCqlZOej4nk7+NcRr47CoUOH4vDhw5g7dy4OHiShxBMnTsRdd92Fl19+GYMHM9QMfkD//v0VpNeqVaucxBgLUVFRiIpS2qtCGs2HAI+fqLlqCqwbhU5HVFFChojQsRUUKMWnyf+kTGlHPxCkEa2U8oUs8RVRFPlkjHFWQiuwKzv9VruKfc9M/aYzD2hn+2hBpwOunE0Cp12pXka+CFRcIuqEcXOk8wRSSrh4L3NUuIvyYZRSTSl13QKiWhIQlSCGBgJEZUd/F1Y2TdN+ymlVMktuXLp6xzi+ARnxEC7OySoEdUNK8Zk1GBjyGNC4l/QYUNlHdgBRjuFfE01KDXmM/A5HVgGl58jr7G+AxfdBQfwMmwWUXwTObAOaDQT63w8kMfJmhKB7Aax9JicA+tytXEYOWtWi9SDJlCnrZaN5jmNLTSml9pAFuA469xfUvmNlofqxZLcTYji1BclBovOw/KaUku0Pd214/syUKjgObPlUvPbHUNdgZ9B5ufj91Yo3RMZSpJSb9wm9noTyF58mVj7Zg6AiV0rYfgjj888/R3x8PCZNkqpbf/jhB1RUVODWW29VWVOJd955B3feeSemT58OgBSDWbp0KRYsWIAnn2QUcwAhoRo08PK+w8HhZ9Dckw7idcugDx9SKigIB/ue1v1cBfSvXnfse64sZF7Y99TUV0xSKkSPHy2okW7u2veEAXB37XuGCKXIQA6nfY/xuXR/Xp6dGig47XuCUko+yGlhLx9EeF3HtFGjRnjllVfw008/4aeffsLLL7+MwsJCfPbZZ25vo6ysDNnZ2cjOzgYAnDhxAtnZ2cjJIWG8s2bNwtSpU53L33PPPTh+/Dgef/xxHDx4EB9++CG+//57PPLII6zNhzdqdPRS5eSmc2cEu0b99tJlkppIO+e0LctfoJVSvpAlvqIxZSejiLgCKEkppn0vKglocyV5HZPqPSFFwxUhBQD1WgK3LScKvHgZQSF08qzVhDQQSMj8o963iWW7A4hSiEaUTGEWESNVSqmRAWPflr6XV+OrL7Mr0oiMJcoyAXJLnYD4dKDZIAA6Yo1sMUzZXpVj0QY9Zd+TWWOveg+48TvyvvgMsPRRSM6/yARCSve6nXzPu9cBV74qJaSm/iq+Prxc+uGs0tT0edzpWmDMG8x2S0Crq2xmdeUP62HKKDsmnfY9Rp6fQinlQaaUzUoqvpn8UHFN7aFQfmzR2PIJ8EFPYqMEpA8a/lJKycktd4O8/Zkp9cVVwD9zgd8eJO+FewEgDTpn5bnRoM8Xln1PDcmZQLMB0oIcDuTaZdcIrd8rRDB79mykpSlJufT0dLz66qtub6e6uhrbt2+XFH/R6/UYMWKEZvGXsrIyNGvWDJmZmbj66quxb98+1WUBnsvJEVhEUOQT/dRrDHFSqqI62KRUGFTf88K+FzaZUjVq31Orvqdh31O759cW+57i2HI8R7u07wmZUir2PbXBT0OEdLBW077HqvpH7WOHoEET/sj5ctr3HM9OioB9zwsRBBpek1L+wLZt29C9e3d0705K3M+cORPdu3fHs88+CwA4f/68k6ACgObNm2Pp0qVYtWoVunbtirfffhuffvopRo2qgQp1tRnDnyH/mw2STqdJk6Qm5L+8wy90QB7YAdy1NjBB5HT1vch49eUCDTq0PaU50Pde7O76LEx25cWJad9LySIh5LetAO7/N7BtdRdC+6zVQOEpcbovSo8mvYDRbwIdryH5UWqgO+7CxZwm2dQ6rr1uB27+CWjtOO9/f1g6nxWGTmPoE0CPW4GBD2lnet36GzDrNNBTRbmgQioyM6VoCN/LVCL1lusjgLv+Ip8rJw9ptBhGlJRGN7Ng6PDzRDere8kViSzrHcB+mJFX4BRuhCzyyFzh2r5nsznUcLKbdMUl4I3mwNsaJCSN6gqSFbXza+U8tYflqiL17S2fRf5vnEP+07+lxU82MoV9z1tSyoeHTkEVK4A+NoQHu+oKohoD1M95WrHnJ0XtSbvsHGzU3S/bDSRycnLQvHlzxfRmzZpJnndcIT8/H1arlVn8JTc3l7lO27ZtsWDBAvz666/46quvYLPZMGDAAJw5c0b1c8Iyl5MjbGDQidd1OlPKwBpgCSE8PKINoiP0uGeoxjNOIOHSvhcCpJ5X9j3xdXxUCFvK3Km+Z7OI5ILWc45f7HtypZTKNplB5+FISslV5BbldKZ9T66UUtmOAGF/+Wrfowkgd+x7/qgkLFdK1Vb7nr8wbNgw2DXYwIULFzLX2blzZwBbVQfR+3ZCuMgJJVphIZBP8syoxj3J/3oBvDFLlFJehqX7A3RZ87PbgFt+xvGdZ5Gw/QnFomL1PeoUS3HsO5YFLVhwXkTNQBnVMRmnkW/kDvreRf4+VLfWooT6vLvWkv90KDvLigYQFWGrEcCen9jzXQWt6w1EseQKer22Mk9FkWWnqu9JMqUEyL9Xy8sJ0ZTegeR/uYPYVKDHVGm+Vq/b2cvS5zGrOh8LLFIqNlVqfQTYCqOIGIBWJ2sGnVdJiVsWKfXzncDeH4kNlYaQa1ddRsgbo4uw2XWvARv/j7xOby8N0VZTSi15EHhkL3teTDIhxgTQSil/BVkqlFJuKrAU+Vt+HNWPZtj3yvJIRVZA/V4gIaXcPA5d4ARNStVvL1VBhijS09Oxe/duZGVlSabv2rUL9ep5oCDzAv3795fEHQwYMADt27fHxx9/jJdeeom5TljmcnKEDQwSpZR43TKGeKZU2wYJ2PXcFYgyBknN48q+pw8BlZHifu5a/UFnSgUlQN5duGPfE5YzRsrse3KyLhD2PZVtMkmp4CtkPIZc5WM1k/6Mr/Y9NbW+IUKqjtJSSjE/l/ocd+x7/qiYLK++V5vtexy1DA27KEevBXUUINqc9HpRbZHWhoRWBxp0ppTcQlXT6HYz+d/vfgCA1WbH99ZhisWcSila5aVmawkmnEopM7DSoZjrcDXQbqx/tt9hAvlfj0G2DHiA/LaTvxLDw5tRJJaryoSC1VCO+i6UUv5CgroqUBjhKzcxRh4iY6UWt8w+pDJg6xHKZbXQsKv4uv8M9cII9HnsbvVK+XerdsiNdbIHXaZ9T14RTSNTyqV9z04IKQDYPFc6jyY1tRRNAo78Kb4+uUE6T20Et/g0UHKePY+2sQFSIqraDXm2O5AHd7tLLrmbL+ENaFJKuB4fXSVOS23BXo+2pKdk+aUpO2zUdeXSEb9sM9CYMmUKHnzwQfz111+wWq2wWq1Ys2YNHnroIdxwww1ubyctLQ0Gg4FZ/MXdzKiIiAh0794dR4+q27WjoqKQmJgo+ePg8BeMkkwpanqI2/cABI+QAlxX35Pfq4MBr+x74uu0+BDOA3bHvkcvV9PV9zyy7wWfjPAYckLFuZ99tO+pvTfIlVIamVJM+x61XXfse/6oJKxRfU/RJnr5IMIjpdTEiRM15xcVFfnSFo5QQ9cpwJqXSCckhVJIjX2HdBSveLlm2kGP+ATTvgcA494Buk4GMonayWqz44S9IR7K/AH1zq3Fs1bScbZBB6vNDkNyU6DffYTwcyf/qaZhcFysyi8CBQ77jZB75Q8MeoR0UlsMVc4b8SIw8GEpGRqdRMLYS866Vt+pBSqnuWnn8hV6vVhBkMJWW1ukxJGbU5nJArPVhgiDjP+PrSeSKmqdeFfIogpKNOzKzpMCiKVpxAvkN243zr1tD30cOLEeuHiAvBesd3qjVL2jppSioUVKWSql7dYKRqVVSQBwUawghspCbRVY2UXgwn71bWmRPcfWAN1vUk6nSdPKIqkk219BlhX50vd5+9xTpfoz6FwO+sEmXmodQ6PussqNFOiCBIx8KG+w355F1LOmEv9+xwDipZdewsmTJ3H55ZfDaCSPYDabDVOnTvUoUyoyMhI9e/bE6tWrMWHCBOd2Vq9ejRkzZri1DavVij179mDMmDEefw8ODn9AWn2Ptu+FPikVVLiy7+lCQHPghX2PJqXCRimlVn2PXk5i66uB6ntqRBeLrAoB25bHUOxDhn2P9UygUEqpbEeAEKHgiX1PS6EFuPd86I+KyXKllEv7XvDJSY9IqaQk7cpnSUlJkmByjjBHUmPgod3kQI6ME6e3vZL81RTocDm1amk1BWMUCaJ2wGIjD1EVEamINyQAjmu7FXqYrTYY9AZSJS9UIYwWnHNYYuPSgW43+m/7xkigyyT2PL2enS3Ta7p72x75ArB/sXRa0/7+CZB3F0lNRIXG4Mewff9hLD/bB3fHRkKnI0Kfogoz6ifIRvxiU0RSylvVSEoz4KoPgEN/AG01OpU6HTDoYc+2HZcG3P8P8PFQ4Hy2SOLQ52LJeWDXt8p1FZlSjhsfnUsVkwpUFhAlkCFKuawALVn8hYPi68oirW8DZH8FSSaVK1Kq2UDg1Eby+vwuNilFt3vXd4Gx75XLSKnvbwGmfAe0Ha29nhopte8X4ORGkpPmbVEKmvyTn2tXf6i+XuOewLHV3n2mFu5YDXw3Beh7j/+3HQBERkZi0aJFePnll5GdnY2YmBh07twZzZo1c72yDDNnzsStt96KXr16oU+fPpgzZw7Ky8ud1fimTp2Kxo0bY/Zscg968cUX0a9fP7Rq1QpFRUV48803cerUKdxxxx1+/Y4cHO5CjZQyhnimVNARFvY9jcIlKqCrGoYNKeXKvgdISQaFYqUGq++xiMEQICM8hloul6TiIYPsc0lKydYR8kE9se9pfS7gXqaUlrLOXdhdKKUU9r3gk5MekVKff/55oNrBEapI8fxB2e/QG4AZ28gJJc+6CTKsDo+8QaeD1RgDOK5FNuhhstic2UIhC0HVcnY7+Z/RIXht8RQpWcAN3wDfOUi0bjcDV39Qs5UrJ34MLLgSGDYLGDwTv5Tuge1sDiINeiRGR6C40oziymolKUUr/nyxMvW4hfwFCkIOUPlF8p9+0P14sDidhvwclVffu/EHUt1vwSiilLJSyipPHmKLqVBoNftewXHgz+eB/Y6KhY17kmO9ooCQSCufJgoueUB21iCS2fXL3UCeSqZUNaX8Ovi7/+17Vgtwbody+pIHgf+4sKqxSKmqYuCnO8jrfb8AM/crCUTFdhgZIDT5E0+RUr1u175+DHqYHD/tx2t/pqeo3wZ4YLt/t1kDaN26NVq3djNDTgWTJ0/GxYsX8eyzzyI3NxfdunXD8uXLneHnOTk50FOd+8LCQtx5553Izc1FSkoKevbsiU2bNqFDhzC67nPUKqja90I8UyrocFV9LyTse54HKdO3nJB+fq5R+54fq++xCLAQsG15jJqy7wnHcDDte95a+RTV9+Sh7mFu3+PgCBrcDYCuYVit5GZhMOhgNYpB6DaHUiqUsTOnECklQBYA5B8mE1NqICPMn6AJkKQmNUtIAYTkeCrPaUGzOpRzRr0OybGElCqqYNwYaTKHDoAONQiqmPIL5GmRfiBiEVKAMrdKUBAJ9r2oBPEmaa4EIrWq77lZFreykD391weAU478qOhkoM9dhGiquAQcXgFs/ZTMEyrHCYiIBTI6ktd5e0k7BOmbcIzRdsSTf0vX94d97weq6qNOLxJN5RfcWJkRdH5ivfjAVpEPlJxTzwQ0lQI7vyIFBWi0G0cIRQF0hUxXAxiRccCwJ91oe+3Gtddeiz59+uCJJ6QFMt544w1s3boVP/zwg0fbmzFjhqpdb+3atZL37777Lt5918ciFhwcfoTUpUcrpTgppQmX9r0Q2H9e2PfuGNwcF0qrMKazm/mXwYLEJqZiqQNq0L7nplKKNT0EFDIegxV0Dnhg34uTvpfPF2BxxFUE077nbTSBx9X33HzeDiC4PpaDwwdYKBLCRpFSVujZlddCBBdKq3DNh5uw54LjolTqCHN2Nwg7VECHTScGqe2UEsFidSjnDDrn9f39NYwQ4VKqZHsoPDyqQSDMzu4A3utG1DauIM8ZElRDNCklKPTMlTK5tYeydgFqpJSQiQWQKqNC6HvFJZGIBUg1TRoRsSSbTG8k37nkLCGu3mwJrHvT8X3KoAp6JOzsDuDwSve+h4DiM0R9JUAjVJ8J+cOF1QxcPCidpkYqAqTowfIngQ96SafL7bZ6PdDIUcWw03WetbGOYv369cwMp9GjR2P9+vVBaBEHR/Agrb7Hns7BQC217w1rm46VjwzFwyNqqGCNt6BJA5/tey6UUn617zGeqWqFfc8s/c9ahp4vPIParbJMMDlJ5SCl/GnfU4t3oJ/btAoAuQtF9b3Qt+9xUoqDwwfYKPuencrdskEX0qTUgfOEIKiwywKHEzIYS4cwaKWUpx33AIBWSgm//7rDjM5/37vJf6E6YahCIKUO/g4UnnRvHQUp5SBvBBKHVkpZ5NX3PJf7AwCKTrOn02H4TQeQzDQAKMsD8jUscBExJA9NCM3f9S3w16uEzPrrZaA0T7vin5AZUFkIzL8M+GYSyXJa9yZQ5obSKXeP+DqjkzLU3+JqZFVu37MqSTStdhxcyp7OUvXd+hsw8yDJIKwh0DyuzRb80T1PUFZWhshIpfQ/IiICJSUljDU4OGovpPY98VxWFAfhkEJNPSMgJO17oftM7DH8at9jEEV0UL1q9T0zexn5PMl0bt8DIC3II9mPcqWUEHRu9J99Tw1WlXb4bN8TMqVC377Hr/ocHD5AUEoZ9DrYI0RSyg4d8stMaqsFHVbHyEAlZBfW+BoMCfcH6Apo9OsgQTwe9Lh3mEaVtCGPA1MWAVfPraGWeQlvCgvIR2iry8jNVqhSSCulXJFS7kKoHFl8BnivB7DpfcdnUVazRt3F72MqAXL+Ud+e0L7WDvvapg+kOVFvtyEPP4YoIJXxOwsjYXt/EqctHEMIrb/fcf199v1C/rcbB9z5l7KinZbKCWCQUmZp0DxAiDk5SnOBM9vVOw/NBiqnRcXXuEpRT7FSljAjpTp37oxFixYppn/33Xc824mjzoFX3/MSajlDAoJdfc9mhdJGHvxOr98gsYkFoPoeTSqq2vdUiDH5PBq1RSnlq32PLt6lpdZ3KqUi/WffU4OEiPKnfa+WVt/j4OCQQhilNxp0sFIXuQhYsO9cCXplpQaraZoQbGaVkAUdh5tSig4MD4HcMVopNbpTAzy3ZB/0OsBut0NHyzsiomu2gqW3YJEQriAfQaoul+YvRcaLN3pADEAHyI2bFdrqCoeXE3XRpvcJQbXyaaDbTaLd8MrXgbh60nWEoPSRLwGrnpHOE0ipy54Gtn5GVFFntig/t0kvMoImkGKpLUi4ukBKndygXOeEC4tWzj/A7kUAdECfO4liS97BKL+grUxiBZ3Lw9dZSqmPhwJlucrp0/8AkjKDX/3UAYNOB6ujw2Ox2RAZRuNrzzzzDCZOnIhjx45h+PDhAIDVq1fjm2++wY8//hjk1nFw1CzoPPNoVOMq/Ub8bevMM6VcIdTtezmbldPk7TRXAXt/JDb+TtcCF/YDxhigSU/tbVeVAHt+IN+x8yQpweAtqivINuu1JIVOBBxcSlTGQvXYdmNJDIC79j1TKbDjf9Lph1cA0BHlsSFCWUUakN7z5b/1xcNkX21fKJ1+7C/yHGQzi9Xl5Ag0KVVRQAbjjNHkt4mQuTHMleT5pqJArOYckyJd1mohx0V0MnlONpWS38ZURn6b8ovkmY/Gru9ItWKhWh4gki52Oyl2Y7OI0+j9s3mu+MxXLFPdC0opT+x7eful0zf+nzI+gYXNc0UCqYgq5HPybyD7WxIjITxfW6uJit5UCrQfBxxZJXUzpLWmqu85tmkqIVmhnScR9ZRwDBujyX7b+H+kEE0QB/g5KcXB4QOEUXq9TgddpCgHjUY19p8LXSuGQJ5U2GWkVFJodDrdhk4HPLyXhBHKLU41jHNFlVi6h2RzGfQ6xEWRy6vNDlSarYiNDMPLbVwaudG6kh6Pmk1u+EdWAp2vAxIbkweTXd8AlUXAqU1kOWO0kmSpokmpau997atflGZefX0deUgBgJaXsddJaAj0uk1JSgk3cWMkuUnv+pZY9+RoNlCaTdV/BrB0pvidTjOILLnqSY79S8j/LtcDLYaR1/IRLTmhZKkmSjTBzirPlLJZRGIwsQlQcgYoOiXO3/MjqerHqgoz5D9AswHaba5h6PUAHIdJuCmlxo8fj8WLF+PVV1/Fjz/+iJiYGHTt2hVr1qxBampoDmJwcAQKNCn1RMR3yNAV4YAtE1X6MBi0CSbk90n5YE4w7Xs2G/DVtcrp8jbv+xn49X7y+txOIPtr8vq5Iu2szS0fA2teJq8tJjEOwRdkfw0se4y8nnWWKIDzj4jVnQUc+A2Y9rv79r3sb4Dsr6TTTm0kf1qQ2Pdkqri5vZXLl+cDX07Q3iarfYB/FWwb3hGV6noD0E22/1Y+LRaYkUOoJH1kJSlIAwAPZpPjZPWL2p+75WPyR0P4rrm7pYVjAEdEg4OMWfuq+naFAVS9kaj8BUTFK5cV5uftkU5f9ax22wX89Yr6vMX3qM9rPhQ4sU45PbmZtF0AOd8MkeT5UjiGDVFkP1w6QgZgOSnFwRGeoJUxkRFiZzMKZlworVJbLegQOnISpVRErLSaVrggRNQbd/5PDMs26nWIjTQ4C7aVmSzhSUoBwPVfAN/dBIUUn0a9VkCbK4CBD5L3rUcQknDXN+RGt+gmMl2w0xmM5CZvs0i3a632/AFpynfAtzcAR/+U3nzPbif/jdFiwDkAjJtDHnDqtwMGPMB+uKBH0dpcSUgpFhp1BxIbkZHOlCyRRCo9T0YCS84q1zmfDeTtE6v7yZG7m/xvPkScJn+QpEmpbQuA3x8hD7EjXyTfST7yWVEgPuQ26kZIqYNLxXb8dDu7LQD5bUMMBqrDYrWGFykFAGPHjsXYsWMBACUlJfj222/x2GOPYfv27bBaa5HFhYPDBfQ68fzN0BUBANrrTyM7lAuAhALk90m7VXrdD+b+s1aLipW+9wL/fkRey9tM29Dz9omvbRbtwZuKAvY2fAG9HVMpeS5gbVuotOtu9T36GSAijj3wwwL9+7nKDwPYg2YsMJVSfrznCAOBAHv/7aGqy6a1Jfur8IR0Wfp1xSXpNgVExAEdJ4hEJo0+dzsIKjvZX6x9Y4gg8RlH/1TOs1mBPd+T106lVCTQazoZ+NTpgW43K9drP44o2ITPu3RU+vyk9hwJAFmDpc+p8nYISG4mHVAEREIqOgloO4YQp9Vl4nNiXBow9m1g6aPkvbB/heuFq6ysGkSY9pI4OEIDVipDKFovPlxF66qRU+FlxYQagMWZKUWRUslNQ7sSXIhjH6WMM+h10Ol0iIs0osxkQbnJCiRorBzKaDcWeOww8E4HdcVULEPhwSJ76lH5S8YYoLpUtoBdau1zhfSOQNvRQIPORLbOqg7YaoRU3t9rOvmj8eBOkqf0+WjynpactxwuXfaGb4jyq+Qs0HokuaFnDQaiE0WlkqVKtO4lZSol4d9MBh7aLancCIAwmHl7yeuMTuJ0+X4vdzxsnNlGCCmAPGD8+TzQ/WYlKVVwQlR/Ne5BgutNJcBHA4DJslFcOYKsQHQFszd2zxDA+vXr8dlnn+Gnn35Co0aNMHHiRMydG+IZcxwcfoZOZbDDFgLlyUMa8mu83SYdvAimfY++X414jtxvik9rV4iTB1RrkVJquUq+gBUyrbVtVfuebB2zI0uz121ksGvzB563TcuqmdyUWL0sbg6CC/u5XmtCXJSe9699TyufSY62V5Jntu0npL+/PBSetZ2EDGDCh4TMPJ8tndfvXlE1Za1mR0LoI4iqv7NK1WCdjtgMhf1qiCCk0di31L9PdBIwSkPtVJoLHP+LPe+6BUB8unJ64QngzFbxfdYgIPuUcjmAEFbXzCPLXzpKqbwMQO87SF7orm/E/Sn87vS1gmVLrEGETxADB0cIQiSlgCijAVtspFrXamsPFJZ7GdpcAxAypST2veSmQWpN7YPR4UmIiyIX+3KTl0GFoYL4dO0HFxZxwcp5qN9WfC3PGhAgzz5ioc/dhIga5wgNb6phL0tv73p7qS2Apv3F90nUuRCdSALHAZIr0W4sefCYtFB8cE5rRfaRMUqs8HdstePzOwCTvybnV08HGVZ8Gpg3UPpdy/PJOpWFREVWv504L7OftL3CCNiBJdLpNgvw6Qjy8AMAsWkAdGR0tvAEmdawq3Qd+UihvNhBCJJStGPPGkb2vdzcXLz22mto3bo1Jk2ahMTERJhMJixevBivvfYaevdm2DI4OGox1IbBOCflAgr7nlWq0gmmfY8mEQyR4mAnK+uQtY6ruACJcshPz1as6nha7VC178naIyijDJEeqlDoErN09T3Z7y4MNrlLLAn7S6cX1dP+tO/ZVIhGFvRUTpOEzJIRVKwgeGE9FnkpDzFnKcFcxSgI+4a27/kKLaJYbftykkirHcJ3EtYRjgnhWiCEs9tkpBR9rLEC3GsQnJTi4PABdLW1KKMek6ufQfuqBShAInIKKjQ7TIXl1Xjyp93YfqpAMv1Qbile+n0/CgJIagntOm6nqmbRygwOn2BwKGAqq8nN8IXf9mktHh6gCSU5mEqpROW09A7q8wXLXMk5120Z8wZwzwagqYOsSaCIlHHvSpd1NydNpyPbvGO1MhR94ifAxE/JaJYrCAHkJxwy/9QWRNb98B5g/BxxuQv7gVcbAe92BorPEvWUkMOR1kZK2o16meRXxTj2s0BKXXIErF/5OjDYkYdx6Sjwj0NxY4wm9kJAlJTHpEgJuLM74HwomforycuYOB9ofxX5a9zL9XeuYdAqCkuY2PfGjx+Ptm3bYvfu3ZgzZw7OnTuH999/P9jN4uAIKtSUUhwuIFd/2K1SEiCo9j2BYNCRjrjQKZaTH2qVd10pbGgCxtuKvXKwFD6aSimafNFQSlU7ip7oja6JEAmo80JCwsl+d6MsF9YVhHbr9OLvEjCllIvfxhAh7hPVY6GaTW7JCRgaEWK+L2wW9vdz+Vs4zh/avucrtIhitfbIp2u120nUyYglgWBzhqTLSCl6/wTZvsdJKQ4OH0BnSllsNtihRyXEzuRHa4+qrvvaHwfx3dbTuPYjaZWSUXPW47MNJ/Dfn/eorOk7BDJtnz1LnBgC1evCGXSxIKFyUEkVuZluPVkYjCb5F5O+IH719lcp50UlKafRdjYB7ceLr7tcL742xoh+ernVDXBd3rrrFFKppctkIpNv0kec50nmWIPOpKKeHJFxQJdJJDfKFVIdFkWhIp98ndFvSN8X55BMgLNiJpmCII5JAaYvA8a8Sd4LpFT+EfI/rRUwbJaoxDq+lvzX6UUiy/ldEohlr/ed5H3ubjgfgJsOINeBLtcDk78kf3KLYQiAVlGES9D5H3/8gdtvvx0vvPACxo4dC4MhyNWxODhCAOqkVHic10GDK/teMMu7OwOUBeJAhfyg2yupmuaClAoJ+57KvhYIFUHBJNj35NXbPIGWfc+oojhXg7DvdHrx3h4s+x5NSqna9yzs7Rg0lFLGaErppJJT6uq3kCulPCIUXWxTaCMNNTJIQUppkEbOfSJbRjj/5Ko054MUda3l9j0OjvCFQErp9ToUMTKk5v99QnXdUwXaNqVdZ4p8apsWhHZbYMSr1luADhOAjhMD9nl1AQaKlaqV5azrtwGmfCsllgSoERdXvCy+7nWbqNoBREscQEa2EhyqveIzyu00G0hIJzUkNgQePwFc48gRiKSCyhObsNcJFNLaSN/LSam+dwM3/ySddklGXjdQUS0KhQgqC8nDWsFx8r5eazI6NmyWdHmdTpntFVuPBF+OeVO6TyMTSLXBMIBUKRUemVIbNmxAaWkpevbsib59++KDDz5Afj4jwJWDow5BTdDTJEWlpD0HgSv7nj8tWZ5C6PQKnWOhM65oM0VECIoiwEP7np9IKX/b9wTlt8S+50mHX7A82mWklIp9z1047Xu62mvf0xul2/XKvieznPrbvhchu76pkUG+2PcEqNr37NL/9DaCBE5KcXD4AAullKJJqV7NSA5Lx0YMC5MDUUbtkfJAZqXQ6oL5ltGwT1qonvHD4RYkpJQjU+rbO8UsoHDpPLtEYmPp+xEvqC9LW/R63SadR2cmVRaIhFURQylljFbexOXQ68UHCfpBrabL29Z3QUoBygys3L3S92r5boLyrPQcUVbZzKScr2BRjE+X7nOdXlqRUGcQM6J0OlI9UEAIZkepQUJKhYlSql+/fpg/fz7Onz+Pu+++G9999x0aNWoEm82GVatWobRUHvrPwVH7oVdRRGUk8ucRTSiq79mkJIA/K6p5CoFgEDrQ7tj36Kp0Lu17dKhgAJRSHtv3GCSZk5RyKKX0ER52+AXCQK4ukxE0ntr3hPbpdJR9z4/HSrDte/oI8t2cCiwzWwnmrlJK/nm+gN4m/TxLq9bk8Mq+JyelXNj36GtwkItdcVKKg8MH2JyZUjr0cBBReh1w/3BSBrS4Uv2mFh2hffoFsq9FEyR2O2AOk1yWUAZdpl7IlOqdJXb0BStf2ENuh+s/Q33ZaIogiZQpduS+97g08l+oLEfDGAVkdHS/jfRDCCvbKpCQk3YsgikyVprVdGSF+DqjE9BqJHvbAsFWWQgsGEVe12spPtDodNISxGUXpPs9Lk368HPZU+Sz0jsCAx/U/FqhBPraGC6ZUgLi4uJw2223YcOGDdizZw8effRRvPbaa0hPT8dVVzGssRwctRi1UFNcM3Bp3wviddEb+55a6Dlz+6Fg36OIE5Z9T1BrC+8NRu+sUfJ9ZpFVJ/bJvhfoTClP7HsqQfee2vfk060WthLMXaWUc3l/ZEpRz160mt8dS54ny8rXEZ75FPY9IVMqdJ6hOCnFweEDLBQpdUu/Zpg9sTPWPnYZUmLJhUOrAl90hLZSKpAlkeXqguraouIJIlj2PaNBj4QoQr4UVYRuNUaPICdZtKp1RFIqHTkppbbsuWzlvIgY4OoPgM6TgNtWKOfLQT/E1LQCUCDXAFLJLlJF4XXzT0ry6YqXgXs3Ki13AlgWRnkAPW39M5dLtxVXX7psZm/g5h+B+zYBfe5kf2aIwS67LlpY5Z7DBG3btsUbb7yBM2fO4Ntvvw12czg4ahw86NxLhJV9T7BCadj3JNPDwb6nokqT2/cEeGvfk/+OdPYW4GPQeSjZ92hS0gf7njDdud1q74LO5Uopf2Qtqdn3tLbtF/ueoJSS2fec197QuQZzUoqDwwdYHR0io16HSKMeU/o0RdN6sUh1kFIFGkRElFH79AukfU++bZM5iA8wtQRGg/h70gRVqYncYN9aeajG2xQwXPm6e8vRhBWLaKFH+YQyvoWMHDZjFKmwd+2nYsU9Lfhr9NQb0MRPfLr6cjHJQAdKGdPvfmDAA9rblgfH6wxAn7uk0xr1kL6niUGaMAtTyC+LgbxO1hQMBgMmTJiAJUuWBLspHBw1Cq6U8hKuqu+FpH1PQyklme5iAE+iTAoF+x6j+h4rM8gr+56clPJVKcWqvleL7HvCM6ff7Xt+yJTSqZBSWtuWz/PKvqcWdB56A3qclOLg8AGCc8QgC7ZOjiMnf5XZhspq9gWfVkqxOla2GsqUAgCTJfQuTuEGV0Hny/bk1mRzAotOjlD8ht20l0uirH6shyda5aOmDgJIdT5P4GqELpCgFWFCMLka6AeTDle73jY90takD3DfP0AzWT5V50kiMdZunHS/xme4/owQh1xByq3HHBzhC66U8hK1ovqeCmlhdXH/DlX7nt2utO8J8Jt9z0ellPPZSEdV3/PjOeiRfS8S/rfvRUqnM+17OumzFAsKUipY9r1I7feseXI1lfBd1TKlQsi+5wfqj4Oj7kJQSslJqYQoIwx6Haw2O4orzYiJVF4AaVKqvNqCxGjpxbXUZMHRC2Vole7C9uRDuwVUc1LKZ9BEFH083D6oOT7boF6FMSwRn06q3UW4IIvi0oDbVpIbMCtA8drPgF/uAYY8BlRrVKP01ILnL0m/N6C/p9wuJ0eT3uRBtcUwILOPZ5+T3l4Zqg4QEuo/R4HyfKKs+neeOE8tQD2MIH9+qg1KKQ6Ouoog5+qGL1j2PUnHPgSUUp5U35NMD1P7Hr3PfbXvCTc6hX3Pj5lSQbfvGWvIvif7foYI1xeesLTvCSoxGXHltO/JCEBW0HmQwZVSHBw+wEplStHQ6XRIiCYXiNIqcgH4ffc57D5TRC0jLl9uYl+8H/h2px9bK0IeDsyVUr5Dr6OVUuKlddqALACu7Zphh9hU16QUADTtCzTozJ6X1hq4czXQdrR25pSnD17BtO8BQGpL8r/bFO3lUpoBjx8Dpnzrfu8sw7Evu9+ivVxcGnkIofcrrVwLUyiUUmGcKcXBUdehC6FR+rACq/qeRL0TAqSUy+p73tr3glx9z2ZlV9+jCRlf7XvC7+dvpZQkUypc7HsaSilv7HtuEUzyoHN/2PdUqu9p2vf8UX1PsO85PseplBIsouqbrGlwpRQHhw8QSCmWXSsh2oiiCjNKqsw4lFuKGd8QgunE7DHQ6XQSe15ZlQVIUmwCB86XBKTdSvte+GRK2e127DtXguZpcYiLCp1LmNHAVkolx5IbhMlCrJws1RwHxEwpFjwlpdqOAXJ3Kyvh1RRuWwFcOqK01rEgz4lyhelLgeIz7lcjpAnBYO0PP0KhlOL2PQ6OsIVOx89fr8C079HqnXCovhem9j2rTL0jfC96mnzAzlP7ntNa5e/qe459GwrV9+igc037HuN40MvIJwFyCxvLvucOOViT9j2t40JNCcZcVs2+p5fOV2RKhc41OHR6dBwcYYhqR4dIz1A5EDteJS6UmFAVI170L5aakJ4YLclCqWmlktzyEk72vRX7cnHPVzvQrkEClj88JNjNccJAK6Uogio+ygijXgeLzY6iymrERHqYj1RXoJkp5eGD1+CZRIXU4jLf2uQt4uuTv0AgOskzIqtxT7Ifzu8CGvdwvXwNwm63Q+ehf0eulArn6nscHHUd3L3nJbh9z71l3YUn9j2bmW3fo8kTX+17NoYlEGCQUl5mSul0cJqlgmXfkyilaPuejKBikZfy7CgBTrWQQMAwqu+5RUrJrkyBtO+5o35ytkvDceGxfc8u/R8CqGV+Eg6OmsPCjSew/vBFAEBUhHpm1L1f74DZKl4U31tzBICUGNJSKtHr+gvyjlxlGFXf+2nHWQDAwdzSILdECoNKppROp0OyoxpjYXmQbWWhDC37nqeZUsYooNuNQGJD39pUG6DTATf9ADx2hFguQwRrDuahx0ur8NfBCx6tpySlQueBioODwzPoQrACVFhAYd+zho59T+j0Ou17AvkRiOp7fipqIs8xAlwopRjV9+jKdnKySG7f07lSzNsJWaCovie377l4NpJ/Dq2UCoh9T4VcYoEmpdTILKuZ2q8G6boAg7TRSed7a9+ryep7WtS8woqnsazH9r3Qu/ZyUoqDwwscOF+C53/b73wfwyClcourqOVFAmWdg8iiO1Mms/rFocLk/4cLuVKqQqVCYCjCEKLJqFrV91IcFr6CchcPW3UZmplSXF3mEwwR/nmo8iNuW7gNhRVmTF+41aP15ByUPB+Pg4MjfMDte15Crm5Q2PeC2OF0KqVc2PfUlEge2ff89EwlyTHy0b7Hyo+S2/fkSioW7DbfM6Xkn0MrpQJSfU8lG4oF2r6nlSklHA+ScHAV+558vtXMsO+58SwUCKWUmn1Pyz4n/35av5Wn1fecnxs612BOSnFweIHR//e35D2LlCqpFG9ory8/6HxdWkUusBZKAaVl3yuv9n95e3lHriIAnxEoyEPlQwVqSikAaJBERrPOFVfWaJvCCrH11OdVFddcOzhCGnaulOLgqDUIzbt5GKA22PfUSIuwte9R35uVdUTbqiLdIKVsVuXvKLfEuVJKyT9HEnQeaPueB0opVfueRTxO6O+iZt+Tz7eaGdX33MiHCkSmlJp9T3MdD8gwb6vvhZBiipNSHBx+QEyk8lSqVrHdlVVZYLfb3bbvqVXm8wXyjlw4KaVCVCglU0pJj4fMVHIDOlNQUaNtCisYZTfSOCqTqX6bmm0LR8DBKg7hDuQclJVnSnFwhC10rFF6rbLnHARhZd9Tq76n8mzrUfW9ULDvCflPjm2wQs31MrWyW0opq+vf0WOllLA9nf/te3a7zHrn4rfRGzyz77FymNRIm5C079HV92j1P7fvCeCkFAeHHxDNUEq9P6U7c1mLzY6/j+RL7XuaSin/P1woSKkAWAQDBVaofChASynVJIXcgFbuz1MoPThUoNMDD+8FJn8NtBgOADh6oRTXfrQJfx+5GOTGcfiKWC+rUMozpczcvsfBEbZg3s1d5u1wKKvv2ZUkQLCeNXy277lQ2ISlfS/CS/uei9/Q4K19LwDV9+T7y53fxi37HoOUUiNgBJLbZ/uejB7xt31PombS+I3lBL1P9j1hX8uVUqHzDMVJKQ4OD8EKHmfZ967o2ADtGyYytzF1wRZsPVngfK+dKeV/pZRcXRBOSima8KkMoXbTD9dyUqplfZKXdDC3FIfzymqwVWEMnR5IzgTaj3NmH9z39Q5sP1WIWz7bEuTGcfiK+CjvRh7lpJQ8H4+DgyN8wJVSXsKVfU+YFgz4bN9z8cwbFvY9mfLbEOEf+54cntr36DB2f9v3FMefG7+NW/Y9x/tIhlIqXO177l7jPPlcg0rOljP83bEtuX0vhDKl+JWfg8NDsMKqY1RG/S0alfPOU0HoVRYrLpRUYcmuc4rlygJh33OoC+KjjCgzWbDl5CVUmVswFV+hBlop1fPlVdg863IkxfhhFCOAGN4u3fn6XHEl2jZICGJrQhh3rQM+GUpeM0rfXirjQfG1BbFeklLy56dAVCflCBKKzwBH/ySvU5oDLYYCFw8BOZuBRt1JB6XoNNB2DLH72u3AkVVAWS65XjhVAAYVS4qOPKB7owzQGUgHsMVQ4PhaoGFXoH5boOwicGQFWab1FUBMCnBoGSnd3mIYcHwd6SS0HQOc/gcoOO7FjtFATArZ9qmNZN+0HA6c3wVEJwFZA8ky53cB53YC0clk2ZzNQOEJMk8fAbS6HDj9L1BZKG43OgloO5bs59I84MhKsk8NkUCbK0klz+oK8l2TMoHCk6SzkzUYSGlGOrqHV5DtxKWRzyQ7krS5zZXA6X8RUaZ85oG5HNi+UFxepyOZg61HAcf/AkrOutgpOrLvDRHkeAqETSWxCfmMw8uBinz/b98VimX74ODv5FygsWNhcAi+0/+Q/4K6RLiXn9okvtbpgaIc9vqeVN8zV1LHig+opqIVzu0k27x0lL3svl+kgeN2G1m+8CR5bzAy1Cpe2Peyv3adp+mqMrH8c4R9S1ffO7URfiEmzLIQ9qpi5W9jkf22AoFiKhGXLafOp/zDHtr3ZNX3tn0GNO4lXcQt1ZNMw6lGfnkCVvVA1mdpfa5b9j15ppRBOr/gOLlXCAN8IaSU4qQUB4eHcFcpBQB5JVXM6XI8++s+PPvrPua8Q7mluKxdOn7afgaDWqehSYqbAXkaENQFCdGElNp49BJu/2Irvr6jn8/bDjQMFFdRUW3FxqP5GNO5YfAa5ECkUWxYhEF644gw6DGwVT1sPHoJhbwCnzoadSM3VGs1kNlHMVsfoiH3HJ4jzmullPQ9V0rVIlw4APz2kPh+xjbgi/FAWZ50uas/BLrfBOT8A3wzqWbbKCAyHvjvWWDpI8CB38i0duOA9uOBX+5WLj/oEWDDu4Fpy6CZwIZ3lNMf3gPEpQOfjwGqyzxvx/j3gJ63kt/k8B/i9G43AxPmApveA9bOVq73fDFwdDXw3RT1bTcbBJzagCS1+fRxIEDte7LQqDshxI6vdW95b+BJewKNTe8ppy19tObbQUNQtgj5OXt+IH+u4Il9z1LFPlZ8wbHV5E8N699UTqPbEBEHRMZJ50fGkukC4tJct2P5k66XiU4iBJMa8ar2OTqdSGjtXkT+/I2qYu3fJipBJERMJexlz24XX8emiq8FgkquBIvPkM7PP0z+aLijUpMPirobTO7uNmniKCFDfR3558bVB5KbAUWnlMuq2fcENR39vX+cLh4zDToDZ7dpt72GEBKk1Ny5c/Hmm28iNzcXXbt2xfvvv48+fZQdEgFz5szBRx99hJycHKSlpeG6667D7NmzER3tgjHm4PADWJ0gNYVRSZX3KqdHRrTBu38extI95wEAb686jIZJ0dg863KvtylAyJRKjI5wKrY2Hr3k83ZrAvJMKTWVWk0jLpJcTjs1TkRCtHJURVBzzfx+F0Z2yGAuwwHg7r/JCOGgRxSzDCGaJ8bhOeKo89Zut0Pn5m8rt+/x6nu1CHFpRJ1zaCl5X3peSUgJ0+n/amg7Bs5R6JxNohKoaX8gJlV1NQWOrFBaiqrLSIeqNJdqV656m85lk/+R8UDzoe5/thZydwPFp4myg4Wyi6QjXE1Zxul2pLWWrpvYhCjA8vYQFYvw3YTvlNAIKD1HlGkAUHBCvW2s/SCsDwCnNrj8epLlAaL4AojSqukA9jpVxWTbZReJigYAMvtpV3f1FKc2kM/J20vex2co1Rg1gYQGQNYgYO9P5Le4QA1sth1b8+2hERED9L6DvO5/PxloElQypeekx924OUTlJZz3rpQbggrS0/PYFWJTCSFmKhWnxaSQznt1KZDQECg5J7YvszchAE5tFpfX6YDutwDN+gM9p5HjsEFnQiTodMDlzxEF1vCnyfY2f8Bui/z36zgBOLuDnJfVZeTcbD8OyOgEXPk6IV+jk4B2Y4F1r5F2p7UBhjxOyJ/8Iw5FlNBOPTDgAfLd5OolX9F6JFG9XjjAnn9+F9DnDiC1BdmXQx4H8mSD8gLZJlyzmw0AWl5GpkXEAu2vcnzWFUDP6UQR2aS3kR4AFwAAfY1JREFU+NzY925CtOTukW63/VVAv3tdfweaQGo+BEhv73odV5DY9yKA6xYAB34HBj2svk6ry4FetxOlaufriBJ26mJgyYNk/9IqTTVLo0B61W9H7j0n1jmu7Y7j+Kr3iaKsx60+fkHfEXRSatGiRZg5cybmzZuHvn37Ys6cORg1ahQOHTqE9PR0xfLffPMNnnzySSxYsAADBgzA4cOHMW3aNOh0OrzzToiMWHDUarBIqSijnrEk0CYjHofzymDU63D/Za3wf6uPuP05XTLJGGJ+mclp6ztfXIV1hy9iaJv6Wqu6hMWRKZUQHfRLgMeQq2VChagQOsu39s9izk+kSKg1By/g6m6Na6JZ4Yf0dsAVLzFnybO6OMIXNJFvsdkV6kI1KEgpHnRee9CoOzDlG+CjQYQYkdtBBAgEkStFxQ3fiHaHheOAk3+T1yNfZCoxVfF6ltTa5myHrOKZzazeJoEgSW5KvqM/sORBYMcX4rblsFYrrVDOdjQjSp/vbxHntRpOOijL/gNs+URZhazlcCD7K/cCjFnLCOu7C/nyFhP5n9FJfR+e3wV8PERajW7E84Qk8Bc+HQmc2SJuP7MvMPlL/23fU3SaCGz9DFg6k7zvey8w+rXgtUeOhl2BSQvF9wd+AxbdTF53uhboNZ38/fYwsP1zuLSSCQqPIY8BrUYEoMEeYsAD7Onj/085bfBM8fWoVwj5IVe5pTRnH99drmd/Tt+7yJ+A9uPY7Xi1CSHXAPK5jbpLf5dgQKcDhj/l/vLXLZC+j0oAxs9RLpfeHrj+f8B7VMGpvvcAo193v10CrnjZP2W/JfY9Izn2O12rvU5kHDBOxm2ktgCm/U4seHM6idMFhRStwuo5nfp8HXm2/niI9PocnwGMfduz7xIgBD3o/J133sGdd96J6dOno0OHDpg3bx5iY2OxYMEC5vKbNm3CwIEDceONNyIrKwtXXHEFpkyZgi1bePAtR81A3ikCoDrK/+FNPTGlTyb+emwYHhnZBncObu7259SPJ1U1CivMKKwQH3RvXbAFF0rdswWqQahYlRjiWUwsyEmoKnNohJ0LXKVadUC6wqJWtUUOdeiDfsfiCARY11Q1yBe12Pi5VOsgZK+YK9jzXQURC6CvxfTosT/yQYTPlwT0WjRIqXL/fjYgdj6EbbPaJ99HdDsUYcyR0v/y/SzYP1yVegfYYdXu2Ga0lhdyfLTCf51l7m1iO/0RUkxDUDwI29eHgFqb/o7+PMYCAbqtdL6PcL66ygATrvm1oUoj69hh5Gn6BXSmVYgM5gYU8uwoT44Xev/4o/KefJv+uCYpKjwK9j2Ne50wTyD45e0KMoL6iF9dXY3t27djxAiR6dbr9RgxYgQ2b97MXGfAgAHYvn27k4Q6fvw4li1bhjFjxjCXN5lMKCkpkfxxcPgCT3J1W6XHY/bELshMJQ9X/x3THntfGIWJPVyrZOonRDk+z478MpNkXn6pb3Jbk4PISY3z88OaA6cLKjBg9mp8tPaY37dtl42iVYYMKUXapUaclFSKnYMLbmaNcUgRKqo4Dt9B/5Ke8ErcvlcHIDw4q5JSjvufK6UUa5vy177AKiN9rNXqRJkQpOyvzwbEDke1Bnkn30fCsgZZ8DLdNrqcOv1fyDdRVG9S+Ww5PM1lkVcWEzpSWqSL0Mmnq9G5UwLeEwidW2clsxAgRwJBugYKdOaN5LcRSCk3lVKBIm9qEqzvECiSU3LtqQPPUp6EhMshyX/y0/kkt+/5ClaFR0B6Tsk/xzngQPUhQ+i5OqhndH5+PqxWKzIypCFfGRkZyM3NZa5z44034sUXX8SgQYMQERGBli1bYtiwYfjvf//LXH727NlISkpy/mVmZvr9e3DULchH5j0hdnQ6HeKjjLhnaEuXy8ZGGhCrkpdUaWaPVF4sNeHqDzbgm39Vqpo4UGUm3yFQpNSzv+7FueIqvL78oN+3LbfrzPp5D47klaos7RsKyqvx6d/HFaQgC3YXSqmm9cQH8kN5ZcxlOLTBg85rJzxRSsk5KK0KpxxhCuHBuVpNAeSmfU+yzQB02q1mz+17/iQMhM69qn2P0R5hWX0Eo8MiKykut+8JpJLwnbWUaiz7nieklJ6h5BKUUloV5YTOJG3f8ycRCIidOLqSWbCh1+iIhhok5yL1Gzv3o5uZUqGgUPMVLEIzUCQnc1/XYsivH54cL4EgpeT2PV+hqPDIsO8p1FSOZejqkSF0LIROS9zE2rVr8eqrr+LDDz/Ejh078PPPP2Pp0qV46SV2BsmsWbNQXFzs/Dt9+nQNt9g9FFeYMfP7bGw8GoTSshweQT6q/8ktPT3ehjtZTka9HhXVbBWQ2vS3Vx7CrjPF+O8ve5jzBVRZAquUOnIhcKSLWUZKVVRbMWHuRpWlSYhytZd2uRnf7MDLSw/g7i+3u1xW6FirWTkfurw1GiaRkd9le87jkhtEF4cUXClVO2H1iJTiSqlaD4MbZAvg2r4n2Sa37yna4at9T+272u1K+57OABg9eN4wRCr3lVMpxe17CtQp+57j+TcUFGq+Imj2vbDr/nsOOWnjkX2P2j9+s++pVN/zFgrCSVC6umHfo8+xEDoWgtqStLQ0GAwG5OVJq6vk5eWhQYMGzHWeeeYZ3HLLLbjjjjvQuXNnXHPNNXj11Vcxe/Zs2BgegKioKCQmJkr+QhHv/nkYP+84i5s+/TfYTeFwAXkHymjw/DSKd6Mculaoc7mJTUrlumkLqwqwfe9MoUpnwg9gZciUq5B0APDwomz0eGmVW2onOTYdIxUJt59ihNzK4LTvqfxsybGR2DzrctRPiILVZkdOgYrlgkMVPOi8dsLmAbFkl11/WYUnOMIc3L7nHny276l0WNy177Fyo9Q+18BQZmnBYFR2Kp2ZUlr2PcfzmM3G7XuhCtVOM7fvAQhceKaEAKwF+84VFPY9T76zSh6hLwi4fU8YVHDDvkcjhI6FoLYkMjISPXv2xOrVq53TbDYbVq9ejf792ZUyKioqoJedsAYD+aHlD6vhhNxinjETLpB3goxedJTjIt1RSukwfyq7zLBg37NYbbBYbc5jv1KDnKHhtO/FSi9Q5iBbYRbvPIu3Vx7SPJc9rbb1a/Y5lJks+Gn7GV+bpwlXQecCBLXUpTI/l+GtA3C1bznCE57wSvJl5cpJjlqAsLHvyarb2cxs25owz5+fDYikjRoRxiKHhGX1EUrSx1P7ntp3ZVX9YymftMC075nEeWqoEfue4zOc9r0QuC8FgnQNFOhOs4FFlNQl+x7j2AmYUorb99wGrSQKWfueARLyTLieaymlmJ8bAtcvB4J+VM6cORPz58/HF198gQMHDuDee+9FeXk5pk8nZQynTp2KWbNmOZcfP348PvroI3z33Xc4ceIEVq1ahWeeeQbjx493klPhCLXsII7Qg5yU8ka9wcrG6dw4SbHMyA4ZGN4uXbFsRbUVP24/g1ZP/YFWT/2BO/+3DYD7leicSql46UX77ZWH3Vo/UHh4UTbeX3MUW0+qK5PUiLMNR/Jhsqh//0B3Xe0ulFIC0hxVFb1RboU61h2+iLl/HcWag3koqfKgw+gmvBlA/GT9Mcz5M7jHNYcS9PnoidpJbt+z8up7tQ+BsO/52zoBEHJMYd9zUZkuEPY9NbDse3Q7AmXfs5mVKiq90bPvzrTvuaGUcqqYrGIb6oR9L4yUUn6rvhf0LqzvqEkxRZ2rvicjbTxRNNLXr1Ctvgewz3stKy/ru4TQeRT0lkyePBlvvfUWnn32WXTr1g3Z2dlYvny5M/w8JycH58+fdy7/9NNP49FHH8XTTz+NDh064Pbbb8eoUaPw8ccfB+sr+AXRnJQKG/iDlGLhxr5NkRyrvGBk1YtzvhbIy6d+2YvHftjlnP7ngQsApJXo3lh+kBkAbrfbYbKwlVLz1h3DlXPWY8Y3O4KqPCwoVydshAyZjMQoyfSbP/sXs5f5P1jdXQiHhVqmlIA0BxG4+2xxoJukiXKTBYu25viVHLt1wRa8ueIQblu4Dbd9vtVv2wUIGbn3rGfVU81WG15ddhBz/jzC1aghBppc8uRaI1/UU+VkbcPcuXORlZWF6Oho9O3b11mZ2BW+++476HQ6TJgwIbAN9AaBsO/pGCPKvsIT+57zswNg31MDSylFr+vSvifbzxGOZxGX9j1GtpY/7HtWD6rvWan7GrfvhRa4fU+EjTWQGiDCqK7Z9wAVJZ4bsFO/S6ja9wAZAcXte37BjBkzcOrUKZhMJvz777/o27evc97atWuxcOFC53uj0YjnnnsOR48eRWVlJXJycjB37lwkJyfXfMP9iJiIELipcbgF+Ui9t6TUl7f3kaigYiIMzHvxQyNaY0znBpg/tReu7tZYdXtWm11CSn249hhGvrteUZ3KRIV+pzAypQ7mluL33eeR7yd7mbsdTneraAlKKVYu18JNJ91ul78hZkppHw/1HEqpb/7NQbnJxah6APHS7/vxxE97MO3zLbhUZsKirf5tzzY3crg8wby1xzxehyaQtVR0HDUPmtv3JOhcPihQl4POFy1ahJkzZ+K5557Djh070LVrV4waNQoXLlzQXO/kyZN47LHHMHjw4BpqqYdwZiW5su95cI+ij7Fg2Pf8/dmAa3JNi5Ri2vcipP9tFqJKETpoETGO7bqw77H2gz/se/Q8NTAzeupC9b0wte/pWeqdOmTfY6rCAnRP85agCWdIzgsPvjOtwA7E+eQ3ootxLklIX5VrPI0QOhZCpyV1HLR9z5PQV46ahz8ypQBgcOv6mDmyjfN9dISemQmVFBOBD2/qiZEdMjRtnuXVFqZq4NhF6YM9bfHT2p67+VSuoBVCLvk8hvXwhd/24c0VUvWTQEolRLu+qNek2kvMlNJerlezFOfr/20+FcAWaWPJrnMAgL1nS3D7F9vwxE978Oyv+4LWHldYnH3W43Xqcgh2RbUFty7Ygm+35AS7KUzQ9znPMqXkpFTdte+98847uPPOOzF9+nR06NAB8+bNQ2xsLBYsWKC6jtVqxU033YQXXngBLVq0qMHWegDhwdmlfc9LEt1f1glrtbRTabcBlhokpfxu35PZP+RKMHfte9bqwNj35O1kgaVa4va90AJL3QHUzep79hocLJOQUnXAvgfIvrO39j0/USW0Ks5fal2X9j3ZtU+nU+6HEDoWOCkVIoimlFLl1cFTT3C4hj/te3GU2ic6woBqF2ohrYpt76w8jLIq5bEjt2cJIecGvQ4RGpUD6UygCyVVuOyttfhw7VHN9gHK/TPjmx0u1wGkpJTVBny/9TQ+33gSc/86Jql+JxBvQjaTFmhVWKBhd1MpdXn7DGfbX19+0C/qpK//PYVHFmW7rTYDpJ377NNFAIDfHESVNwjFQhN1WUWzYl8u1h2+iFk/7/EbwexP0MefJwMxCqVUHbXvVVdXY/v27RgxYoRzml6vx4gRI7B582bV9V588UWkp6fj9ttvd+tzTCYTSkpKJH8BR8Dte37qzLLap9Zm52eHmX2PXr/G7XtekFKs39bfJA237/kGiXqF7pxz+x5BDdj3QijcOqDwVh3m7YCHFiRqXX9lSrHsey5UkwqiKnTOo9BpSR0HrbYpC6Klh8M1lEop708jWqkU7YaFs4ssDJ3Gwk0nUco4di6Vkwd4gWQSlFKuLKMlleJD5bt/HsaJ/HK8sfwQ5q8/LlmusLwaB86XOEmJg7nSTsvaQxc1P0cA3XG+UFqFx3/a7Xy/8Wi+87XZsf+bpMQwt0MHoZvMNUdKCZ1sdwYd6HDmvw5pW23cwVO/7MUvO8/ij725bq/D5AF8eE6p6Spo7pBgdVl1GktV+NyR418rpT9AX0d9CTqvq8Rjfn4+rFarM39TQEZGBnJz2deBDRs24LPPPsP8+fPd/pzZs2cjKSnJ+ZeZmelTu91CoO17/kK1F6SUX+17PpBS7tr36H1c4/Y9leU17Xuym5jO4H8lAK++5xvUArfd3Y+1yr7HIqW4fc9vkJwXnlTfC8BAXiByqnSMnCq9SnVLtWmhcP1yoI4claEPOlMjmDkzHK4hzz/xRdlJk1LuXBZuH9wcz4/vgP4t6rn9GZ/9fRw3f/ovujy/koSfXygDQOyCALD2sWH48vY+uP+ylpL1iilS6lyRGBL9yrIDztcmixUj3lmH0f/3NxZtPY2DuSUY+94GRRs2H7uEvWeLsf8cIayKK8yKSoG0UupsodS2caaQPOiv2p+H4472q5FSFRS5RecIBdrKJfBMrpRSgJQQmvHNTqdSyVd4RGj7eXfUdGaTO2SExUuLWG0ATdrRBHOogL6MyokmLcjFgHWVlPIUpaWluOWWWzB//nykpaW5vd6sWbNQXFzs/Dt9+nQAW+lAoO17nkLt+GS1j0VU0fCrUsqFBcSlfU+ulJJlklhlVfQEUspuIzc8T+17HimlNEgpT+x7/rbuAWLn1hZK9j0XHdFQgtpxIBAldan6Xk3az+siKeW1UioAz7MS+14ArhnCNUCr+h4gy3ELrePAzyUpOLwFPaJ/9EIZWqUnBLE1HFqQqy98U0qJp6A7favYSCOmDWyO4/nl2Hz8klufseuMWOXtQyosOspILopZaXHISovDcVn21NELZSipMiMxOgKXZNXwLpRWIT0hGgXl1U4l1q/Z53ChlF3Jbcr8f5yvNz05HANeW4MmKTHY8MRw53RaKfXbbqmN7PttZ5CeEI0P/hLtg5mpsczPqqi2ICmGXIhp+56vVr4tJwrQp3mq6nx3g84B4M3ruuCuL7c7389clI3Vjw7FxVIT9pwtxvB26S6r+AmgyTZP8s1YRIAvhSSrAqxKk+8Ps9WmaT8F5GqcupU9RJM17ua61SRoct8zUsoue1+3flcBaWlpMBgMyMvLk0zPy8tDgwYNFMsfO3YMJ0+exPjx453TbI59ZzQacejQIbRs2VKxXlRUFKKiXFul/YpA2PcCAW/sezWZKUUrpXR6aWefmSkVKf1P2/fkyiWbWZ0UVLPveZop5Q/7XiAIGnlHLiTsey46oqEE1eO2Dtr3mAQcr77nN3idKRUIpVSAn1VY9j1XGXshdhyEVmvqMOjR378Oumd34ggO5EopXzKl6HVT4iKQEE1Iqm6ZyZrrtUqP9/ozBdSLlz4YNEyKlrx/e9VhDJi9Bu+sPIS9Z6WWvD6vrEa1xYZyk3jh/vfEJZzIV7FbUFh9gHSizhRWStQcNCmVV6Ikt2hCCgC6N00Wt/noUOe+U1NK+arkuf7jzZqWMWGWO4fDFR2lncbj+eU4kV+OGz75B7d/sc0ZQi6H2WqTWCUBaQad0eAbKaXz4WFIrnwLNMwWd5RSNuq1+8THWysO4eN1nlf78xTHL5bh639PeZQF5i5o8qYiBHMKJZlSPgSd17RtNFQQGRmJnj17YvXq1c5pNpsNq1evRv/+/RXLt2vXDnv27EF2drbz76qrrsJll12G7OzsmrHluQuDC1LKmWnkZaaUp1BbV2gfXSkulOx7tFJKyIOi11WUCxfse47BMptFXF9OEmlZA1kKLY/te0Z1NYEn1ff8FSgs+Qx5UHAIdKXCyr6nppSqi9X3eNB5QOFt9b1A2/f8BfpnZNn3WL+z5PwLreOAK6VCBDTRcarAdceeI3jwV/U9Ae9N6Y7c4kq0a5CIH+7pjwUbTuDhEW0013HXvtc2IwGH8kqZ89o3SJS8b5SstMOVmSx4bw073Hz7qUJFx/KXna4rpNE2vUqz1akWY1Xf00J6QjS+ur0v8kqq0LJ+POIijSitsqDCQZTlFlfh993nncubzDbsPVuM1hnxiDTo3VYi0TBZbKrZX2KmlHfHw9aTBTjuIPUe+i4bZworce/QltBTx9cTP+7GzzvP4rnxHTB9YHMAQCkVbu8Jt8EiAirNVpwtqkRjxrHgCjVt3zNZrQC0H75pEY279s0D50ucBOidg1tI9r+/MfztdQDIsXnboOZ+3bZNYgn3329jtdlxqdyE9IRo1wtrwOZlppRSKVU3SSkAmDlzJm699Vb06tULffr0wZw5c1BeXo7p06cDAKZOnYrGjRtj9uzZiI6ORqdOnSTrJycnA4BietAhPFyrWeEEMsQTpZQvmVJq6wqZV4YIADqiLAol+x5NHEXEANXUs4AhgnTSaAWVwr5HKaUMsup51mr1TCkWYaU3ekYQaRFYQbfvyYmvECBHwsmaRT8j0eeW2/Y9ofpeiH9PdxAIRY4a6qRSykurWiCs4QFRX1GvhesQfS1g3bu4fY/DFWgLwqUyD8I7OWoc/qy+BwBXdW3kfN2uQSLeuK6ry3VaZyTgpQmdEGnQYf7fJ3DUkbMEAI2SojGmc0OM69oI3TKTcccXW/HnAWWYdruGUouoXCnlCrQlTw0f3dQD934trb5HZ1Ut2HACM4a3BiBVOLnCA8NbAQAGtRazUWKjyAX52MUyNE2Nxdj3/nZaCwESBL9w00kAQEKUEZmpsRjbpSHOF1cit9iEtyZ1QXKs9gPsgo0ncN+wVsx5on3Pve9wRYcMrNwvWm/OyHK03lxxCB0aJaJz4yRntb6fHaTfC7/tx+Lsc3jvhm4S25w/1EoDX1uDk6+N9Xi9QNv3FJ9X7frzaKWUu+QFbWO12OyIDCApJWDTsXy/k1I0QenPnMJ7v9qOlfvz8Mt9A9C9aYrX26F/Do9IKXnQeQBUZuGCyZMn4+LFi3j22WeRm5uLbt26Yfny5c7w85ycHOj9Vc66JuFKKeUNKRUICJlSAillRuja9yJjAXq8ky4fbqmSbs9p36PWN0RKOzM2i4Z9r1r52xgi4dGovNZ34/Y9JULdsucWPLXvhcB+9xWBtnTRCCfi0l+QWNU8se8FuPpeICCQva7uCyFs3+OkVIiAfq7OL2Pn8nCEBvxNSnmLW/o1AwAMaJmGJ37ajU3HSMZUs3pxeHpcB+dyM0e2xZYTBSihFDWjOmZIyDAASI3z/4jilZ0a4Kd7++Paj8QS5XP/Em1Rb608jN93n8f/buvjllJqbJeGmHtjD+Y8ITT+4UXZaJwcIyGk5Cg1WbD/fAn2nxdtifPWHceTo9sBAJbsOocjDIXZG8sPqZJSTvuem8fDG9d1wZDd53H8YjkWbDyB9xmKtOmfbwUAtMmIx4tXS9UMu04X4eq5G/H4qHbOaf6y0Fltdo+Pa2+VUmeLKlFWZUHbBto5evLWuBPqTp+r7tr3zhaJnUqz1YZIY+Bv2r7mnbFAD3SU+9G+JxCpX/5zyidSiv5tPHlWk2f61fWg8xkzZmDGjBnMeWvXrtVcd+HChf5vkD/g7Dyp/LahaN9z13oUSvY953+BlNKw7wnfUR/hqK6nZd+zMOx7EZ79Bnqj+vLcvqeERAUTWpYcTTCr73H7XkBQF0kpb6vvBSKrMtD2PQGe2PdC7DgIrdbUYdBWi8IKc50e/Q11yDNNDEF+AMhMjcWs0e2d77s0SZLM79AoEUsfHOx8P7F7Y3x8Sy/Ui5eG13prOwOAZvWUoeN9slKh0+nQs5l6ODgAHMwtxQd/HUVucZXmcgDw6Eh1WyMdGn+2SKVqkwZoQuepn/cwSSIAilypzzeewGM/7HJ2jt3lcpJjI3Fzv2ZoUT/O5bKH88pw06f/KqYXVZjx31/2ON9XVlths9lx4/x/cNOn/zjb+tehC3j0+11uV+crrPBcremtUmrga2swas56XChR/v52ux2Ld55lZpW5Q7RIqu+5SV6cp45Diwd5ReeKKvHtlhyviEFTAFRm9C2kwo/2PQFp8b6FX9PXUbn6SQvyQYHqABB6HEGGK7IlFO177trygmnfk6wrI6DotqnZ9+j1XNr3ZPP0Rs9+A0OE+vKa9j0dJD21QFbfc74Pga6UK8tOqMJT+x5NFoRYh9or1GShDsm1J4yIS1/gdfW9MLTvCVCzxwpwRVoFEVwpFSKQd34KyquRnuhbZgdHYCAfmQ9k5oy76NQ4EcPa1se6wxcxpnNDxXzamqdWIY+FIW3q493ru+LquRsV9jIardMTcOqSqDB5YHgrXN2tsfP969d2xhM/7WGtCoBY9w7mlqjOv3toC7RIi0OL+uoB74JSylss3HQSDZOisWxvLko1yBt5rtQLv+2XLeHZ8eBu594di9OZwkqcKax0quaKKsyw2OxOxVW3zCTc0j/L5bb2nClG/5b1VPOzWPA1U+roxTLFNW/JrnN4eFE2AKC1LNz/zwN56J2lTXh6opSy2ezQ63UwU2xOtRuDA4Xl1Vh98AJeWbofhRVmnCuqxKNXtHW5Ho2qAORxWVXC8H0BbQP0VVkpzaNzvxMlLGvQ62C12QOy7ziCDFdqoupyYO/PQFme9nKBRv4R8l+w77mDmrTvFZwAqh3W/kjZwBFt31ObVl0BHFsjnWaIIDbFQ8ugqmjZ+yNQnu9ZW+Xw1r4HENJI6FTWFftebVANqdn3yvOBk3+T6fZaRkoFzb4X/H5LjcDr6nuBsO+FyAAat+9xuIL8oXzNwQu4oU/TILWGQwvuKi5qEjqdDvOn9kJ+mQkNk5Qh1UaDeOHxpANYXFGNevFR2PDEcGQ9uVR1ubgoA/56bBiWZJ/D3UNbKMiMyb2b4tstp5F9uoi5/o/bzzhfj+vSEJ0aJ+G1Pw46p9FKMDW4IqU+vqUn4iKNmPl9tioxN5v6TDVUVls1yRpPOcq4KHFbsyd2xqyf1ck7V1i07TQua5fufF9Uacb+cyLZdzC3FBXVFkkGGQvTF25Fy/pxWP3oMLc/2xulFK0601MPSVVmK2757F9sPVmouu7H647jgeGtER+lfhuzuhmmvSOnELcu2IJZo9tLBggsboxi3vbFVuzMKXK+X7Ev12NSKhBKKfo65a9MKVpFFmHw7WGG/jk8uaYKPGFspAGlVZYazzLjqAHIVT1ymEqAH6ez5yU1BYpzgFhZMZCkJt63p3474DQjQzFnE/lvjHb/4d7ox8FGo4v9RLc5Rma1FdpBt0d4HeH4by4H/npFtnwMgGJgxX/VP3ffL4y2RgNR2hZtCfRGIDpZZZ4Loon+LeqKfY+GJ/s52IgTc0FV7XuLbhHPNRq1IUcrsZFyWqz2YJvXoK+rgVAQhiLoa6TRg++c1Nj1Mp4iPt31Mp4ioQG536khOkk5LYTJSU5KhQjkHaZNxy5xUipEEarVniIMeiYhJWDezT3w1srD+O8YdYLnup5NJARRUaXSHiEP6AZIB7F5WhweGtFaddsJ0e5dbl6d2BmJ0RESUsodxESwt58UE4Fdz13hfL/lqRESgm1E+3RmELwaKs1WJDFUNQL0Hl7kOzUiN430hChM6dMUh3JL8cvOs/jjocEY//4GzWwsFpbtESsOXvbWWsm8g7mluPqDjTjigpQCgGNU4Lc78Ma2Rp9L9F77ZedZBSHF2q1FFdWapJTFTaXUg9/uRGmVBf/9ZQ8m9hAfRswW1+c6TUgB3l0fAlG5kG6HJ0UEtFBE2TpZx74noIkoT3aZoAATqm36K0eNI4TQ+gqg4zVA2UVCkHS6Dti/WKwUZ6KuX1HxgKmUqJYmLSQP/mtfA4Y8Jt3mgAeAwlNAh6s8b8+1nwJ/Pg9YTUBFIRCbQkLOzVXkwtRzGllu+0Ki5sjoCCQ2BI78CbS8DKgoAM7vIh3wNld6tUuYqNcS6HM3kLcPiK8PtBwO7P6edD71EUBVMVkuMg4Y9l8gsTFwfjdZtvUoMm/YLCD7ayC9PZDmuH8nZQL9ZwDnssl7nQ7ofTt5Pfwp8hnCgEJMMmAxAUdXsduY2Jhsu/dtQHo79jIs6A1A4x6kHQeWAEVUx8tVh5omjepK9T0AGP0GUHwaaNA52C1xjWs+BnI2k/NcgHCTlytKih3PpA27ApEOwq354PAi39Qw6BGg5BzQcQL5f/B3ck4GAp0nkeuQ3QZ0nRKYzwg19LuXZP/FpgItLnN/vcGPAWUXyL3HX+g5DcjdDbQa6b9tjnwRWP8m0Ola6fRx7wIXDwFZg5Tr0CR6iBHqnJQKEQgP2g0So5FbUuVW6DNHcOBBzExI4cpODXFlJ6W1j8bLEzphVMcGuPN/2wAARkr288t9A7D9VCGu752JLs+vlKznjvhq79li5+s2GfE4nKckRh67og0SowmL/383dMN/ftytGmwux+hODfDzzjNolBSDj2/piVeXHYDRoMcb13ZRLJtVLxYnL1WgfcNEfHRzT9z39Q6s2u+eFeSN5Qfx16GL+OW+AYpcLsBzUiolLhKbZw13ZmI9f1VHPDOuAwx6HVrUj9MkpY68Mhqtn/pDMm3JrnOqy2efLpKQFc3T4ph5Td7AG8WKmTqZ6Ewzd0mU4kozmmhkbVvdzJSifzLahmr2Iu/BG846MEHnYkOqrTbkl5mQGhvpk92YtjP6muUkyZTyYKcJv6NQbTMQKjOOICM2lRBMNLp50Im67jPltMg44JqPvGtPciZ7m3J0lnVgBj3i3ee5C50OGPOGdFqPqerLXzlbOa3bFOW+1emAUa+wt9FjKvsznmeMyAPADd8AjbqJ70e+CKx6Vr2NzjYYxHYkNARWPiXOc5WlRZNEdcW+BwB97w52C9xH1xvInwQq9j0hn+yqD4CGyue5sEZUvPS61EtFAeoP1GsJTPk2cNsPRTQfTP48RXQicM08/7bFGAVcPde/22zWH7jlZ+X0Xrepr0NfH0OMlAqt1tRhWB2dM8HKw0d/QxfWmgwmrGFERxgwskMGPrqpB5qkxODNSV2d87o3TcEdg1sgMToCX93eF+9O7oq3JnVF09RYXNWNIUGWYabD0vTIiDZY+chQ5/RBrUT5dsdG4oPt1d0aY98LozCyQ4ZbbR/RIQPr/3MZfrl/ADo1TsI3d/bD/27rgwZJSrvE13f2w0OXt8ZXt/dBhEGP+VN7YVLPJohzI5dqcfY5FFeaMfztdVh3+KJivjdq2IZJMUiKER+ehcp35RoB1TERBkQY9Phz5hBNtRANeee/aaoyoN5beKP2oUkfer+xCCQdI7OlmKHkE3Aivxw3fCJWfdRSStFE4vZTokLLGzWQN0qpQFzv6UypnTlF6PXyn3h2yV6ftklbG30lpaxeZkoJ+zfOQeJWW21Yvve81iocHBzBgpwUcrcTRHecFMHi3L5XK6Fm3xMqOdYGux4HR7AhuX5x+x4HA8IDerxDJVLpJ7sFh/8h9FP7tUjFxzf3Cm5jAoTRnRtiNCMwXcCg1iKRdF1P97I6bu7bFINbpTmJkM+n98a3/+Zg9sTO2HeuBDtzijC0TX3JOp7m1mS6SbI0To7BI7JKfq9f2wWvTuyMXaeLcN28zSprSvHgtzsV0zxVSmlBK6D6u7v6AQBapSdg5SNDMOC1NZrbatcgAQdzSyXT6sVFon+Leth8/BJzHavN7iTIXIGllLLb7ZpVHWmSg17KVSi5gN1nijGgZRpz3oPf7pSolrTIZLXfzJPqe+LneGPfC6xSSsBX/+TgqTEdEONlUQA6Y8udEHgt0D+HR6SUY1k6Q+6er3bg5GtjfWoPBwdHACC3z7lL4EgsJrLrlUv7HrVuXam+VxvgrL4nV0oJofV1JAeJgyOQCGH7Xmi1pg5DUAbEO5RSFdVWPPbDLjzx4+5gNouDAaED1TQ1FkmxfOTGXeh0OmSlxTntQ5e1TccnU3uhXnwUhrSpj4dGtA5qJUO9XocIgx69slJx6OUr0SItzsvt+K9NzTXaEE9ldDVKjsH4rtpqtbcmdUXf5tIAzcSYCHx0cw/MGs3O+jirUXFRDpbax+yC1Pn7iKg0e+T7bBzJI6SZuyTFa38chEWFHDlXJG27FtGlxpt5Q7zkllR5rHzyVXXEgppd8WyR+7+pHGY/KqV8tu/5WG2Tg4MjEJBdTBVKJTfv8ZKOk2wdbt+rpXBh3wuE6o2Do66B2/c4XIEObwWAg7kl+HH7GSzadhplfqqcxOEfCB0oAx8dq7WIMhqw5rFhmD3R88BQfyqlXrmmM8Z2aYgHh7dSzBOuFQIqNK4TqXGR6NQ4CYvu7o87BjV3Tk+KiUBybCTuHtoSSx8chB5NkyXrDX97rdttZal9tOx1APDQd9nO16cLKnHrgi0APCMpLpVXY9me85LMMkBJbGlt099Kqf3nS1wvSK/jgoSze6AkEqBGwrlTUVB1mzQp5atSivpOnnw9YV/5Wv2Pg4MjAJCrWbh9j8NdcPseB0fgoeOkFIcLWJxKKXIDpfsT/irnzeEfWJykVJAbwhFwTPGiAqY/xV6Nk2Mw98YemHlFW5x8bSz+d1sf5zy5BYsORH96rLTC4kc3iWHxdHYV/bpjoyTMpZYD3LfRAWylVO9X/vQoa+pccRUANoGkRtys2JeL+77egXHvb5BMl29Cm5RiT/e2wty9X23HlhMFbi+vRcpUma0Y+e56j1Wzamozb4g257q0fc/XTCkb/dpzpZTcVqqmmOPg4KhByImDgNj3XJFSAVZKhWr1vXCH075HXcvtdsDG7XscHH4DPQjrx0F0f4B3q0METvtetHJUp7SKk1KhBGenKMROZo7QgFaGku/bFl/L7Uv0+9sHNUf/FvUAAG9c1wV9Ha8BSCynNCkFACmx3j/0qZFP54uqmNO11D8sQoUmQWjS7e8j+W5tw92gcxreklJ5JSZc/7F7uWSusPrABRy9UIZF2057tJ4a0eOLwsmf9j3693elFKMhfC+51ff9NUex+0yRT23i4ODwEXJlkvy9u/dHrdF8V0STxL4XACKD2/cCBIZ9z0qprbl9j4PDd3D7HocrOCsKMapoLd+rtKZwBA9CByqY+UccNYdre7gX5C7An/Y9OWiiQW5fev6qjujYKBHzbu4JnU6HL2/vg99mDFK0n664JyeloiOUD9eXv70WBZQKS4DdbsfH645h3eGLyD5dhDMq+VOVZiu2nyrEI4uycaG0Ch+sOYLBb6xhLi+cUiwCiSZBOjVOwvB26QCA/DIT83PlPMfTi/cqcqbEz1UjpbxXFbmCN5Y8T6BG9Jh9IJNoNZK3hJ0Aq8S+50nQOfkv/83+b/URvPbHQZ/axMHB4SO4fY/DW7DsezaKlOL2PQ4O3yEh/EOrH8tp5xCBMKrPKu3+1srDeGvlYaz7zzA0q+dd+DKH/8CVUnULr07shCl9MtGhUSI+WnsM7685qrl8ILnKHs1SALAD0NtkJGDpg4Od740GPTo3SVIs17GROM1gcN3YYxfL8fG6Y5g1RmoJ3HKiALPdIAFKKs2Y/Mk/AICKagtW7MsDALyz6rBiWSEnixXSTZMgNrsd6QlRAICLpSIpRVf7kyulqi023P/NDvxy30DFttXi4WgSpqTKjGcW78WE7o1xWdt09goyaFUvdNeyRnOPJosVUUb3RuWtKoSaJ5ZMOcw2/yml6N/YE35LvP4q53VqrDzeOTg4ahAu7XveBJ17at+jq+8Fwr4nJ8m4UsovYNn3aKUUt+9xcPgOXn2PwxXkmVIsDH1zbcBH1zlcwxl07kaHniP8EWU0oFdWKmIjjZg5so0irwmQPmcHUimVGB2BvS+MwspHhni9jfoOMgcAWqfHu7XOgdxSxbTCCnaI+ZjODSTv6bBzOmeptEq5vqAUdaWUgh1OUopWStHrsSyAO3OKsOt0kWK62m9GW93eWXkYv2afw/TPtzKXBQCjjIC6pKLikrcVUM9EottWYXI/n0tNKeWLfY9uo+9B5/RrT5RS6krVjo0SfWoTBweHj1DY97xUSmna91yQExJSqibse/xZ0D/g9j0OjoCD2/c4XEHLvkfjeH55TTSHQwMWrpSqs9DpdLhjcAvF9IaJ0dQygW1DfJTR58pj/8y6HEsfHIQmKbGuFwabQFIj0Ed1bID2DUVyoIgir6rMIpHBqtYXFUG+F0tFZJIopYD6jn1Ob1MIW79YapJMp/Hkz3sU09RywOhQcNa19/ONJyTv68VLO0AXSt0npVj7A5BaCD2pxMpSmwHe2/f+OngBL/y23/lerb3ugiaiWKTUkbxSZni+sKxBp8P3d/fH+K6NkBYfieTYCPRvWU+xPAcHRw1CrkySy1Ddtu/p2a8B1+SExPpXE/Y9rpTyC7Tse3ojJ/84OPwByfUqtM4pTkqFCGzOMtc6TbVUdk5RDbWIQw3OThHPlOJwoGFyjPN1IJVS/kKDpGiJjY/G8ocH4z+j2qIBRbSxcpvMNjYpEWU0YN7NYhW/Q3miyoo+ZUwM0kjgJiyMbdNKKdq+R0Mgol5ddoDZNvIZSgJE7VSWVpuTEiRVZquEpAGA1Dhpm84UVqi2Q66MOlNYifu/3oFNx6TB7TQxU1Htu1LKW/ve9IVShZjPmVIS+560TSv35WLku+sxdcEWxXp09b0+zVPx/pTu+GfW5fhn1uVIT4hWLM/BwVGD8ETF5O5yIVd9j9v3AgIt+x637nFw+AfcvsfhCs6KQjodJnRvpLocS7HAUbOgfysODoCQPALC/bho1yAR91/WCrMndnZOu1hqUpA5LFIJAKIj9GhWLw439W0KANh4VCRZyilShVWtzw47bDY7vvonR7ONdkCFlCLbPF2gTgalxSvXU7fvsTOUblu4FYUVyvD3enHSB+d7vtohybyiISeHRs1Zj6V7zuPG+f9KqshVUfupvNp9pZQaZySQSaVVZqYSyV1UW2z4YdtpPP7jLlXroRbow0nOky3aSioN0nZPAcJH0fY9o0HPDOnn4OCoYbhUJvmj+l6w7Xuy7xBiHbuwB8u+5yrcnoODwz3oOSnF4QIC0WHU6/Dk6PbODp0c5R6MlHMEBkLnNNLITx8Ogpq079UULmuXjt8fGASAKJAGvf4Xft99DusOX0S1xcYklQA4g7gF8ucgI49K2KYcFqsdOxmZT3LodUDDpBjFdMFSFquhNk2NU3ZSVJVSKhlKaw5ewIi31ymWT45VPjgv35fLVGdZNCr7XfXBRudrej+Ve2Dfs6oo2aotNlSZrbjsrbW4cs56r3MKqy02/OfH3fh+2xn8vvu8x+vTSi651TAhWv33s1L2PQ4OjhCDq/PS5+p7OtfKJMm6ASAzuH0vMNCy7xl4nhQHh1+gRfgHGaHVmjoMp/pGT+x7r1zTGU9c2U6x3JsrDuFwHruTx1EzEJQGkT7m+nCEL2aNboc0Kj8ogyKlfK1KFkro1DjJaSc+W1SJGd/sxK0LtuDWBVtUlVICoTC8nXaFugKG0qjKbFVVFgmIizSgf4t6yEhUKp4+/OsozFYb4qPUOwmnCioURIxaphRtUZP/rqwBAlZlvGcW78XsPw7CbrfjbFGl87NZFkUWaDVTuSPo3GK1SQLkWVATL1lsdpwprER+WTVOXqrA4bwyt9ohB03SseydR/JK8d7qI6pEmlamVEK0ekeStu9xcHCEGXy177ljx5MopQKRKSXPuOKklF/gtO9xpRQHR8AgubaG1nMU71WHCIRBc3r0t2ESOx/jinfX4wgnpoIGIXiYK6XqLu4e2hIbnxzufB8TKT6UspQ44Yz6DJvc5uOXUKJiJRaKNXRpkoRpA7IAsO97rMp0hRVm3PPVds32LLq7P4wGPXQ6HW4b2Fwy7+edZ/HLjrOIjVTviOw6XYRfdp6VTFPjN2giyh2yUe2a8Mn64/hk/XEMfG0NPttAwtG1lFI0TBQplVNAwtYnf/IPur6wEueLK1XXU6toZ7baUEkRaltOKi1y7oAOTDczvsvId9fjnVWH8eaKQ4p5drtd2u+QtTWeUkrJCURn9b0Qe5ji4OBwA75W33PHjhdo+56chOLXIj9Bo/oez5Ti4PAPePU9Dldwjv4axJsbKzNFwIKNJwPdJA4VCBYhXyugcYQ3aFWMHcDWp0Zg45PDXVbQDDfUZ2QwAVBV6gjKKp1Oh+ev6oiTr43F4ZdHo0mK1G7nSd52DJUXlBQjjpjOGqNUkz7+027klVRpbu+91Uck73UqOScF5eJ3dIeUitIgqmf/cRAA8PJSEsLubuB4FfW53287gzdXHMT2U4UAgD/25Kqux6pgCBACqdQkfq+8Yu19pQZaKaWVKbWVQXrJmyZ/T9v3SmVKK1Ep5W5LOTg4QgbuEjhq1ffcqabH7XvhCW7f4+AIPCSEf2gR6vwsDxEIHRRaKZUcqz4ywB/IgwenfY8rpTgcsNvtTEVRbUBiDPuhfu2hi8zprOqhEQa9w5KlruzRQkpsBCqLibqH3s9qxPDfR/KZ0wXISRC1SnUXKTVXtRth3jQp1To9HkcuqFvj3LXv/bk/z/n66IUyHKW2qeVgUyelbCirEokeVmA7CxEGnUQRRVcCNGsQbCxFmLxt8kypCKoTWlxhRiJl56Ot7hwcHGEGdztBPtn36OW5fS9soFV9j9v3ODj8A159j0MNxZVmWG12Zk5G+4YJuKprI0wfmIU3ru0iWY+HvAYPAikVYeC/QV2HcL72bV4vyC0JHMpMbEXUnrPFzOnREezbSoIPCrIoSinlbZW1RpQdWm5tUyNwftt1Di/9vh8AUFrlOmQ80qjHnzOH4KWrO+LBy1trLuuOfe9iqQnH88tV52sRM6qklMWGMkp9VFThXkVXee4WTUqphaoDbPLN1f6nSUK5Io8HnXNwhDFqpX2Pd6X8A5Z9zzFowu17HBz+Aa++p425c+ciKysL0dHR6Nu3L7Zs2aK5fFFREe6//340bNgQUVFRaNOmDZYtW1ZDrfUfThdUoOsLK3HLZ/8yczJ0Oh3em9Idz43vqHiI56PEwYOz+h6Xq9V5/Pvfy7HswcFo2yAh2E0JGMpcVHxr1yABw9rWd75XCw1PjHFNSv3fDd2Y07Vsce7g2h5NMJsi9s8UVmLm99nOEG5Bqfr2pK7Y/+IozJ/ay7nsZxtOwGK1OW27Wog06NEqPQG39M9SzQQUrhuu7Ht/7DmPU5fUCSlAfV8DInkzsXtjPD++A27uRyq6mm12yW/qjlLKZrNr2hdZmVLOdjC+p0IppUFSVZqlgfI86JyDI4zha/W9kLDvyb4Dt+/5B0z7nuNexe17HBz+geR6FVrPUUHvVS9atAgzZ87Ec889hx07dqBr164YNWoULly4wFy+uroaI0eOxMmTJ/Hjjz/i0KFDmD9/Pho3blzDLfcdP+04AwDYdOySs7x6lIrKwCyzjgjk1cHcEkyYuxF/H2FbaTj8D27f4xCQFh+FDo0Sg92MgOLJK9trzu/YKEkz/04AXaHwgxu7M5dpWT9eMc2g10lypLzBDX0yERcp7Tj8vOMs5v99HICYiZQaH4nYSCOa1YuVLLv/fIlbn0Nfv9XsnALBppXDBAD3fr0DD3y7U3MZeQg4DYG86dsiFdMGNkekgXz/siqLpMLhpmOXcKFUO1eqyqKsNEhDrcIewCas5IScFilVJSOleNA5B0c4oybse7z6XnhCUEpR98YqhyKb2/c4OPwDbt9TxzvvvIM777wT06dPR4cOHTBv3jzExsZiwYIFzOUXLFiAgoICLF68GAMHDkRWVhaGDh2Krl271nDL1bH6QJ7LEW5AOgIslFdXUwQY9NLpO3MK8eU/p3DPl9uRfboIt3ymrS7j8B+qHZ0sHnTOURcwqHUatj09Aidmj8HJ18ainUwVRizIrrczuXcmAKBtRgLaNWATeR0bJeKeoS3x4tUdndNiIw14/qqOSIw2YtZoZbC5gM6Nk5AcK31wHdkhA3cObo5ezVKY5IhAzggkiJBlJA9ln//3CdXPHdu5ofN1mwxx36SpBMQLVkS1oHga5x0h5G0z2Eo8uoqeHALxI5A3EUbyf8HGE3h/zVHJsn1eWY2R76zDH3vOe/w5gLaazj2llPr8KrNNNo/850opDo4QREIj8XVsmnK+N/Y9mogyuJHd6OnynkJOQrmj3uJwDWemFHVD+PN56TwODg7foA/doPOgnuXV1dXYvn07RowY4Zym1+sxYsQIbN68mbnOkiVL0L9/f9x///3IyMhAp06d8Oqrr8Jq1X5orilsOpaP27/YhqFvrsW/xy9pLltFPegLo8FqeSlXd2skeb8jpwjPLN6Lk5cqfGyxf1FltmLa51vw+I+7gt2UgKHaoRrgSimOuoK0+CinVUx+jSqpMqNlepzLbXRpkozlDw/GV3f0VVU+6XQ6PDm6Hab2z3JOi4s0on3DRGQ/ewXuHtpSdftNU2OdxJeAp8e2x1NjO0CnY6utvv43B1abXSw04SA6YiOlnYzfdp1jfubXd/TF0+NEJVnvrFSx3SoZWvllJmw/VYhCN7OcAGk1Ohqz/zioqrgS1EdGR/adK7vxkQtluPfrHcx5cgudHFp5WyyboiJDSuO9XCnF7XscHCGIm38G2lwJjHsHuOZjoHEv8loOt0kparmGXYEOE8j/ATNcr9v7TqBRD6D1KKD5YPc+zxPI7XoNurCX4/AMLPtelGNApmHoCA84OMIaugDbm31AUOn9/Px8WK1WZGRkSKZnZGTg4MGDzHWOHz+ONWvW4KabbsKyZctw9OhR3HfffTCbzXjuuecUy5tMJphMolWhpMQ9G4a32JlT5Hw9+ZN/8M0dfTGgFWO0CNIRYCGvRE0pFRdlxH3DWuLDtcf819gA4ON1x51VuV65pnOtVBOZuVKKow4jVmaDq58QhdsGNkdJpQUj2qdrrisopLTyieSIiyKfp5aj9/3d/fHVP6fw9Lj2eG/1Eef0BdN6oVk9kSzr0CgRDw5vhfdkKqH1hy86SRAjVbwgPSEKFyibGwtdM5MRH2XEs+M6ID0xSkFEjevSEL/vVqqPvvrnFDo0dN/2GasREr9w00ncMbiFYrpVppQSVFeucK6oEo2SY5BbXIUP1x7F1P5ZmjZBALhUrp5LxQo6d1V9jw465/Y9Do4wQKvLyR8AdL2B/LHgNilFnd+GCOD6L9xvS7sx5C9QoL9Dq5E878hvYNj3hOp7Ha+p+eZwcNRGBNre7APCrldts9mQnp6OTz75BD179sTkyZPx1FNPYd68eczlZ8+ejaSkJOdfZmYmczl/QU4qrTusnvVUQdv3HJ00rcpSrms1BR85BaJyS96ZqC1wZkpxUoqjDiKGukYZ9Do8M7YDoiMMeHJ0O/SilEJa8ERlqKY4EtCneSrem9Id6QnREqJ4eLsMxbKPjGyjmHbsYpnznDZSxNdP9w7AY1col5e0zUHQ3TaoOcZ1aaSY//6U7nh7UlfUi5NWDlq6+zxWH8wDAGQkivaSKX2aonGy1DoIAPFR6veFl5cewOw/DiimW2WKIlaVvQaJyjD28e9vAAA8+O1O/G/zKdzwyWanUirKqJeEwAsoKFcn71hVBuVElVxNJVFKyQhMUSml+pEcHByhCm+UUqEGvUplQA7fwLLvCaQUr77HweEf0HFAIXZeBfVqmpaWBoPBgLy8PMn0vLw8NGjQgLlOw4YN0aZNGxgM4k2hffv2yM3NRXW1crR21qxZKC4udv6dPn3av19CBvnordxiUFRRjdIqcpFl5XRoVZlKjQ2tg4cFm109C6S2wFl9j9v3OOog6DDvVY8MQQOVKnP+Qlyk+yM59w5tiXYNEvD8+A7M+axqdWcKK0WlFHWzzkyNxYzhrTU/T6v6nTD/2p5NsO3pEVj96FC8NYlYEKqtNvxzvAAA0DpdzIvqnZUiIancxcfrjuN8caVkmnAtNjja+MjI1midHi9RszVMVv52guppy0nSvvyyalQ47lWNU2IwsoOS7LtUpq6UKjNZkFciVWnJxVPH86UZjBJSqporpTg4ag3cPW9D+fymiSgecu4/MKvvCaRUaCk6ODjCFiFs3wtqrzoyMhI9e/bE6tWrndNsNhtWr16N/v37M9cZOHAgjh49Chv1VHv48GE0bNgQkZFK0iYqKgqJiYmSv0BCXoXow7XHsP1UIQCgoLwal7+9DmPf2wCrzY6KamUOh5ZS6uZ+zTQ/u6Lagr8OXUCBhpUi0Khk5GTVNlQ7VBURhhB+aOLgCBCO5JU5X2emxmosqY0PbuyOaQOy8PsDg9CxUSIWTu8tmS+QJ3cNUVrT1JCeGI3lDw/BtIHN3V5n49F8p1IqUDlFOp0OLevHM5VJbang+Eijnkny5Ze6vqbLg9OFnKcYh5qrY6MkrJo5FNf1bOJcJiOBTSgu35sreX/DJ/8AUFo3BVRUW5n3MwF9X10teS9XSv226xy+3yYOGGlW3+OZUhwc4Qu3yaYQPr9DuHpVrQBLKRVinWcOjrAFt++pY+bMmZg/fz6++OILHDhwAPfeey/Ky8sxffp0AMDUqVMxa9Ys5/L33nsvCgoK8NBDD+Hw4cNYunQpXn31Vdx///3B+goSlFQpLRLXfrQJL/2+H9d/vBmXyquRU1CBS2UmRcaHXie1j8gRE2nA+1PYpdQB4P01RzH9861O+wULJhelvX0FbUl0FY4brjA7Samgnz4cHDWOsV1ItbkR7dN9OgfGdWmE56/qiE6Nk7D0wcEY1laaRzXv5p74+/HLcFk77Zwqb9EkJQY6HQn4zncofViky7ybe/jtM1PjlAMn7alsKYvVjoQo5cN3fploj5s2IAvf3dVPsUyxzJ4nDE7IqwDSdki1APUnftrNnG7Uq//eWmopOVgV+d5ZeZg5v0p2zxKUqvz6y8ERhuD2PQ41cPseB0fgQV+/Quy8CjpFNnnyZFy8eBHPPvsscnNz0a1bNyxfvtwZfp6TkwM9banIzMSKFSvwyCOPoEuXLmjcuDEeeughPPHEE8H6ChKoVSH6bIO0pPjpwkpJ/hJAVFKu7CDpCerWjvWO/KqzRZWw2eyKYOC/j1zEbQu34umxHXDrgCzNz/EWldRoea1VSrkIpQ9FWK1WmM3uV/vi4FDD1D6N0L1RHPq1CgxZJMBo0PukxFLDaxM745P1x/HZtN74afsZfPAXCT5vnByDZvWUn3dlp4bO11FGvTP/zxu0qB+HBonRyKXsbFnUZxZXmjGwdRoWbTuNa3s0wU87zgAAmqfF4ZnxHfDzjrN4ZGQbJMVEYMczIxFp1KPTcysAAEWVZhSWV0Ov18FqsztD2uVEWDxFSsWrkFJy1ZUAucJpTOcG2HayEBdKTcwBGRpWm92pbmJV5KOVTxabug283HGPUSPUODg4QhjeBJ2HGiT2F27f8xu4fY+DI/AIYfteSJzlM2bMwIwZ7DKva9euVUzr378//vnnnwC3yju4ejAXsPVkgWK02B2So7dGkDAdZFthtko6HwDw3K/7YLba8dySfYEjpWilFCMzK9xhtdkh/GzhMFJvt9uRm5uLoqKiYDeFoxYhXQccP1aC5ORkNGjQwCWZHkq4oU9T3NCnKQBgQvfGTlJqSJv6Lr/HxB5N0CAxGu/+eRhtMuI9/uzoCAP+emwYbv9iKzYduwQASKcsdBmJ0RjVMQM9miajcXIMbh/UHAs3ncDDI9qgUXIMLqPUZALZNKxtfaw9dBHniypx+TvrFPZtLVKqU6Mk5+vlDw/Gv8cL8NLv+5mkEQCYLdLpsZFGp7rM1fW+oLwa9R2DKiylFE1K2Rj2vZxLFdDpxIEfTkpxcIQjapt9j5NS/oNG9b0Q6zxzcIQtQti+F1qtqQVQU0rJkZ1TpJimlSclQK/XoX3DRBw4X6KYd7ZIDLq97fOteHNSF7z4236M7dIQg1qnScJk7XZ7QDqSZdT3D0f73ndbcvD+mqP4fHpvtMlIUMwXrHtAeASdC4RUeno6YmNjw4o84Ahd2O12VFRU4MKFCwBIAYpwRMv6cc7XV3RUBngLmD4wC99tOY27h7RAo+QYtEqPR78W7lUalCMm0oBW6fFOUqp+QhS+v7s/duQU4ooOGdDpdGiSQtRTHRol4o3rumpuLymGPKw//9t+5nz5fSXKKL6/vH06XprQCa3qx6Ndg0S0a5CILzadVASPCzDLlFJGvQ4xjiD6KfP/wXd39UPPZuz9kl9m0iSljBpKqSqzFUPe/AsAkJlKqhPGM2yOHBwcIQ5u3+NQg9y+Z7dTSqnQshlxcIQtuH2v7mB0p4ZoWT8eCzed1Fxu+b5cxTR37WDpCVE4cF57mS0nCzD0zbUAgNUHL2CKQxkgIL9MHLUWQFv+ykwWVJgsSGcE86ph1f48nLwkWhLDUSn15M97AADPL9mHb+5U5rbQ1h1jiAedW61WJyFVr169YDeHo5YhJoaQAxcuXEB6erqkImq4QKfT4df7B+JEfrlEhSTHc+M74snR7ZyEjpCr5S2ap4lkWEykAX2ap6JPc+9ILoGUchep8eJDSHyUEbfICmgkaGxPIOV7NkvB9lOFuL53Jo5cKHPMs+P6j//BsVfHMNelc7FYSizabi6t4mpFYYWo/sorMTnbzsHBEWbg9j0ONcjtezZqkD/EFB0cHGELSfXQ0Brc4xS/n3Fj36Z4/qqOmHdzT4/XdUcpBQDPqZQ718LFUpPk/ZnCCglp9NaKQ+jx8iqczC+HzWbHwNfWoM+rq3GpzCTflCoe+m6n5P29X+/wuJ2hAloRRcNCK6VC3L4nZEjFxvo/l4eDAxCPrXDOK+uamYwJ3Ru7XI5WGPmK63o2QduMBNzYt6nrhV0ggzFwsOqRIRjRPgOvXNNJMS8+yoilDw7CykeGwMi6htnZ1j1AtO99e2c/bHxyOHo0TZGEw7MUUALonCob4zMMOh3sdjt+zT6LoxfECo+lJotku0KmH7fvcXCEIdxWFoUyKUW1jdv3/AiZfc9KPVeEWOeZgyNsIbHvhdZ5Fdq96jDGsLb1ce+wlh6t4661qkX9eHx5ex+Ptt1UFhh8zYebMOa9v53E1Ad/HUVRhRnP/7YPnZ5f4exA7Dlb7PZnxDBINbtGByeUoWbNs1DlyMPFChcu7eQIP/BjyzskREdgxSND8Oo1nX3e1sgOUtthy/pxaJ2RgE9v7YWb+jZjrtOxURLTngwAGrySk6yPNOrROJko5VjXfRYqTOIgiMXKVkr9vvs8HvouW3LfKa40K8LOAa6UEjB37lxkZWUhOjoaffv2xZYtW1SX/fnnn9GrVy8kJycjLi4O3bp1w5dfflmDreWo83D3nhHKtjiJfY/fA/0GuX3PSuUjhpjNiIMjbCGx73FSqk4gOsKAJ65s51HukCcETqt0Zcju2seGoWezFObyVpvyof5Efjl+23UOc/4US3GvPXQRFZSC6oM1R3GmsAJVZisznJ1GCqPcebWK4ijUoaaKEDplRj1/EAknZGVlYc6cOW4vv3btWuh0Oh4QzxHyaJORgGfHiepZedC5p6BVTD/e0x9dmohh6N2bJiuWp5VSAr7655RiWjlVmZV1HzlwvgQPfLtTMb2k0sys5BrHSSksWrQIM2fOxHPPPYcdO3aga9euGDVqlDPrTY7U1FQ89dRT2Lx5M3bv3o3p06dj+vTpWLFiRQ23nKPOgtv3ONSgad8Lrc4zB0fYIoSr73FSKsDY9ORwrHh4iFtWL3nGkxYaUJaNST2bYPbEzshKi1Mdtb6oYsM7lFeKOX8eUf2cbacK8ciibPznx92YNG8z/u/Pw9h7tlhBoB2/WCaxXAigR8dDGSfyy/HYD7uc7yMNepQyKikKI/zhUHkvHKHT6TT/nn/+ea+2u3XrVtx1111uLz9gwACcP38eSUlJrhf2AZz84vAHbhvUHKM7NQAATB/Y3KdtCQooAOiVlYolMwbhz5lDcfug5pg9sYtieXrgJTqCvH568V7FcuUmsYNhYQySqOFEfjnKTNICIlFGfVgUmgg03nnnHdx5552YPn06OnTogHnz5iE2NhYLFixgLj9s2DBcc801aN++PVq2bImHHnoIXbp0wYYNG2q45Rx1FrXCvqdnv+bwESr2PZ0htElKDo5wQgjb9/hQY4CRFh+FtPgo5kN487Q4vHN9V1zz4SYAULVTsKDT6fDWpK44lFuCWaPbO0Ni1R72zxdXMaefusSuskRj68lCAIUAgPfWHMV7a47i41t6YlTHBs5l6BHusV0aYuluksReXm1hKqhCDZ+sP4Yft59xvl++Lxd/HsjD7w8OQrsGic7ptH2Pw/84f15M8F+0aBGeffZZHDp0yDktPl5UCNrtdlitVhiNri9j9evX96gdkZGRaNCggesFOThCBG9N6orJvTMxoGWaT9t57qqOqKi24rZBWc5prdLj8cw4dpYhXXG2ymxTVdOWUwpcVqaUFl5fflDyPiE6tB6kgoHq6mps374ds2bNck7T6/UYMWIENm/e7HJ9u92ONWvW4NChQ3j99dcD2VQODhFu2/dC+BlLYt/jSim/Qc2+x617HBz+A7fvcXRukgwASKTCWastNnRvmoKXJnRCn6xUPDC8lUfbvK5nEzw1toOkatGQNuzOd56DlJrSJ1My/c8DbJm/K8jtGfvOlThfT+6ViVQHEVXBqMBns9lxy2f/4vK31+LYRaW6qiYg7zjRHSsBFpsdr/1xUDaNkH4RIV55L1zRoEED519SUhJ0Op3z/cGDB5GQkIA//vgDPXv2RFRUFDZs2IBjx47h6quvRkZGBuLj49G7d2/8+eefku3K7Xs6nQ6ffvoprrnmGsTGxqJ169ZYsmSJc75cwbRw4UIkJydjxYoVaN++PeLj43HllVdKSDSLxYIHH3wQycnJqFevHp544gnceuutmDBhgtf7o7CwEFOnTkVKSgpiY2MxevRoHDkiKhtPnTqF8ePHIyUlBXFxcejYsSOWLVvmXPemm25C/fr1ERMTg9atW+Pzzz/3ui0coY24KCOGtU33WUHUODkGX93RF8PbZbheGJBUxgOAr/9VWvcAmVLKoTh1t9+5M6dI8p6HnAP5+fmwWq3IyJD+ThkZGcjNVVb3FVBcXIz4+HhERkZi7NixeP/99zFy5EjV5U0mE0pKSiR/HBzeoxaQUty+Fxio2fdCrOPMwRHW4NX3OD6Y0h2TejbBj/cOcE4zOaoI3dKvGb6/pz+SY30fDbh7SEv8Z1RbXN2tkWT6OQcp1b9lGk6+Nhaf3CKtDuhpWXGt5eOijM7Hjnu+3K6Y/8j32fj7SD6OXSzH5xtPePS5/sDes8Xo/PwKfLj2qHOaUNFJjrWHLkoq8QmdKaM+/E4du92OimpLUP78GXj/5JNP4rXXXsOBAwfQpUsXlJWVYcyYMVi9ejV27tyJK6+8EuPHj0dOTo7mdl544QVcf/312L17N8aMGYObbroJBQUFqstXVFTgrbfewpdffon169cjJycHjz32mHP+66+/jq+//hqff/45Nm7ciJKSEixevNin7zpt2jRs27YNS5YswebNm2G32zFmzBhntbv7778fJpMJ69evx549e/D666871WTPPPMM9u/fjz/++AMHDhzARx99hLQ031Q0HBxyFJZLbc7P/rqPudz/Np9CvsNGLgwK6L3seEZx657XSEhIQHZ2NrZu3YpXXnkFM2fOxNq1a1WXnz17NpKSkpx/mZmZqstycLhErbDv0dX3+LXIf5Db9xwDHno+CMHB4Tdw+x5HZmos3pzUFQBw15AW+GT9cTwzrr3fP8eg1+H+y1ph79li/Jp9TjE/zhFK2zwtTjL92h5NsMADguj33efx9vVWZiB4QrQRlxxhu8cdeSBRRj3mrT2Gy9tnYOW+POeyOQWVztenCyqw4Wg+ruvZJGCZTXP/Ooo3VxA72BvLD+G+YUSdVskI0hWw7WQh+resB4AKOg9DpVSl2YoOzwYn0Hb/i6MQG+mfy82LL74oGdlPTU1F165dne9feukl/PLLL1iyZAlmzJihup1p06ZhypQpAIBXX30V7733HrZs2YIrr7ySubzZbMa8efPQsiWpqjljxgy8+OKLzvnvv/8+Zs2ahWuuuQYA8MEHHzhVS97gyJEjWLJkCTZu3IgBAwiZ/fXXXyMzMxOLFy/GpEmTkJOTg2uvvRadO5Mqbi1atHCun5OTg+7du6NXr14AiFqMg8PfuHVAFv77yx63ll209TRuH9QcP+88CwDo3DgJ2aeLPP5MrYIbdQVpaWkwGAzIy8uTTM/Ly9O0Huv1erRqRe573bp1w4EDBzB79mwMGzaMufysWbMwc+ZM5/uSkhJOTHF4D7eDzkOY7JHY90K4neEGgewTLu9CphS373Fw+A/cvsdBY9bodtj05HBc3a1xwD6jab1Y5vQYBymVkSQGpSdGG9E8jb28Fr7YdBIAFJWR5ITS7jNFWJJ9Dm+vOowx7/0tIYAO55Y6bR1XzlmPWT/vwfy/j3vcFnchEFJysKo7CSihAs+FTCkedB48CCSLgLKyMjz22GNo3749kpOTER8fjwMHDrhUSnXpIoY2x8XFITExUbVqFQDExsY6CSkAaNiwoXP54uJi5OXloU+fPs75BoMBPXv2VGzHXRw4cABGoxF9+/Z1TqtXrx7atm2LAwcOAAAefPBBvPzyyxg4cCCee+457N6927nsvffei++++w7dunXD448/jk2bNnndFg4ONdzQOxO/3j8QE7u7vp9ZrHb8vOMsVu0nREpclAHzblY/R+be2IOpirJwUgqRkZHo2bMnVq9e7Zxms9mwevVq9O/f3+3t2Gw2mEzsQigAEBUVhcTERMkfB4fXqBXV92j7C7fv+Q0K+55ASoVWx5mDI6wRwtX3uFIqCNDpdGhEVTgKBBJVgmDjHGoVer5er/NKxZJbTB5kzxaJaqf0hChJ9SYAyD5dhDeWs8mg3JIqdHxuBdLiI51BuGsPXnQqmAINs9WGCIPeSZQ9PbY9Xl56QLIMTVg5lVJhGHQeE2HA/hdHBe2z/YW4OKnK77HHHsOqVavw1ltvoVWrVoiJicF1112H6upqlS0QRERIzxGdTgebRlUw1vL+tCV6gzvuuAOjRo3C0qVLsXLlSsyePRtvv/02HnjgAYwePRqnTp3CsmXLsGrVKlx++eW4//778dZbbwW1zRy1C3q9Dl0zk3H30JZOBZQaiiqrcbFMLLqRX1otyVmUw2jQoUlKDI5dlBbk8KR6X23GzJkzceutt6JXr17o06cP5syZg/LyckyfPh0AMHXqVDRu3BizZ88GQKx4vXr1QsuWLWEymbBs2TJ8+eWX+Oijj4L5NTjqEtwmm0L4GUvHlVKBgUr1PW7f4+DwH0LYvsevpnUMqSqV8OKi1EmDH+5hj7pWWQhZc6aQkFIt0uKw7j+XKYJ21QgpGvllIoEgbJfGpTIT5q8/7swk8RdyHVlblQ5CrF68cv/QpJRgGzGGoVJKpyPkYzD+dAEc9dy4cSOmTZuGa665Bp07d0aDBg1w8uTJgH0eC0lJScjIyMDWrVud06xWK3bs2OH1Ntu3bw+LxYJ///3XOe3SpUs4dOgQOnQQK6FlZmbinnvuwc8//4xHH30U8+fPd86rX78+br31Vnz11VeYM2cOPvnkE6/bw8GhhfoJUS6XOVtYCQN1Lag0WzWD2fU6HZqmKlW8QrZfXcfkyZPx1ltv4dlnn0W3bt2QnZ2N5cuXO8PPc3JyJMUYysvLcd9996Fjx44YOHAgfvrpJ3z11Ve44447gvUVOOoaap19jyul/AZF9T1u3+Pg8DtC2L7H6edajB/u6Y9J86SloWkVU+PkGJwtqsTwdulIlAWXfzq1F15auh+PXdEWvbNSsfKRIbji3fWSZc4VVSL7dBHeXXUYAMmpEuyBk3o2wQ/bz6i2LSbCoJrjxLLSPfRdNjYczceSXefw4739mVlWNPadK8abKw7hmXEd0LI+CX5mqVryy0zITI1FlZmMzKTFKztWwrwzhRU4eL4UQHgqpWorWrdujZ9//hnjx4+HTqfDM888o6l4ChQeeOABzJ49G61atUK7du3w/vvvo7Cw0C1Cbs+ePUhISHC+1+l06Nq1K66++mrceeed+Pjjj5GQkIAnn3wSjRs3xtVXXw0AePjhhzF69Gi0adMGhYWF+Ouvv9C+Pcmqe/bZZ9GzZ0907NgRJpMJv//+u3MeB4e/kRIbgcm9MlFebUG7Bgl4a+Vh57woox4miw1HLpQhnlJGmSxWTSt0tcXGJKXMnJRyYsaMGarZefIA85dffhkvv/xyDbSKg0MFtc2+F8rkWbiBtu+V5gEX9pO3IdZx5uAIa3D7Hkcw0DsrFe9N6Y4Hv93pnKanyJSv7+iLX3aexW0DmyvsEEPa1Me6Dpc537fJSMB7U7rj//48jGkDm+OZxXtxIr8cE+ZudC7TKiPe+frViZ3RKj0es/84yGybzW7HhG6NsJgRxn44rwxFFdWSaoQbjuYDAPacLcaId9Zh/X8u0+zsT/t8Ky6WmnAkbws2PjkcADvMvLjSLJnHIqWEeYNe/8s5LRyDzmsr3nnnHdx2220YMGAA0tLS8MQTTwSlbPkTTzyB3NxcTJ06FQaDAXfddRdGjRoFg8H1SOqQIUMk7w0GAywWCz7//HM89NBDGDduHKqrqzFkyBAsW7bMaSW0Wq24//77cebMGSQmJuLKK6/Eu+++C4BkzsyaNQsnT55ETEwMBg8ejO+++87/X5yDA4RIff06ktP25eaTknmdGydh26lCnMgvlyiqqsw2TaVUpdmKTAYpNcBReIKDgyPM4C6JY3CtvAwaJPY9/izoPzj2ZWkuMKeTWH2Pk1IcHP6DRCkVWipETkrVcozv0hBH8krx/pqjmNxLWjEnKy0Oj4xsw1yP1VG4qmsjXNW1EYorzHhm8V6culQhmd+xUZLzdYRBj7FdGqqSUiaLDbMndmGSUgAw9r0NWPrgIAkxJeB0QSU+/fsE7hzSgrEmwcVSZd5VuUkkpdpmJOBQXqlISjnse/FRylOiymxVVHuK0PPRsUBj2rRpmDZtmvP9sGHDmGq3rKwsrFmzRjLt/vvvl7yX2/lY2ykqKlL9LHlbAGDChAmSZYxGI95//328//77AEiAcPv27XH99dczv5/WdxKQkpKC//3vf6rzhc9i4emnn8bTTz+tOp+DI1CQ25tHdWyA/DITTl6qwJYTBc7pVWYr6qlYygFCStFKqTsGNUd8tBHTBmT5vc0cHBw1ALpDFJ0MNOgMlJ4nI/Y2CyF5MjoBjXsErYkuwZ//AgOBsKwqIv/1RiC5KdBzetCaxMFR69CkN5A1mGS3Ne0X7NZIwEmpWg6dTodHr2iLW/o1Y6qAaKQnROFCqevMpqTYCLSsH6cIn+3cOEnyXh54Lodg9WPhbFEllu/NxeTemUxF1CvLDuCOwc2Z8+QEkt1uh06nc1b5i4s0ICstFofySlFSaYbdbneqoVhtqjLbFJZCrpTikOPUqVNYuXIlhg4dCpPJhA8++AAnTpzAjTfeGOymcXDUKGhL3uyJnXFdzybYfqoQJ2UDGSaLDemJ0WiVHo+jF8oU26mstqBpvRTn++Ht0jGgVVrgGs7BwRFY0HaRmBRg2u/Ba4u34Ja9wED+PJ/aEpixJTht4eCorYiKD9nrLr+y1hGkJ0ZLrHssvHBVRwDA2C4NXW6ve9MUxbSselKbhU6nw9jOZFudGnteRvrJn/dgyJt/oaTKzJxf5iCZ5Pjm31OS9xUOFVR5NVk+NsqIJEeG1jO/7pPY+lhV4qrMVgYpxU8dDin0ej0WLlyI3r17Y+DAgdizZw/+/PNPnuPEUedAK06n9GmKCIMeXTKTVJe/pV8z5vSruzVGZop4X2leP465HAcHR5iAtmKFmHXEbfBw8wBB1kfhtj0OjjoFrpTicGJ054ZY+uAgNElRZnjI0aNpCn6kgsyn9GnKVjRd0wnTBmahd1Yqpi7YgvWHL7rc9he39cGtC8joyOmCSnR5fiVzuYulJiREK29afx/Jl7wvrbIgLsrotO/FRxmho25+/x4X7STRaqSURZq5FcGDzjlkyMzMxMaNG10vyMFRyzGifTqGtKmPLpR69tb+Wdh3tgRL95xXLH99r0wszj6Lvs3rYd66YwCAyb0ykZEYDQB49ZrOqDJb0TBJW33LwcER4qCJqHBVHOk5KRUQyI8HPe+icnDUJfAznkMCOhdKC92bJjtf10+Icqqs5EiOjUTvrFQAQGqse6MeA1rWQ2ZqDE4XVGoud6HUhIZJMThbVIHmafHQgQS57zsnDbnOLanC2aJKlJmI4io20oAT+aL1cPrCrQCAhGgjDAyyqdJsxZkCqe2E2/c4ODg42DAa9PjfbX0k0+KijJh7Uw8ceXcdDudJrXoxkQb8ct9AAEBplRk/7TiDBy5v5Zx/Y9+mgW80BwdH4FEbiIZwJdNCHfKBba6U4uCoU6gFdweOYKBNhli+vl+LepoVlAQ8MbqdM9i8pYYNI8KgR1yk60Pzhk/+QaOkaJwrrkLj5BhUmq3446HBznDz+CgjykwW3DT/H5RXWzGwFanYlJ4QhRv7NsOWkwWS7aU4QtW7ZSYj+3QR+jZPxb8nCrB093n8KgtkN/KgSw4ODg6P8fsDg9Hm6T9U579yTWc8N76jW/cUDg6OMENtIBq4fS9AkJNSYWrv5ODg8Ar8qY/DKxj0Otw9tAX+v707j4uq3v8H/poZYJhhG2RHQdRAFlEJ1Iu7yQ3IyIXSvKS4Xw3MJZfM/dc1vNfqWjejm9/Eh49SzB6h3lyIyExNBRcQElFLxVsiuSDgLvP5/UGc6wgq4DCHgdfz8ZiHcM5nzvmctwhv33yWDi42ePM5/zq9x8NBgxNvReHvscFYP/HRK/7XtgtebX67dgtA1cLoV67fQfpPxQAAK5USuj9GZl3/Y02pfacvAwC8W2nx50A3vBjaxuBa2j8WOf9sQg+kvdoTL/2xW+E9fc3d0ThSioio/qwslEgaFgwAD/3ZwYIUUTPVHAoNnL7XOB4cKdUcRtURUZ0x86MGmxcdgMzX+9drnQ9rSxVGdPOW1gp5Lti91nYLnw+EhVIBB40lLO8rANmpLbB6dBhc7WrfSXDRlp8AAE62Vvjv1dqn/3n9scX4Sw8Upa7drJreZ6u2QIi3I2wesTsgR0oRETXMyO7eyFn0Z0zq20HurhCRKTWHgk4t66eSEXD6HlGLxjI0yerdl7ri5W5XYK+xxIyNOejjW7XddxcvHY4teRbaP6bx6fUCeiGkXe/yfr2GDzJPPfS6zrZqXPhjFNWDqhdyf7qtI1rZWOHK9TsAIP1Zzc3B+qHXt+RIKSKiBtNpm8GICSIiMhJO3yNqyTjcg2SlsVKhr58LunrpsGtWf/y/wZ2kc9r71pVSKhVSQQoA/NxspY+n9O+A0LaOBtd1tn34D7Pqc5YqJb6Z0fe+44ajr9roHj4CjNP3iIiIiBqII47ofpy+R9SisShFZum5Th6Y+sxTSBnTDXOj/PF8Zw+D8862aqx4sXOt73W0sTJot3HSn9C5jQP+9ZeQGtd4GE7fa9r69++P6dOnS5/7+Phg5cqVj3yPQqHA5s2bn/jexroOERFRsyVqrtdJLdiDuxpy+h5Ri8L/WZNZUioVeP3Zjhjg7woA0rS/aj7ONtJC5Q9yfGDaSI/2Ttia2BtPexuOtlIqH/5bPE7faxwxMTGIioqq9dyePXugUChw7Nixel83OzsbkyZNetLuGViyZAm6du1a4/iFCxcQHR1t1Hs9aO3atdDpdI16DyIiIiLT4PQ9opaMRSlqFp5ytcPo8LbS59WLmdfGQVP3374MDWld63GXhyy0Tk9m/PjxyMjIwH//+98a51JSUhAWFobOnWsfAfcoLi4u0Gof/jVhTO7u7lCr+fVBRET0UJy+R/fj9D2iFo1FKWo2lsQESR+3/aMotXp0GLp46RDd6X+7/KkeMQLqQcuGdkLCgJo7RIU8MKqKjOP555+Hi4sL1q5da3C8oqICmzZtwvjx43H58mWMHDkSrVu3hlarRXBwMDZs2PDI6z44fe/UqVPo27cvrK2tERgYiIyMjBrvmTt3Lvz8/KDVatG+fXssXLgQd+9W7dC4du1aLF26FLm5uVAoFFAoFFKfH5y+l5eXh2eeeQYajQZOTk6YNGkSKioqpPNjxozBkCFD8M4778DDwwNOTk5ISEiQ7tUQRUVFGDx4MGxtbWFvb4/hw4fj4sWL0vnc3FwMGDAAdnZ2sLe3R2hoKA4dOgQAOHfuHGJiYuDo6AgbGxsEBQVh+/btDe4LERFRDZy+R/fj9D2iFo1laGo2lEoFkoYFo/jaLXRu4wAA+HOgG/4c6Ib/2/MLduQX1/uaWisLzI70R/d2TohfkyUd93e3M1q/TUYI4O4Nee5tqa3Tb0UtLCwwevRorF27FvPnz4fij/ds2rQJlZWVGDlyJCoqKhAaGoq5c+fC3t4e27Ztw6hRo9ChQwd07979sffQ6/UYNmwY3NzccPDgQVy7ds1g/alqdnZ2WLt2LTw9PZGXl4eJEyfCzs4Oc+bMwYgRI5Cfn4+dO3fi22+/BQA4ODjUuMb169cRGRmJ8PBwZGdno6SkBBMmTEBiYqJB4W3Xrl3w8PDArl27cPr0aYwYMQJdu3bFxIkTH/s8tT1fdUFq9+7duHfvHhISEjBixAh8//33AIC4uDiEhIQgOTkZKpUKOTk5sLSsSgATEhJw584d/PDDD7CxscHx48dha2v7iDsSERERPQlO3yNqyViUomZlZHfvWo/H9WiLA79cxsAAtwZdt5+fC7q3a4WsM1cAALZqM/ync/cG8LanPPd+8zfAyqZOTceNG4cVK1Zg9+7d6N+/P4CqqXuxsbFwcHCAg4MDZs2aJbWfOnUq0tPT8cUXX9SpKPXtt9/ixIkTSE9Ph6dnVTzefvvtGutALViwQPrYx8cHs2bNQmpqKubMmQONRgNbW1tYWFjA3d0dD7N+/XrcunUL69atg41N1fN/+OGHiImJwd///ne4uVV9PTo6OuLDDz+ESqWCv78/Bg0ahMzMzAYVpTIzM5GXl4czZ87Ay6tqXbV169YhKCgI2dnZ6NatG4qKijB79mz4+/sDAHx9faX3FxUVITY2FsHBwQCA9u3b17sPREREj8Tpe3Q/Tt8jatE4fY9aBI2VCv8X3+2hRau6uH23UvpYwWSq0fj7+6Nnz55Ys2YNAOD06dPYs2cPxo8fDwCorKzEW2+9heDgYLRq1Qq2trZIT09HUVFRna5fUFAALy8vqSAFAOHh4TXabdy4Eb169YK7uztsbW2xYMGCOt/j/nt16dJFKkgBQK9evaDX61FYWCgdCwoKgkqlkj738PBASUlJve51/z29vLykghQABAYGQqfToaCgAAAwc+ZMTJgwAREREVi+fDl+/vlnqe1rr72Gv/3tb+jVqxcWL17coIXliYiIiOqM0/eIWjSWoYnq6PY9vdxdeDKW2qoRS3Ldux7Gjx+PqVOnYtWqVUhJSUGHDh3Qr18/AMCKFSvw/vvvY+XKlQgODoaNjQ2mT5+OO3fuGK27+/fvR1xcHJYuXYrIyEg4ODggNTUV7777rtHucb/qqXPVFAoF9PrG+3pbsmQJ/vKXv2Dbtm3YsWMHFi9ejNTUVAwdOhQTJkxAZGQktm3bhm+++QZJSUl49913MXXq1EbrDxERtTBcU4oMPDhSikUpopakSYyUWrVqFXx8fGBtbY0ePXogKyvr8W8CkJqaCoVCgSFDhjRuB4kA3LpvpJRZUiiqptDJ8arnyLLhw4dDqVRi/fr1WLduHcaNGyeNTtu3bx8GDx6MV155BV26dEH79u1x8uTJOl87ICAA58+fx4ULF6RjBw4cMGjz448/om3btpg/fz7CwsLg6+uLc+fOGbSxsrJCZeWjvyYCAgKQm5uL69evS8f27dsHpVKJjh071rnP9VH9fOfPn5eOHT9+HKWlpQgMDJSO+fn5YcaMGfjmm28wbNgwpKSkSOe8vLwwefJkfPXVV3j99dexevXqRukrERERUY08kWtKEbUoshelNm7ciJkzZ2Lx4sU4cuQIunTpgsjIyMdOXTl79ixmzZqFPn36mKin1NLdumvmI6XMiK2tLUaMGIF58+bhwoULGDNmjHTO19cXGRkZ+PHHH1FQUIC//vWvBjvLPU5ERAT8/PwQHx+P3Nxc7NmzB/Pnzzdo4+vri6KiIqSmpuLnn3/GBx98gLS0NIM2Pj4+OHPmDHJycnDp0iXcvn27xr3i4uJgbW2N+Ph45OfnY9euXZg6dSpGjRolrSfVUJWVlcjJyTF4FRQUICIiAsHBwYiLi8ORI0eQlZWF0aNHo1+/fggLC8PNmzeRmJiI77//HufOncO+ffuQnZ2NgIAAAMD06dORnp6OM2fO4MiRI9i1a5d0joiIyCi4DALdr8b0PU7mIWpJZC9Kvffee5g4cSLGjh2LwMBAfPzxx9BqtdJ6MrWprKyUptZwEV4ylZvmPlLKzIwfPx5Xr15FZGSkwfpPCxYswNNPP43IyEj0798f7u7u9RotqVQqkZaWhps3b6J79+6YMGECli1bZtDmhRdewIwZM5CYmIiuXbvixx9/xMKFCw3axMbGIioqCgMGDICLiws2bNhQ415arRbp6em4cuUKunXrhhdffBEDBw7Ehx9+WL9g1KKiogIhISEGr5iYGCgUCmzZsgWOjo7o27cvIiIi0L59e2zcuBEAoFKpcPnyZYwePRp+fn4YPnw4oqOjsXTpUgBV318TEhIQEBCAqKgo+Pn54aOPPnri/hIREUmaw/S95vAMTRWn7xG1KAoh5PuOeufOHWi1Wnz55ZcG/6mMj49HaWkptmzZUuv7qhffTUtLw5gxY1BaWorNmzfX6Z5lZWVwcHDAtWvXYG9vb4SnoJbCf+EOabTU2eWDZO7N4926dQtnzpxBu3btYG1tLXd3qBni1xi1FMwdqjAO9MSWOFT96RIAJBx4dNumqvoZ+s4Bnpn/6LZUN1d+AT4I+d/n0SuAHpPk6w8RGUVd8wZZx0ZeunQJlZWVNaaxuLm54cSJE7W+Z+/evfj000+Rk5NTp3vcvn3bYFpNWVlZg/tLLZut2hK37tacokVERERE9WDdDIqalvxljNFw+h5Riyb79L36KC8vx6hRo7B69Wo4OzvX6T1JSUlwcHCQXvdvk05UH6tHh8LPzRbrxnWXuytERERE5ufFFMA1CBi8Su6eNFzEEsDzaaDbRLl70nw4eAEdngGsdYCTL9Cun9w9IiITkrUM7ezsDJVKVWOR4osXL8Ld3b1G+59//hlnz55FTEyMdKx623QLCwsUFhaiQ4cOBu+ZN28eZs6cKX1eVlbGwhQ1SIi3I76ZwR+SRERERA3SaVjVy5z1nlH1IuNRqoBRaY9vR0TNkqxFKSsrK4SGhiIzM1NaU0qv1yMzMxOJiYk12vv7+yMvL8/g2IIFC1BeXo7333+/1mKTWq2GWq1ulP4TEREREREREVHDyD5hd+bMmYiPj0dYWBi6d++OlStX4vr16xg7diwAYPTo0WjdujWSkpJgbW2NTp06Gbxfp9MBQI3jRERERERERETUdMlelBoxYgR+//13LFq0CMXFxejatSt27twpLX5eVFQEpdKslr4ialJk3GCTmjl+bRERERER0ZNQiBb2vwpuZ0wtRWVlJU6ePAlXV1c4OTnJ3R1qhi5fvoySkhL4+flBpVLJ3R2iRsPcoQrjQERERHVV17xB9pFSRNQ4VCoVdDodSkpKAABarRYKhULmXlFzIITAjRs3UFJSAp1Ox4IUERERERE1CItSRM1Y9S6W1YUpImPS6XS17pRKRERERERUFyxKETVjCoUCHh4ecHV1xd27d+XuDjUjlpaWHCFFRERERERPhEUpohZApVKxgEBERERERERNCre1IyIiIiIiIiIik2NRioiIiIiIiIiITI5FKSIiIiIiIiIiMrkWt6aUEAIAUFZWJnNPiIiIyBxU5wzVOURLxRyKiIiI6qqu+VOLK0qVl5cDALy8vGTuCREREZmT8vJyODg4yN0N2TCHIiIiovp6XP6kEC3s1356vR6//fYb7OzsoFAojH79srIyeHl54fz587C3tzf69enRGH95Mf7yYvzlw9jLq7HjL4RAeXk5PD09oVS23JUPmEM1b4y/fBh7eTH+8mL85dWY8a9r/tTiRkoplUq0adOm0e9jb2/Pf1QyYvzlxfjLi/GXD2Mvr8aMf0seIVWNOVTLwPjLh7GXF+MvL8ZfXo0V/7rkTy33131ERERERERERCQbFqWIiIiIiIiIiMjkWJQyMrVajcWLF0OtVsvdlRaJ8ZcX4y8vxl8+jL28GP/mgX+P8mL85cPYy4vxlxfjL6+mEP8Wt9A5ERERERERERHJjyOliIiIiIiIiIjI5FiUIiIiIiIiIiIik2NRioiIiIiIiIiITI5FKSNbtWoVfHx8YG1tjR49eiArK0vuLpm9pKQkdOvWDXZ2dnB1dcWQIUNQWFho0ObWrVtISEiAk5MTbG1tERsbi4sXLxq0KSoqwqBBg6DVauHq6orZs2fj3r17pnwUs7d8+XIoFApMnz5dOsbYN65ff/0Vr7zyCpycnKDRaBAcHIxDhw5J54UQWLRoETw8PKDRaBAREYFTp04ZXOPKlSuIi4uDvb09dDodxo8fj4qKClM/itmprKzEwoUL0a5dO2g0GnTo0AFvvfUW7l+KkfE3nh9++AExMTHw9PSEQqHA5s2bDc4bK9bHjh1Dnz59YG1tDS8vL/zjH/9o7EejOmD+ZHzMn5oW5lCmxxxKPsyhTMvscyhBRpOamiqsrKzEmjVrxE8//SQmTpwodDqduHjxotxdM2uRkZEiJSVF5Ofni5ycHPHcc88Jb29vUVFRIbWZPHmy8PLyEpmZmeLQoUPiT3/6k+jZs6d0/t69e6JTp04iIiJCHD16VGzfvl04OzuLefPmyfFIZikrK0v4+PiIzp07i2nTpknHGfvGc+XKFdG2bVsxZswYcfDgQfHLL7+I9PR0cfr0aanN8uXLhYODg9i8ebPIzc0VL7zwgmjXrp24efOm1CYqKkp06dJFHDhwQOzZs0c89dRTYuTIkXI8kllZtmyZcHJyEl9//bU4c+aM2LRpk7C1tRXvv/++1IbxN57t27eL+fPni6+++koAEGlpaQbnjRHra9euCTc3NxEXFyfy8/PFhg0bhEajEf/+979N9ZhUC+ZPjYP5U9PBHMr0mEPJizmUaZl7DsWilBF1795dJCQkSJ9XVlYKT09PkZSUJGOvmp+SkhIBQOzevVsIIURpaamwtLQUmzZtktoUFBQIAGL//v1CiKp/qEqlUhQXF0ttkpOThb29vbh9+7ZpH8AMlZeXC19fX5GRkSH69esnJVSMfeOaO3eu6N2790PP6/V64e7uLlasWCEdKy0tFWq1WmzYsEEIIcTx48cFAJGdnS212bFjh1AoFOLXX39tvM43A4MGDRLjxo0zODZs2DARFxcnhGD8G9ODCZWxYv3RRx8JR0dHg+89c+fOFR07dmzkJ6JHYf5kGsyf5MEcSh7MoeTFHEo+5phDcfqekdy5cweHDx9GRESEdEypVCIiIgL79++XsWfNz7Vr1wAArVq1AgAcPnwYd+/eNYi9v78/vL29pdjv378fwcHBcHNzk9pERkairKwMP/30kwl7b54SEhIwaNAggxgDjH1j27p1K8LCwvDSSy/B1dUVISEhWL16tXT+zJkzKC4uNoi/g4MDevToYRB/nU6HsLAwqU1ERASUSiUOHjxouocxQz179kRmZiZOnjwJAMjNzcXevXsRHR0NgPE3JWPFev/+/ejbty+srKykNpGRkSgsLMTVq1dN9DR0P+ZPpsP8SR7MoeTBHEpezKGaDnPIoSye6N0kuXTpEiorKw1+aACAm5sbTpw4IVOvmh+9Xo/p06ejV69e6NSpEwCguLgYVlZW0Ol0Bm3d3NxQXFwstant76b6HD1camoqjhw5guzs7BrnGPvG9csvvyA5ORkzZ87Em2++iezsbLz22muwsrJCfHy8FL/a4nt//F1dXQ3OW1hYoFWrVoz/Y7zxxhsoKyuDv78/VCoVKisrsWzZMsTFxQEA429Cxop1cXEx2rVrV+Ma1eccHR0bpf/0cMyfTIP5kzyYQ8mHOZS8mEM1HeaQQ7EoRWYlISEB+fn52Lt3r9xdaRHOnz+PadOmISMjA9bW1nJ3p8XR6/UICwvD22+/DQAICQlBfn4+Pv74Y8THx8vcu+bviy++wOeff47169cjKCgIOTk5mD59Ojw9PRl/IjIrzJ9MjzmUvJhDyYs5FNUHp+8ZibOzM1QqVY0dMy5evAh3d3eZetW8JCYm4uuvv8auXbvQpk0b6bi7uzvu3LmD0tJSg/b3x97d3b3Wv5vqc1S7w4cPo6SkBE8//TQsLCxgYWGB3bt344MPPoCFhQXc3NwY+0bk4eGBwMBAg2MBAQEoKioC8L/4Per7jru7O0pKSgzO37t3D1euXGH8H2P27Nl444038PLLLyM4OBijRo3CjBkzkJSUBIDxNyVjxZrfj5oe5k+Nj/mTPJhDyYs5lLyYQzUd5pBDsShlJFZWVggNDUVmZqZ0TK/XIzMzE+Hh4TL2zPwJIZCYmIi0tDR89913NYYNhoaGwtLS0iD2hYWFKCoqkmIfHh6OvLw8g39sGRkZsLe3r/EDi/5n4MCByMvLQ05OjvQKCwtDXFyc9DFj33h69epVY/vukydPom3btgCAdu3awd3d3SD+ZWVlOHjwoEH8S0tLcfjwYanNd999B71ejx49epjgKczXjRs3oFQa/phUqVTQ6/UAGH9TMlasw8PD8cMPP+Du3btSm4yMDHTs2JFT92TC/KnxMH+SF3MoeTGHkhdzqKbDLHKoJ14qnSSpqalCrVaLtWvXiuPHj4tJkyYJnU5nsGMG1d+UKVOEg4OD+P7778WFCxek140bN6Q2kydPFt7e3uK7774Thw4dEuHh4SI8PFw6X72l7rPPPitycnLEzp07hYuLC7fUbYD7d44RgrFvTFlZWcLCwkIsW7ZMnDp1Snz++edCq9WKzz77TGqzfPlyodPpxJYtW8SxY8fE4MGDa93iNSQkRBw8eFDs3btX+Pr6cjvdOoiPjxetW7eWtjP+6quvhLOzs5gzZ47UhvE3nvLycnH06FFx9OhRAUC899574ujRo+LcuXNCCOPEurS0VLi5uYlRo0aJ/Px8kZqaKrRarVG2M6aGY/7UOJg/NT3MoUyHOZS8mEOZlrnnUCxKGdm//vUv4e3tLaysrET37t3FgQMH5O6S2QNQ6yslJUVqc/PmTfHqq68KR0dHodVqxdChQ8WFCxcMrnP27FkRHR0tNBqNcHZ2Fq+//rq4e/euiZ/G/D2YUDH2jes///mP6NSpk1Cr1cLf31988sknBuf1er1YuHChcHNzE2q1WgwcOFAUFhYatLl8+bIYOXKksLW1Ffb29mLs2LGivLzclI9hlsrKysS0adOEt7e3sLa2Fu3btxfz58832AqX8TeeXbt21fq9Pj4+XghhvFjn5uaK3r17C7VaLVq3bi2WL19uqkekR2D+ZHzMn5oe5lCmxRxKPsyhTMvccyiFEEI82VgrIiIiIiIiIiKi+uGaUkREREREREREZHIsShERERERERERkcmxKEVERERERERERCbHohQREREREREREZkci1JERERERERERGRyLEoREREREREREZHJsShFREREREREREQmx6IUERERERERERGZHItSRET1pFAosHnzZrm7QURERGQ2mD8RUW1YlCIiszJmzBgoFIoar6ioKLm7RkRERNQkMX8ioqbKQu4OEBHVV1RUFFJSUgyOqdVqmXpDRERE1PQxfyKipogjpYjI7KjVari7uxu8HB0dAVQNDU9OTkZ0dDQ0Gg3at2+PL7/80uD9eXl5eOaZZ6DRaODk5IRJkyahoqLCoM2aNWsQFBQEtVoNDw8PJCYmGpy/dOkShg4dCq1WC19fX2zdulU6d/XqVcTFxcHFxQUajQa+vr41kkAiIiIiU2L+RERNEYtSRNTsLFy4ELGxscjNzUVcXBxefvllFBQUAACuX7+OyMhIODo6Ijs7G5s2bcK3335rkDQlJycjISEBkyZNQl5eHrZu3YqnnnrK4B5Lly7F8OHDcezYMTz33HOIi4vDlStXpPsfP34cO3bsQEFBAZKTk+Hs7Gy6ABARERHVE/MnIpKFICIyI/Hx8UKlUgkbGxuD17Jly4QQQgAQkydPNnhPjx49xJQpU4QQQnzyySfC0dFRVFRUSOe3bdsmlEqlKC4uFkII4enpKebPn//QPgAQCxYskD6vqKgQAMSOHTuEEELExMSIsWPHGueBiYiIiJ4Q8yciaqq4phQRmZ0BAwYgOTnZ4FirVq2kj8PDww3OhYeHIycnBwBQUFCALl26wMbGRjrfq1cv6PV6FBYWQqFQ4LfffsPAgQMf2YfOnTtLH9vY2MDe3h4lJSUAgClTpiA2NhZHjhzBs88+iyFDhqBnz54NelYiIiIiY2D+RERNEYtSRGR2bGxsagwHNxaNRlOndpaWlgafKxQK6PV6AEB0dDTOnTuH7du3IyMjAwMHDkRCQgLeeecdo/eXiIiIqC6YPxFRU8Q1pYio2Tlw4ECNzwMCAgAAAQEByM3NxfXr16Xz+/btg1KpRMeOHWFnZwcfHx9kZmY+UR9cXFwQHx+Pzz77DCtXrsQnn3zyRNcjIiIiakzMn4hIDhwpRURm5/bt2yguLjY4ZmFhIS2GuWnTJoSFhaF37974/PPPkZWVhU8//RQAEBcXh8WLFyM+Ph5LlizB77//jqlTp2LUqFFwc3MDACxZsgSTJ0+Gq6sroqOjUV5ejn379mHq1Kl16t+iRYsQGhqKoKAg3L59G19//bWU1BERERHJgfkTETVFLEoRkdnZuXMnPDw8DI517NgRJ06cAFC1s0tqaipeffVVeHh4YMOGDQgMDAQAaLVapKenY9q0aejWrRu0Wi1iY2Px3nvvSdeKj4/HrVu38M9//hOzZs2Cs7MzXnzxxTr3z8rKCvPmzcPZs2eh0WjQp08fpKamGuHJiYiIiBqG+RMRNUUKIYSQuxNERMaiUCiQlpaGIUOGyN0VIiIiIrPA/ImI5MI1pYiIiIiIiIiIyORYlCIiIiIiIiIiIpPj9D0iIiIiIiIiIjI5jpQiIiIiIiIiIiKTY1GKiIiIiIiIiIhMjkUpIiIiIiIiIiIyORaliIiIiIiIiIjI5FiUIiIiIiIiIiIik2NRioiIiIiIiIiITI5FKSIiIiIiIiIiMjkWpYiIiIiIiIiIyORYlCIiIiIiIiIiIpP7/5U/VqgDUsE/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Evolution')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eaf1c28-5144-4392-8beb-bb870999e114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = len(X_train)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ada08f-66d3-451a-97e9-965df054cc78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 51)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08299854-9041-467a-bb9e-c64dc08a886e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a3dd21-456c-4114-b467-98dff0e41fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f44d9f-04c3-479b-8afe-40b40ba47a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 910ms/step - loss: 2.8130 - accuracy: 0.2727\n",
      "Test Accuracy: 27.27%\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array(y_test)\n",
    "\n",
    "# 모델 평가\n",
    "\n",
    "X_test = [x.to_numpy() if isinstance(x, pd.DataFrame) else x for x in X_test]\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', dtype='float32')\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926cd20c-f420-4594-8bc2-910cb7e45538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAILAB\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model.save('walk_classification_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
