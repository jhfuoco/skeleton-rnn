{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbca6f-59ff-46bf-a0a3-51f583b3e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mysite/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "video 1/3 (1/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mysite/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. (240.3ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (2/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (219.3ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (3/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (214.8ms) Inference, (0.6ms) NMS\n",
      "video 1/3 (4/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (217.1ms) Inference, (1.3ms) NMS\n",
      "video 1/3 (5/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (215.9ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (6/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (218.5ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (7/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (255.0ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (8/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (264.0ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (9/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (214.3ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (10/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (235.3ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (11/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (241.9ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (12/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (233.5ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (13/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (257.8ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (14/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (263.8ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (15/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (290.9ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (16/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (475.4ms) Inference, (3.6ms) NMS\n",
      "video 1/3 (17/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (465.4ms) Inference, (0.6ms) NMS\n",
      "video 1/3 (18/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (300.6ms) Inference, (0.6ms) NMS\n",
      "video 1/3 (19/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (282.4ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (20/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (284.5ms) Inference, (0.8ms) NMS\n",
      "video 1/3 (21/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (303.2ms) Inference, (1.3ms) NMS\n",
      "video 1/3 (22/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (303.8ms) Inference, (0.9ms) NMS\n",
      "video 1/3 (23/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (303.7ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (24/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (301.8ms) Inference, (1.4ms) NMS\n",
      "video 1/3 (25/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (273.1ms) Inference, (0.9ms) NMS\n",
      "video 1/3 (26/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (284.7ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (27/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (272.5ms) Inference, (2.4ms) NMS\n",
      "video 1/3 (28/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (295.2ms) Inference, (1.5ms) NMS\n",
      "video 1/3 (29/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (294.7ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (30/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (294.4ms) Inference, (6.4ms) NMS\n",
      "video 1/3 (31/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (270.4ms) Inference, (6.2ms) NMS\n",
      "video 1/3 (32/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (282.3ms) Inference, (6.6ms) NMS\n",
      "video 1/3 (33/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (340.3ms) Inference, (1.0ms) NMS\n",
      "video 1/3 (34/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (307.3ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (35/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (345.8ms) Inference, (1.5ms) NMS\n",
      "video 1/3 (36/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (320.8ms) Inference, (2.8ms) NMS\n",
      "video 1/3 (37/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (302.5ms) Inference, (7.3ms) NMS\n",
      "video 1/3 (38/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (313.5ms) Inference, (3.6ms) NMS\n",
      "video 1/3 (39/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (314.4ms) Inference, (0.9ms) NMS\n",
      "video 1/3 (40/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (284.2ms) Inference, (1.5ms) NMS\n",
      "video 1/3 (41/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (241.3ms) Inference, (1.3ms) NMS\n",
      "video 1/3 (42/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (234.7ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (43/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (293.8ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (44/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (290.4ms) Inference, (1.5ms) NMS\n",
      "video 1/3 (45/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (285.2ms) Inference, (1.7ms) NMS\n",
      "video 1/3 (46/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (326.4ms) Inference, (14.4ms) NMS\n",
      "video 1/3 (47/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (284.6ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (48/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (337.8ms) Inference, (0.9ms) NMS\n",
      "video 1/3 (49/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (338.4ms) Inference, (1.6ms) NMS\n",
      "video 1/3 (50/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (320.7ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (51/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (321.2ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (52/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (300.0ms) Inference, (0.4ms) NMS\n",
      "video 1/3 (53/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (306.0ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (54/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (288.9ms) Inference, (10.8ms) NMS\n",
      "video 1/3 (55/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (304.7ms) Inference, (6.4ms) NMS\n",
      "video 1/3 (56/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (282.8ms) Inference, (6.4ms) NMS\n",
      "video 1/3 (57/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (307.7ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (58/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (292.9ms) Inference, (5.4ms) NMS\n",
      "video 1/3 (59/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (300.8ms) Inference, (25.2ms) NMS\n",
      "video 1/3 (60/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (275.3ms) Inference, (4.9ms) NMS\n",
      "video 1/3 (61/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (327.6ms) Inference, (10.9ms) NMS\n",
      "video 1/3 (62/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (329.1ms) Inference, (2.6ms) NMS\n",
      "video 1/3 (63/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (336.7ms) Inference, (10.3ms) NMS\n",
      "video 1/3 (64/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (304.8ms) Inference, (2.0ms) NMS\n",
      "video 1/3 (65/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (362.4ms) Inference, (1.0ms) NMS\n",
      "video 1/3 (66/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (271.4ms) Inference, (0.8ms) NMS\n",
      "video 1/3 (67/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (286.8ms) Inference, (1.7ms) NMS\n",
      "video 1/3 (68/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (333.1ms) Inference, (3.0ms) NMS\n",
      "video 1/3 (69/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (245.8ms) Inference, (1.0ms) NMS\n",
      "video 1/3 (70/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (333.4ms) Inference, (2.0ms) NMS\n",
      "video 1/3 (71/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (330.2ms) Inference, (1.5ms) NMS\n",
      "video 1/3 (72/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (376.3ms) Inference, (1.0ms) NMS\n",
      "video 1/3 (73/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (317.1ms) Inference, (0.6ms) NMS\n",
      "video 1/3 (74/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (312.3ms) Inference, (0.7ms) NMS\n",
      "video 1/3 (75/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (353.5ms) Inference, (1.3ms) NMS\n",
      "video 1/3 (76/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (324.0ms) Inference, (5.5ms) NMS\n",
      "video 1/3 (77/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (315.4ms) Inference, (1.0ms) NMS\n",
      "video 1/3 (78/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (328.7ms) Inference, (0.8ms) NMS\n",
      "video 1/3 (79/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (239.5ms) Inference, (0.6ms) NMS\n",
      "video 1/3 (80/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (317.1ms) Inference, (2.4ms) NMS\n",
      "video 1/3 (81/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (359.6ms) Inference, (2.7ms) NMS\n",
      "video 1/3 (82/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:28:47.970717: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Prediction probabilities: [    0.25523     0.32084     0.42393]\n",
      "Done. (405.9ms) Inference, (7.4ms) NMS\n",
      "1/1 [==============================] - 0s 144ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25523     0.32085     0.42392]\n",
      "Done. (339.1ms) Inference, (0.7ms) NMS\n",
      "1/1 [==============================] - 0s 133ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25523     0.32085     0.42392]\n",
      "Done. (316.5ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (85/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (259.4ms) Inference, (2.4ms) NMS\n",
      "video 1/3 (86/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: Done. (204.2ms) Inference, (0.7ms) NMS\n",
      "1/1 [==============================] - 0s 137ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25618     0.32215     0.42167]\n",
      "Done. (245.7ms) Inference, (0.6ms) NMS\n",
      "1/1 [==============================] - 0s 134ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25618     0.32214     0.42168]\n",
      "Done. (321.5ms) Inference, (2.3ms) NMS\n",
      "1/1 [==============================] - 0s 133ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25623     0.32222     0.42155]\n",
      "Done. (330.7ms) Inference, (0.6ms) NMS\n",
      "1/1 [==============================] - 0s 144ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [     0.2562     0.32217     0.42164]\n",
      "Done. (319.1ms) Inference, (1.1ms) NMS\n",
      "1/1 [==============================] - 0s 132ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [     0.2562     0.32218     0.42162]\n",
      "Done. (331.6ms) Inference, (0.6ms) NMS\n",
      "1/1 [==============================] - 0s 141ms/stepinput/지훈_LF_H1.MOV\n",
      "Prediction probabilities: [    0.25653     0.32264     0.42083]\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "Prediction probabilities: [     0.2548     0.32029     0.42491]\n",
      "Done. (261.1ms) Inference, (0.5ms) NMS\n",
      "video 1/3 (93/484) /Users/jihoon/venvs/skeleton-rnn/input/지훈_LF_H1.MOV: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jihoon/venvs/yolov7-pose-tracking')\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_imshow, increment_path, set_logging, check_img_size, non_max_suppression, scale_coords\n",
    "from utils.plots import draw_boxes\n",
    "from utils.torch_utils import select_device, time_synchronized, TracedModel\n",
    "from pose.utils.datasets import LoadImages as PoseLoadImages\n",
    "from pose.detect import detect\n",
    "from sort import Sort\n",
    "\n",
    "\n",
    "\n",
    "def model_load(weights, device, imgsz):\n",
    "    model = attempt_load(weights, map_location=device)\n",
    "    stride = int(model.stride.max())\n",
    "    imgsz = check_img_size(imgsz, s=stride)\n",
    "    return model, stride, imgsz\n",
    "\n",
    "\n",
    "def view_image(p, im0):\n",
    "    cv2.imshow(str(p), im0)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def img_prep(img, device, half):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def track(det, img, im0, sort_tracker):\n",
    "    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "    dets_to_sort = np.empty((0, 6))\n",
    "    for x1, y1, x2, y2, conf, detclass in det.cpu().detach().numpy():\n",
    "        dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, detclass])))\n",
    "    tracked_dets = sort_tracker.update(dets_to_sort)\n",
    "    return tracked_dets\n",
    "\n",
    "\n",
    "def predict_walk_type_with_confidence(model, skeleton_data):\n",
    "    MAX_SEQUENCE_LENGTH = 632  # 학습 데이터의 최대 시퀀스 길이\n",
    "    \n",
    "    # Add this line to print the length and some values of skeleton_data\n",
    "    # print(\"Length of skeleton_data:\", len(skeleton_data))\n",
    "    # print(\"First few values of skeleton_data:\", skeleton_data[:10])\n",
    "       \n",
    "    # Convert the skeleton_data to a numpy array and reshape it to fit LSTM input shape\n",
    "    skeleton_data = np.array(skeleton_data).reshape(1, 1, 34)\n",
    "    \n",
    "    # Adjust the padding to match training data length\n",
    "    skeleton_data_padded = tf.keras.preprocessing.sequence.pad_sequences(skeleton_data, padding='post', dtype='float32', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    skeleton_data_padded = skeleton_data_padded.reshape(1, MAX_SEQUENCE_LENGTH, 34)  # Assuming that the skeleton data has 34 features per timestep\n",
    "    \n",
    "    # 이제 패딩된 데이터를 사용하여 모델에서 예측\n",
    "    prediction = model.predict(skeleton_data_padded)\n",
    "    \n",
    "    # 출력: skeleton_data_padded의 shape 및 데이터 타입\n",
    "    # print(\"Skeleton data shape:\", skeleton_data_padded.shape)\n",
    "    # print(\"Skeleton data type:\", skeleton_data_padded.dtype)\n",
    "    \n",
    "    #확률값 출력\n",
    "    print(\"Prediction probabilities:\", prediction[0])\n",
    "    \n",
    "    walk_type = np.argmax(prediction, axis=1)\n",
    "    walk_labels = [\"Parkinson's Walk\", \"Shuffling Walk\", \"Normal Walk\"]\n",
    "    return walk_labels[walk_type[0]]\n",
    "\n",
    "def select_keypoints(kpts, remove_columns):\n",
    "    # 모든 keypoints의 컬럼명 리스트를 여기에 적어주세요\n",
    "    all_columns = ['nose-x', 'nose-y', \"nose-conf\",\n",
    "            'left-eye-x', 'left-eye-y', \"left-eye-conf\",\n",
    "            'right-eye-x', 'right-eye-y', \"right-eye-conf\",\n",
    "            'left-ear-x', 'left-ear-y', \"left-ear-conf\",\n",
    "            'right-ear-x', 'right-ear-y', \"right-ear-conf\",\n",
    "            'left-shoulder-x', 'left-shoulder-y', \"left-shoulder-conf\",\n",
    "            'right-shoulder-x', 'right-shoulder-y', \"right-shoulder-conf\",\n",
    "            'left-elbow-x', 'left-elbow-y', \"left-elbow-conf\",\n",
    "            'right-elbow-x', 'right-elbow-y', \"right-elbow-conf\",\n",
    "            'left-hand-x', 'left-hand-y', \"left-hand-conf\",\n",
    "            'right-hand-x', 'right-hand-y', \"right-hand-conf\",\n",
    "            'left-hip-x', 'left-hip-y', \"left-hip-conf\",\n",
    "            'right-hip-x', 'right-hip-y', \"right-hip-conf\",\n",
    "            'left-knee-x', 'left-knee-y', \"left-knee-conf\",\n",
    "            'right-knee-x', 'right-knee-y', \"right-knee-conf\",\n",
    "            'left-foot-x', 'left-foot-y', \"left-foot-conf\",\n",
    "            'right-foot-x', 'right-foot-y', \"right-foot-conf\"]\n",
    "    \n",
    "    indices_to_remove = [all_columns.index(col) for col in remove_columns]\n",
    "    return [kpts[i] for i in range(len(kpts)) if i not in indices_to_remove]\n",
    "\n",
    "\n",
    "def visualize(tracked_dets, im0, model, imgsz, stride, device, half, names, h5_model):\n",
    "    bbox_xyxy = tracked_dets[:, :4]\n",
    "    identities = tracked_dets[:, -1]\n",
    "    categories = tracked_dets[:, 4]\n",
    "    \n",
    "    for idx, box in enumerate(bbox_xyxy):\n",
    "        cat = int(categories[idx]) if categories is not None else 0\n",
    "        if cat != 0:\n",
    "            continue\n",
    "        \n",
    "        id = int(identities[idx]) if identities is not None else 0\n",
    "        x1, y1, x2, y2 = [int(x) for x in box]\n",
    "        obj = im0[y1:y2, x1:x2]\n",
    "        if not obj.shape[0] or not obj.shape[1]:\n",
    "            continue\n",
    "        \n",
    "        d = PoseLoadImages(obj, imgsz, stride)\n",
    "        kpts, obj = detect(d, model, device, half, xy=[x1, y1])\n",
    "        if kpts is None:\n",
    "            continue\n",
    "            \n",
    "        # Print keypoints and skeleton_data for debugging\n",
    "        # print(\"Keypoints for frame:\", kpts)\n",
    "        \n",
    "        # 여기에서 kpts 값을 필터링\n",
    "        remove_columns = [\"nose-conf\", \"left-eye-conf\", \"right-eye-conf\", \"left-ear-conf\", \n",
    "                          \"right-ear-conf\", \"left-shoulder-conf\", \"right-shoulder-conf\", \n",
    "                          \"left-elbow-conf\", \"right-elbow-conf\", \"left-hand-conf\", \n",
    "                          \"right-hand-conf\", \"left-hip-conf\", \"right-hip-conf\", \n",
    "                          \"left-knee-conf\", \"right-knee-conf\", \"left-foot-conf\", \"right-foot-conf\"]\n",
    "\n",
    "        kpts = select_keypoints(kpts, remove_columns)\n",
    "        \n",
    "        kpts = np.array(kpts).reshape(1, -1)  # Assuming you are reshaping the data before feeding to model\n",
    "        # print(\"Skeleton data for frame:\", skeleton_data)\n",
    "\n",
    "        # 이제 필터링된 kpts 값을 .h5 모델에 입력\n",
    "\n",
    "        # Predict walk type based on keypoints\n",
    "        walk_label = predict_walk_type_with_confidence(h5_model, kpts)\n",
    "        \n",
    "        # Draw the prediction result on the video frame\n",
    "        label = f\"ID: {id} Type: {walk_label}\"\n",
    "        im0 = cv2.putText(im0, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    draw_boxes(im0, bbox_xyxy, identities, categories, names)\n",
    "    return im0\n",
    "\n",
    "def main(source):\n",
    "    device = 'cpu'\n",
    "    img_size, conf_thres, iou_thres = 640, 0.25, 0.45\n",
    "    view_img, save_json = False, True\n",
    "    save_img = not source.endswith('.txt')\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    # Initialize SORT tracker and save directory\n",
    "    sort_tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.2)\n",
    "    save_dir = Path(increment_path(Path('output') / 'obj', exist_ok=False))\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Initialize device and model\n",
    "    device = select_device(device)\n",
    "    half = device.type != 'cpu'\n",
    "    model, stride, imgsz = model_load('yolov7.pt', device, img_size)\n",
    "    model_, stride_, imgsz_ = model_load('yolov7-w6-pose.pt', device, img_size)\n",
    "    model = TracedModel(model, device, img_size)\n",
    "    if half:\n",
    "        model.half()\n",
    "        model_.half()\n",
    "    # Load the h5 model for walk prediction\n",
    "    h5_model_path = \"/Users/jihoon/venvs/skeleton-rnn/best_model_val_loss.h5\"\n",
    "    h5_model = tf.keras.models.load_model(h5_model_path)\n",
    "    \n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "        model_(torch.zeros(1, 3, imgsz_, imgsz_).to(device).type_as(next(model_.parameters())))\n",
    "    t0 = time.time()\n",
    "    nf = 0\n",
    "    results = {}\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        nf += 1\n",
    "        img = img_prep(img, device, half)\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=False)[0]\n",
    "        t2 = time_synchronized()\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=0)\n",
    "        t3 = time_synchronized()\n",
    "        for i, det in enumerate(pred):\n",
    "            if webcam:\n",
    "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "            else:\n",
    "                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "            p = Path(p)\n",
    "            save_path = str(save_dir / p.name)\n",
    "            if len(det):\n",
    "                tracked_dets = track(det, img, im0, sort_tracker)\n",
    "                if len(tracked_dets) > 0:\n",
    "                    results = visualize(tracked_dets, im0, model_, imgsz_, stride_, device, half, names, h5_model)\n",
    "            else:\n",
    "                tracked_dets = sort_tracker.update()\n",
    "        print(f'Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "        if view_img:\n",
    "            view_image(p, im0)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'image':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:\n",
    "                if vid_path != save_path:\n",
    "                    vid_path = save_path\n",
    "                    if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                        vid_writer.release()\n",
    "                    if vid_cap:\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    else:\n",
    "                        fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                        save_path += '.mp4'\n",
    "                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                vid_writer.write(im0)\n",
    "    if save_json:\n",
    "        t4 = time.time()\n",
    "        results['tag'] = {'time': t4-t0,\n",
    "                          'num of frame': nf,\n",
    "                          'total detected': len(results.keys()),\n",
    "                          'frame/time': nf/(t4-t0)\n",
    "                          }\n",
    "        with open(save_path.split('.')[0]+'.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "\n",
    "def convert_json_to_csv(base='./output/'):\n",
    "    path_list = [base + f for f in os.listdir(base) if os.path.isdir(base + f)]\n",
    "    labels = ['nose-x', 'nose-y',\n",
    "            'left-eye-x', 'left-eye-y',\n",
    "            'right-eye-x', 'right-eye-y',\n",
    "            'left-ear-x', 'left-ear-y',\n",
    "            'right-ear-x', 'right-ear-y',\n",
    "            'left-shoulder-x', 'left-shoulder-y',\n",
    "            'right-shoulder-x', 'right-shoulder-y',\n",
    "            'left-elbow-x', 'left-elbow-y',\n",
    "            'right-elbow-x', 'right-elbow-y',\n",
    "            'left-hand-x', 'left-hand-y',\n",
    "            'right-hand-x', 'right-hand-y',\n",
    "            'left-hip-x', 'left-hip-y',\n",
    "            'right-hip-x', 'right-hip-y',\n",
    "            'left-knee-x', 'left-knee-y',\n",
    "            'right-knee-x', 'right-knee-y',\n",
    "            'left-foot-x', 'left-foot-y',\n",
    "            'right-foot-x', 'right-foot-y']  # Use the full labels list you provided before\n",
    "    for path in path_list:\n",
    "        files = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "        for file in files:\n",
    "            name = file.split('.')[0]\n",
    "            with open(f'{path}/{file}', 'r') as j:\n",
    "                data = json.load(j)\n",
    "                frame = data['tag']['num of frame']\n",
    "                for key in data.keys():\n",
    "                    if len(data[key]) < (frame*0.5):\n",
    "                        continue\n",
    "                    with open(f'{path}/{name}-{key}.csv', 'w', newline='') as c:\n",
    "                        w = csv.writer(c)\n",
    "                        w.writerow(labels)\n",
    "                        for kpts in data[key]:\n",
    "                            w.writerow(kpts)\n",
    "\n",
    "\n",
    "def process_video_and_visualize(source):\n",
    "    with torch.no_grad():\n",
    "        main(source)\n",
    "    convert_json_to_csv()\n",
    "    h5_model_path = \"/Users/jihoon/venvs/skeleton-rnn/best_model_val_loss.h5\"\n",
    "    h5_model = tf.keras.models.load_model(h5_model_path)\n",
    "    base = './output/'\n",
    "    path_list = [base + f for f in os.listdir(base) if os.path.isdir(base + f)]\n",
    "    for path in path_list:\n",
    "        files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "        for file in files:\n",
    "            skeleton_data = np.loadtxt(f'{path}/{file}', delimiter=',', skiprows=1)\n",
    "            predictions = predict_walk_type_with_confidence(h5_model, skeleton_data)\n",
    "            # Add the prediction results on the video or save it separately.\n",
    "            # ...\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    source = '/Users/jihoon/venvs/skeleton-rnn/input'\n",
    "    main(source)\n",
    "    # process_video_and_visualize(video_source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
